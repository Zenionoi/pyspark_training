{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709bd626-fd3f-4f6c-829d-12024eb0d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/23 23:45:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.179:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Functions 3.5.0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109ef8f50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find [why] for questions\n",
    "# why I get error with eval_type = read_int(infile)\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Functions 3.5.0\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaee414-52b3-436d-84a8-e6b68c5e0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------------+--------------------+-----+\n",
      "|    Player Name|Season|       Statistic|            Variable|Value|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "|Robert Garrigus|  2010|Driving Distance|Driving Distance ...|   71|\n",
      "|   Bubba Watson|  2010|Driving Distance|Driving Distance ...|   77|\n",
      "| Dustin Johnson|  2010|Driving Distance|Driving Distance ...|   83|\n",
      "|Brett Wetterich|  2010|Driving Distance|Driving Distance ...|   54|\n",
      "|    J.B. Holmes|  2010|Driving Distance|Driving Distance ...|  100|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/23 23:45:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "path = \"datasets/\"\n",
    "students = spark.read.csv(path+\"students.csv\", inferSchema=True, header=True)\n",
    "tour = spark.read.csv(path+\"pga_tour_historical.csv\", inferSchema=True, header=True)\n",
    "tour.limit(100).dropna().dropDuplicates().limit(5).show()\n",
    "students.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a4f2-8716-4107-995d-1b5cb6367cef",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72b5f0-c773-4bb2-9337-cbac3444af6f",
   "metadata": {},
   "source": [
    "## Normal Functions¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c22d6-0d81-4337-84b2-987ae967b7a5",
   "metadata": {},
   "source": [
    "## Math Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833b2a1-bc8b-427d-9abb-64ce1791aefa",
   "metadata": {},
   "source": [
    "## Datetime Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a06536-d0d4-4470-81c0-78092b1dbd37",
   "metadata": {},
   "source": [
    "## Collection Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e810582-e040-40dc-9c41-a2a56b23a6d0",
   "metadata": {},
   "source": [
    "## [Partition Transformation Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#partition-transformation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bb2cf-633c-4ac2-958f-f0aa844be4da",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "\n",
    "Get AnalysisException: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `catalog`.`db`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389b6af-43f6-4891-b876-441f993c6d51",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.years(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fba128cd-b329-43d1-b3dd-58fc7d34541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    years(\"ts\")\\n).createOrReplace()\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    years(\"ts\")\n",
    ").createOrReplace()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb0cd3-f469-4818-8be6-dd94057f9fc8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.months(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68af08dc-8b2a-4b91-9e7d-91c5a0be2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(\\n    months(\"ts\")\\n).createOrReplace()\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(\n",
    "    months(\"ts\")\n",
    ").createOrReplace()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e926fad-6cb4-4554-b4cb-b99b9779b3d8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.days(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70a5212a-b7fb-4cfc-a021-6ffcfaba7ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    days(\"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    days(\"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95a306-1aba-4844-ab34-dc46f53256d7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hours(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps to partition data into hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24dfaec9-ccca-4f90-92eb-edd6fb7671e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(   \\n    hours(\"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(   \n",
    "    hours(\"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f06b9-90cb-44d8-8514-fdbb3fd8bde5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bucket(numBuckets: Union[pyspark.sql.column.Column, int], col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for any type that partitions by a hash of the input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6959a902-e792-4a40-ac58-569b65972ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    bucket(42, \"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    bucket(42, \"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31236ce4-bf64-426f-8701-15c58f00a748",
   "metadata": {},
   "source": [
    "## [Aggregate Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782904d8-fd58-4d0e-9ce5-fca48f9fd054",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.any_value(col: ColumnOrName, ignoreNulls: Union[bool, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Returns some value of col for a group of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a74c4900-1378-4d5b-8f91-49adb92b327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+\n",
      "|any_value(Player name)|any_value(Value)|\n",
      "+----------------------+----------------+\n",
      "|       Robert Garrigus|              71|\n",
      "+----------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.select(any_value(\"Player name\"), any_value(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5622c514-d6a9-4a79-a5cc-c0336baf4e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any_value(c1)=None, any_value(c2)=1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.select(any_value('c1'), any_value('c2')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "082237b0-2f82-4fa5-ba26-e0c9dc94513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any_value(c1)='a', any_value(c2)=1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(any_value('c1', True), any_value('c2', True)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2caf8f-d9f5-486c-8a60-738776e79d9b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.approxCountDistinct(col: ColumnOrName, rsd: Optional[float] = None) → pyspark.sql.column.Column\n",
    "##### Use approx_count_distinct() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5f75d-a617-423a-8ee9-4e0355dffdab",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.approx_count_distinct(col: ColumnOrName, rsd: Optional[float] = None) → pyspark.sql.column.Column\n",
    "Aggregate function: returns a new Column for approximate distinct count of column col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2684e784-b10a-4d67-857e-4dddb6acbd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/24 01:27:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|distinct_values|\n",
      "+---------------+\n",
      "|              3|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df.agg(approx_count_distinct(\"value\").alias('distinct_values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce40fe-94b4-42ac-a03c-37fb0c4fac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dbb05-5838-4c1a-b85c-95b4719162d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30da06a-d0ee-40a0-8b85-ae9cc7925626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1769c-f89e-442e-872d-532ed2ca5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9f682-1212-45c0-adc6-d08345118421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde365ad-e90a-4b30-a047-1ca4601c420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011165b3-4c3d-4601-a6bd-92a17d4ca8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462ccf8-38b7-4d4c-88bd-69e833397538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0173d3c-c16e-4b26-813a-112d2f401e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d8be0c8-051c-4dd0-b8c1-4cf4acd9eff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Window Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#window-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b0a83-3f2d-4e9a-9f0a-9a60447ea817",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.cume_dist() → pyspark.sql.column.Column\n",
    "Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "40472995-65ac-4977-985a-608fd41d74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| cd|\n",
      "+-----+---+\n",
      "|    1|0.2|\n",
      "|    2|0.4|\n",
      "|    3|0.8|\n",
      "|    3|0.8|\n",
      "|    4|1.0|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 2, 3, 3, 4], IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"cd\", cume_dist().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e927603d-e288-4920-8ffe-c967c9fde412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|  cd|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|0.02|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|0.06|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|0.06|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|0.08|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|0.16|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|0.16|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|0.16|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|0.16|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|0.18|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92| 0.2|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|0.24|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|0.24|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|0.26|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|0.34|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|0.34|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|0.34|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|0.34|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|0.46|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|0.46|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|0.46|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|0.46|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|0.46|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|0.46|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|0.48|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93| 0.5|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|0.56|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|0.56|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|0.56|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|0.64|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|0.64|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|0.64|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|0.64|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|0.68|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|0.68|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100| 0.7|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|0.74|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|0.74|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100| 0.8|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96| 0.8|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91| 0.8|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|0.82|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|0.86|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|0.86|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93| 1.0|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100| 1.0|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100| 1.0|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100| 1.0|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99| 1.0|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86| 1.0|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97| 1.0|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"math score\")\n",
    "students.orderBy(desc(\"reading score\")).limit(50).withColumn(\"cd\", cume_dist().over(w)).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7740c8d-8442-4632-ab14-c78aa966af92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.dense_rank() → pyspark.sql.column.Column\n",
    "Window function: returns the rank of rows within a window partition, without any gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0878be-7169-4e88-acb7-652e4b9119af",
   "metadata": {},
   "source": [
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "This is equivalent to the DENSE_RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "25c2bdbd-caa8-43c7-a35a-dd584f6ccb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|drank|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    1|    1|\n",
      "|    2|    2|\n",
      "|    3|    3|\n",
      "|    3|    3|\n",
      "|    4|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"drank\", dense_rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a45d403f-ea49-403a-8801-369879b09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|drank|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|    1|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|    2|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|    3|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|    4|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|    4|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|    5|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92|    6|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|    7|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|    8|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|    9|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|    9|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|    9|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|   10|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|   10|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|   10|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|   10|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|   11|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|   12|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|   13|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|   13|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|   13|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|   14|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|   14|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|   14|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|   14|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|   15|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|   15|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100|   16|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|   17|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|   17|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100|   18|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96|   18|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91|   18|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|   19|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|   20|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|   20|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93|   21|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100|   21|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100|   21|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100|   21|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99|   21|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86|   21|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97|   21|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"math score\")\n",
    "students.orderBy(desc(\"reading score\")).limit(50).withColumn(\"drank\", dense_rank().over(w)).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766e255-6385-40a4-88e7-04f2d3e2c29b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.lag(col: ColumnOrName, offset: int = 1, default: Optional[Any] = None) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is offset rows before the current row, and default if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition.\n",
    "\n",
    "This is equivalent to the LAG function in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8eff8-5b01-4a4d-8e0b-997539d34039",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int, optional default 1 | \n",
    "number of row to extend\n",
    "- default: optional | \n",
    "default value\n",
    "\n",
    "Returns – Column | \n",
    "value before current row based on offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a98ed371-1bf7-4dc7-9a28-7bca28d1a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|         NULL|\n",
      "|  a|  2|            1|\n",
      "|  a|  3|            2|\n",
      "|  b|  2|         NULL|\n",
      "|  b|  8|            2|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()\n",
    "\n",
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"previos_value\", lag(\"c2\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "922dfaab-9393-47ba-9a0a-14121c4e3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|            0|\n",
      "|  a|  2|            1|\n",
      "|  a|  3|            2|\n",
      "|  b|  2|            0|\n",
      "|  b|  8|            2|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"previos_value\", lag(\"c2\", 1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a090f3aa-8f9c-452d-8cb3-c5a29ae059e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|           -1|\n",
      "|  a|  2|           -1|\n",
      "|  a|  3|            1|\n",
      "|  b|  2|           -1|\n",
      "|  b|  8|           -1|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"previos_value\", lag(\"c2\", 2, -1).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aabec6-9774-467d-94a4-2983732252dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.lead(col: ColumnOrName, offset: int = 1, default: Optional[Any] = None) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is offset rows after the current row, and default if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "\n",
    "This is equivalent to the LEAD function in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876026a-5319-4011-aad4-7e0c4d4ac059",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int, optional default 1 | \n",
    "number of row to extend\n",
    "- default: optional | \n",
    "default value\n",
    "\n",
    "Returns – Column | \n",
    "value after current row based on offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4b0cf86c-3ab7-42db-908f-07fba0ac322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "24e4fa77-7af8-4ee5-be96-a758c7f193e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         2|\n",
      "|  a|  2|         3|\n",
      "|  a|  3|      NULL|\n",
      "|  b|  2|         8|\n",
      "|  b|  8|      NULL|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"next_value\", lead(\"c2\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "999bd3ae-5145-421c-a3fa-d3f7cd884d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         2|\n",
      "|  a|  2|         3|\n",
      "|  a|  3|         0|\n",
      "|  b|  2|         8|\n",
      "|  b|  8|         0|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_value\", lead(\"c2\", 1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b22223c9-bf1e-49c2-a0eb-ee337cf3f8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         3|\n",
      "|  a|  2|        -1|\n",
      "|  a|  3|        -1|\n",
      "|  b|  2|        -1|\n",
      "|  b|  8|        -1|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_value\", lead(\"c2\", 2, -1).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fdb31-6d59-4430-9062-c4dba8b43d1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.nth_value(col: ColumnOrName, offset: int, ignoreNulls: Optional[bool] = False) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is the offsetth row of the window frame (counting from 1), and null if the size of window frame is less than offset rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a313e6c-faf8-4d07-a2c4-548520d9e8bf",
   "metadata": {},
   "source": [
    "It will return the offsetth non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "This is equivalent to the nth_value function in SQL.\n",
    "\n",
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int | \n",
    "number of row to use as the value\n",
    "- ignoreNulls: bool, optional | \n",
    "indicates the Nth value should skip null in the determination of which row to use\n",
    "\n",
    "Returns – Column | \n",
    "value of nth row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a4cdda8d-1f64-45a5-bd80-5d625d456c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fd6f4d69-33d3-4339-b7b1-1716c2365fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| c1| c2|nth_value|\n",
      "+---+---+---------+\n",
      "|  a|  1|        1|\n",
      "|  a|  2|        1|\n",
      "|  a|  3|        1|\n",
      "|  b|  2|        2|\n",
      "|  b|  8|        2|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"nth_value\", nth_value(\"c2\", 1).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5449c855-5b08-45fa-9f35-d1c7cd4a2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| c1| c2|nth_value|\n",
      "+---+---+---------+\n",
      "|  a|  1|     NULL|\n",
      "|  a|  2|        2|\n",
      "|  a|  3|        2|\n",
      "|  b|  2|     NULL|\n",
      "|  b|  8|        8|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"nth_value\", nth_value(\"c2\", 2).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cb38f-df80-4951-86be-f59ce0131964",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.ntile(n: int) → pyspark.sql.column.Column\n",
    "Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4.\n",
    "\n",
    "This is equivalent to the NTILE function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a8635865-3fa3-4302-ba7a-23509bdf10a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "59924f16-d1b0-47dc-ab8a-0fd2772dbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| c1| c2|ntile|\n",
      "+---+---+-----+\n",
      "|  a|  1|    1|\n",
      "|  a|  2|    1|\n",
      "|  a|  3|    2|\n",
      "|  b|  2|    1|\n",
      "|  b|  8|    2|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"ntile\", ntile(2).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "337a9752-5853-4217-a50c-9a071b48f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|ntile|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        26|           31|           38|    1|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        37|           57|           56|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        39|           64|           57|    1|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        40|           52|           43|    1|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        40|           48|           50|    1|\n",
      "|  male|       group A|         associate's degree|free/reduced|              completed|        40|           55|           53|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        40|           59|           51|    1|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        41|           51|           48|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        42|           61|           58|    1|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        43|           45|           50|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        43|           60|           58|    1|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        44|           41|           38|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        46|           43|           42|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        46|           58|           57|    1|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        46|           43|           41|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        46|           56|           57|    1|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        46|           61|           55|    1|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        46|           43|           44|    1|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        47|           57|           44|    1|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        47|           49|           50|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        47|           53|           58|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        47|           37|           35|    1|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        48|           43|           45|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        49|           51|           43|    2|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        49|           52|           54|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        49|           53|           53|    2|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        49|           51|           51|    2|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        50|           56|           54|    2|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        51|           60|           58|    2|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        51|           51|           54|    2|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        52|           55|           49|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        52|           76|           70|    2|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        52|           66|           73|    2|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        52|           59|           56|    2|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        52|           57|           50|    2|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        52|           59|           62|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        52|           55|           57|    2|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        53|           58|           65|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        53|           71|           67|    2|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        53|           61|           62|    2|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        53|           54|           48|    2|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        53|           45|           40|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        53|           70|           70|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        53|           62|           53|    2|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        54|           58|           61|    2|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        54|           53|           47|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        54|           65|           65|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        54|           61|           58|    3|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        55|           61|           54|    3|\n",
      "|female|       group A|         associate's degree|    standard|              completed|        55|           65|           62|    3|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        55|           72|           79|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        55|           76|           76|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        56|           65|           63|    3|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        56|           68|           70|    3|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        57|           56|           57|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        57|           74|           76|    3|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        57|           69|           68|    3|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        57|           78|           67|    3|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        57|           54|           56|    3|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        57|           78|           79|    3|\n",
      "|female|       group E|         associate's degree|free/reduced|              completed|        57|           68|           73|    3|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        57|           77|           80|    3|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        58|           54|           52|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        58|           73|           68|    3|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        58|           63|           65|    3|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        58|           55|           53|    3|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        58|           57|           53|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        59|           66|           67|    3|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        59|           70|           66|    4|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        59|           70|           65|    4|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        59|           73|           72|    4|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        59|           62|           69|    4|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        60|           51|           56|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        60|           75|           74|    4|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        61|           58|           56|    4|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        61|           86|           87|    4|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        61|           55|           52|    4|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        61|           48|           46|    4|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        61|           42|           41|    4|\n",
      "|  male|       group D|         associate's degree|free/reduced|              completed|        61|           71|           73|    4|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        62|           61|           55|    4|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        62|           56|           53|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        62|           74|           70|    4|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        62|           61|           58|    4|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        62|           76|           80|    4|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        62|           65|           58|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        63|           67|           70|    4|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        63|           61|           61|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        64|           73|           68|    4|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        64|           66|           59|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        64|           64|           70|    5|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        64|           76|           74|    5|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        64|           56|           52|    5|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        65|           54|           57|    5|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        65|           69|           70|    5|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        65|           67|           65|    5|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        65|           65|           63|    5|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        65|           77|           74|    5|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        65|           85|           76|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        65|           76|           76|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        65|           77|           74|    5|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        65|           73|           68|    5|\n",
      "|female|       group A|         associate's degree|    standard|              completed|        65|           70|           74|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        65|           84|           84|    5|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        65|           75|           77|    5|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        66|           63|           64|    5|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        66|           62|           64|    5|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        66|           65|           69|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        66|           77|           73|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        67|           84|           86|    5|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        67|           62|           60|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        67|           84|           81|    5|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        67|           72|           67|    6|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        67|           54|           63|    6|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        67|           57|           53|    6|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        68|           67|           69|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        68|           86|           84|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        68|           67|           73|    6|\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        68|           77|           80|    6|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        68|           65|           61|    6|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        68|           76|           67|    6|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        69|           80|           71|    6|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        69|           77|           69|    6|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        69|           70|           63|    6|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        70|           84|           81|    6|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        71|           83|           78|    6|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        71|           71|           74|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        71|           77|           77|    6|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        71|           74|           68|    6|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        71|           66|           60|    6|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        72|           64|           63|    6|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        72|           79|           74|    6|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        72|           57|           62|    6|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        73|           68|           66|    6|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        73|           76|           80|    7|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        73|           83|           76|    7|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        73|           75|           80|    7|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        73|           76|           78|    7|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        73|           78|           72|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        74|           81|           83|    7|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        74|           75|           83|    7|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        74|           73|           67|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        74|           88|           90|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        75|           90|           88|    7|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        75|           68|           64|    7|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        75|           66|           73|    7|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        75|           82|           90|    7|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        76|           70|           68|    7|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        76|           71|           67|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        76|           74|           73|    7|\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|    7|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        77|           69|           68|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        77|           89|           98|    7|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        77|           67|           64|    7|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        77|           78|           73|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        77|           77|           73|    7|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        78|           81|           82|    8|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        78|           77|           77|    8|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        78|           74|           72|    8|\n",
      "|  male|       group A|         associate's degree|free/reduced|              completed|        79|           82|           82|    8|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        79|           88|           94|    8|\n",
      "|  male|       group D|         associate's degree|free/reduced|              completed|        79|           82|           80|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           75|           77|    8|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        80|           76|           64|    8|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        80|           86|           83|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           68|           72|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           63|           63|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           75|           69|    8|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        81|           81|           79|    8|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        81|           82|           82|    8|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        81|           73|           72|    8|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        81|           77|           79|    8|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        81|           72|           77|    8|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        81|           75|           78|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        81|           71|           73|    8|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        82|           85|           86|    8|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        82|           78|           74|    8|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        82|           75|           77|    8|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        82|           80|           77|    9|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|    9|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|    9|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|    9|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        82|           84|           78|    9|\n",
      "|female|       group E|         associate's degree|free/reduced|              completed|        83|           86|           88|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        83|           72|           78|    9|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        83|           85|           90|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        84|           80|           80|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92|    9|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        85|           91|           89|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        85|           76|           71|    9|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        85|           89|           95|    9|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        85|           84|           82|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        85|           92|           85|    9|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        87|           85|           73|    9|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        87|           74|           76|    9|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|    9|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        87|           84|           85|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|    9|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        87|           73|           72|    9|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|    9|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        89|           76|           74|   10|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        90|           87|           75|   10|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        90|           78|           81|   10|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        90|           90|           91|   10|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        90|           87|           85|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        90|           90|           82|   10|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        91|           89|           92|   10|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           86|           84|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        91|           73|           80|   10|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|   10|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        92|           79|           84|   10|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|   10|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        94|           87|           92|   10|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        94|           85|           82|   10|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        95|           89|           92|   10|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|   10|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        97|           82|           88|   10|\n",
      "|  male|       group A|         associate's degree|    standard|              completed|        97|           92|           86|   10|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91|   10|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        98|           87|           90|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93|   10|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100|   10|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        29|           41|           47|    1|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        37|           56|           47|    1|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        37|           45|           38|    1|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        39|           42|           38|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        43|           51|           54|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        43|           62|           61|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        44|           63|           62|    1|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        45|           59|           64|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        47|           62|           66|    1|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        48|           51|           46|    1|\n",
      "|  male|       group A|          bachelor's degree|free/reduced|              completed|        49|           58|           60|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        50|           60|           59|    1|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        50|           42|           48|    2|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        51|           49|           51|    2|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        51|           72|           79|    2|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        52|           65|           69|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        52|           61|           66|    2|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        53|           58|           55|    2|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        54|           49|           47|    2|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        55|           59|           54|    2|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        55|           46|           44|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        56|           79|           72|    2|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        58|           55|           48|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        59|           64|           75|    2|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        59|           70|           73|    3|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        59|           72|           70|    3|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        59|           54|           51|    3|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        61|           72|           70|    3|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        61|           66|           61|    3|\n",
      "|female|       group E|          bachelor's degree|free/reduced|                   none|        61|           58|           62|    3|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        61|           70|           76|    3|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        62|           63|           56|    3|\n",
      "|  male|       group A|          bachelor's degree|free/reduced|                   none|        62|           72|           65|    3|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        62|           78|           79|    3|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        62|           72|           74|    3|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        63|           66|           67|    3|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        63|           75|           81|    4|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        63|           71|           69|    4|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        63|           73|           78|    4|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        63|           64|           66|    4|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        64|           73|           70|    4|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        64|           60|           58|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        65|           72|           74|    4|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        65|           67|           62|    4|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        65|           73|           75|    4|\n",
      "|female|       group B|          bachelor's degree|    standard|              completed|        65|           81|           81|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        65|           79|           81|    4|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        66|           60|           57|    4|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        66|           64|           62|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        66|           83|           83|    5|\n",
      "|female|       group B|          bachelor's degree|    standard|              completed|        66|           74|           81|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        66|           74|           81|    5|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        67|           69|           75|    5|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        67|           86|           83|    5|\n",
      "|  male|       group D|          bachelor's degree|    standard|              completed|        67|           61|           68|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        67|           75|           72|    5|\n",
      "|  male|       group D|          bachelor's degree|    standard|              completed|        68|           74|           74|    5|\n",
      "|female|       group D|          bachelor's degree|    standard|              completed|        68|           75|           81|    5|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        68|           68|           67|    5|\n",
      "|  male|       group E|          bachelor's degree|    standard|                   none|        68|           68|           64|    5|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        69|           63|           61|    6|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        69|           58|           57|    6|\n",
      "|  male|       group E|          bachelor's degree|free/reduced|              completed|        70|           68|           72|    6|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        70|           64|           70|    6|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|              completed|        70|           75|           74|    6|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        71|           74|           68|    6|\n",
      "|female|       group D|          bachelor's degree|    standard|              completed|        71|           76|           83|    6|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        71|           70|           70|    6|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        72|           72|           74|    6|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        73|           56|           57|    6|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        73|           79|           84|    6|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        74|           71|           80|    6|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        74|           79|           75|    7|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        74|           86|           89|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        75|           85|           82|    7|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        75|           58|           62|    7|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        75|           73|           74|    7|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        75|           84|           80|    7|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        76|           62|           66|    7|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        77|           67|           68|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        77|           88|           87|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        77|           85|           87|    7|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        78|           82|           79|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        78|           79|           76|    8|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        78|           90|           93|    8|\n",
      "|  male|       group E|          bachelor's degree|free/reduced|              completed|        79|           74|           72|    8|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        79|           92|           89|    8|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        79|           89|           89|    8|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        79|           81|           82|    8|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        80|           78|           81|    8|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        80|           83|           83|    8|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        80|           73|           72|    8|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        81|           88|           90|    8|\n",
      "|  male|       group E|          bachelor's degree|    standard|                   none|        82|           62|           62|    8|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        83|           82|           84|    8|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        83|           78|           73|    9|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|    9|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        85|           66|           71|    9|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        86|           83|           86|    9|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|    9|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|              completed|        87|           90|           88|    9|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        87|           84|           87|    9|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        88|           75|           76|    9|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        88|           78|           83|    9|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|    9|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|    9|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        91|           81|           79|   10|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|   10|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|   10|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|   10|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        94|           90|           91|   10|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|   10|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        96|           90|           92|   10|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96|   10|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|   10|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100|   10|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100|   10|\n",
      "|female|       group B|                high school|free/reduced|                   none|         8|           24|           23|    1|\n",
      "|female|       group B|                high school|free/reduced|              completed|        23|           44|           36|    1|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        27|           34|           36|    1|\n",
      "|female|       group C|                high school|    standard|                   none|        29|           29|           30|    1|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        30|           24|           15|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        33|           41|           43|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        34|           42|           39|    1|\n",
      "|female|       group A|                high school|free/reduced|              completed|        34|           48|           41|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        35|           61|           54|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        35|           53|           46|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        36|           53|           43|    1|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        36|           29|           27|    1|\n",
      "|female|       group B|                high school|free/reduced|                   none|        38|           60|           50|    1|\n",
      "|female|       group D|                high school|free/reduced|                   none|        39|           52|           46|    1|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        40|           46|           50|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        41|           46|           43|    1|\n",
      "|  male|       group D|                high school|    standard|                   none|        41|           52|           51|    1|\n",
      "|female|       group E|                high school|free/reduced|                   none|        41|           45|           40|    1|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        42|           39|           34|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        42|           62|           60|    1|\n",
      "|female|       group B|                high school|    standard|                   none|        42|           52|           51|    2|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        44|           51|           48|    2|\n",
      "|female|       group C|                high school|    standard|                   none|        44|           61|           52|    2|\n",
      "|  male|       group D|                high school|    standard|                   none|        45|           48|           46|    2|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        45|           47|           49|    2|\n",
      "|female|       group D|                high school|    standard|                   none|        45|           63|           59|    2|\n",
      "|female|       group B|                high school|free/reduced|              completed|        46|           54|           58|    2|\n",
      "|  male|       group D|                high school|    standard|                   none|        46|           34|           36|    2|\n",
      "|  male|       group B|                high school|    standard|                   none|        47|           46|           42|    2|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        48|           45|           41|    2|\n",
      "|female|       group B|                high school|    standard|                   none|        48|           62|           60|    2|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        49|           45|           45|    2|\n",
      "|female|       group D|                high school|free/reduced|                   none|        49|           57|           52|    2|\n",
      "|female|       group E|                high school|    standard|                   none|        50|           50|           47|    2|\n",
      "|female|       group B|                high school|free/reduced|                   none|        50|           67|           63|    2|\n",
      "|female|       group C|                high school|free/reduced|              completed|        50|           66|           64|    2|\n",
      "|female|       group B|                high school|    standard|                   none|        50|           53|           55|    2|\n",
      "|  male|       group C|                high school|    standard|                   none|        50|           48|           42|    2|\n",
      "|female|       group D|                high school|    standard|                   none|        51|           66|           62|    2|\n",
      "|  male|       group C|                high school|    standard|                   none|        52|           53|           49|    2|\n",
      "|female|       group D|                high school|free/reduced|              completed|        52|           57|           56|    3|\n",
      "|  male|       group B|                high school|    standard|                   none|        52|           48|           49|    3|\n",
      "|  male|       group B|                high school|    standard|              completed|        52|           49|           46|    3|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        53|           51|           51|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        53|           52|           42|    3|\n",
      "|  male|       group C|                high school|    standard|              completed|        53|           52|           49|    3|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        53|           58|           44|    3|\n",
      "|female|       group C|                high school|free/reduced|                   none|        53|           72|           64|    3|\n",
      "|female|       group A|                high school|free/reduced|              completed|        53|           50|           60|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        54|           52|           52|    3|\n",
      "|female|       group C|                high school|    standard|                   none|        54|           59|           62|    3|\n",
      "|female|       group B|                high school|    standard|                   none|        54|           64|           68|    3|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        54|           72|           59|    3|\n",
      "|  male|       group E|                high school|free/reduced|                   none|        55|           56|           51|    3|\n",
      "|female|       group A|                high school|    standard|                   none|        55|           73|           73|    3|\n",
      "|  male|       group D|                high school|    standard|              completed|        55|           41|           48|    3|\n",
      "|female|       group D|                high school|    standard|              completed|        56|           68|           74|    3|\n",
      "|female|       group D|                high school|    standard|                   none|        56|           52|           55|    3|\n",
      "|  male|       group A|                high school|    standard|                   none|        57|           43|           47|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        57|           50|           54|    3|\n",
      "|  male|       group E|                high school|free/reduced|              completed|        57|           56|           54|    4|\n",
      "|female|       group D|                high school|    standard|              completed|        57|           58|           64|    4|\n",
      "|  male|       group B|                high school|    standard|                   none|        57|           48|           51|    4|\n",
      "|female|       group E|                high school|free/reduced|                   none|        57|           58|           57|    4|\n",
      "|female|       group E|                high school|free/reduced|              completed|        57|           75|           73|    4|\n",
      "|  male|       group A|                high school|    standard|                   none|        57|           51|           54|    4|\n",
      "|female|       group B|                high school|    standard|              completed|        58|           70|           68|    4|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        58|           61|           52|    4|\n",
      "|female|       group B|                high school|    standard|                   none|        58|           62|           59|    4|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        58|           51|           52|    4|\n",
      "|female|       group C|                high school|    standard|              completed|        58|           75|           77|    4|\n",
      "|female|       group B|                high school|    standard|                   none|        58|           68|           61|    4|\n",
      "|  male|       group C|                high school|    standard|              completed|        58|           52|           54|    4|\n",
      "|female|       group E|                high school|    standard|              completed|        59|           63|           75|    4|\n",
      "|  male|       group A|                high school|    standard|                   none|        59|           52|           46|    4|\n",
      "|female|       group C|                high school|    standard|                   none|        59|           72|           68|    4|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        59|           53|           52|    4|\n",
      "|  male|       group B|                high school|    standard|                   none|        59|           58|           47|    4|\n",
      "|female|       group C|                high school|free/reduced|              completed|        59|           71|           65|    4|\n",
      "|  male|       group B|                high school|    standard|              completed|        60|           44|           47|    4|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        60|           57|           51|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        60|           68|           72|    5|\n",
      "|  male|       group B|                high school|    standard|                   none|        60|           68|           60|    5|\n",
      "|female|       group B|                high school|free/reduced|                   none|        60|           72|           68|    5|\n",
      "|female|       group C|                high school|    standard|              completed|        60|           64|           74|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        61|           73|           63|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        61|           72|           70|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        61|           56|           55|    5|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        61|           60|           55|    5|\n",
      "|female|       group A|                high school|    standard|                   none|        61|           68|           63|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        62|           55|           49|    5|\n",
      "|female|       group D|                high school|    standard|                   none|        62|           64|           64|    5|\n",
      "|female|       group C|                high school|free/reduced|                   none|        62|           67|           64|    5|\n",
      "|  male|       group B|                high school|    standard|                   none|        62|           55|           54|    5|\n",
      "|female|       group B|                high school|    standard|                   none|        62|           62|           63|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        62|           67|           58|    5|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        62|           55|           55|    5|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        63|           57|           56|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        63|           69|           74|    5|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        63|           48|           47|    5|\n",
      "|  male|       group A|                high school|    standard|                   none|        63|           63|           62|    6|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        64|           64|           67|    6|\n",
      "|female|       group E|                high school|free/reduced|                   none|        64|           62|           68|    6|\n",
      "|  male|       group D|                high school|    standard|                   none|        64|           54|           50|    6|\n",
      "|female|       group B|                high school|free/reduced|                   none|        64|           73|           71|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        65|           81|           73|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        65|           64|           62|    6|\n",
      "|female|       group C|                high school|    standard|                   none|        65|           69|           67|    6|\n",
      "|female|       group D|                high school|free/reduced|              completed|        65|           61|           71|    6|\n",
      "|  male|       group D|                high school|    standard|                   none|        66|           69|           63|    6|\n",
      "|female|       group C|                high school|    standard|                   none|        66|           71|           76|    6|\n",
      "|female|       group E|                high school|free/reduced|              completed|        66|           74|           78|    6|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        66|           77|           70|    6|\n",
      "|female|       group C|                high school|free/reduced|                   none|        66|           76|           68|    6|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        66|           66|           59|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        66|           72|           70|    6|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        66|           74|           69|    6|\n",
      "|female|       group C|                high school|free/reduced|              completed|        67|           79|           84|    6|\n",
      "|female|       group B|                high school|free/reduced|              completed|        67|           78|           79|    6|\n",
      "|female|       group B|                high school|free/reduced|              completed|        67|           80|           81|    6|\n",
      "|female|       group D|                high school|    standard|                   none|        67|           72|           74|    7|\n",
      "|female|       group B|                high school|    standard|              completed|        68|           83|           78|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        68|           60|           53|    7|\n",
      "|  male|       group D|                high school|    standard|              completed|        68|           64|           66|    7|\n",
      "|  male|       group A|                high school|    standard|                   none|        68|           70|           66|    7|\n",
      "|female|       group A|                high school|    standard|              completed|        68|           80|           76|    7|\n",
      "|female|       group D|                high school|    standard|                   none|        69|           72|           77|    7|\n",
      "|female|       group B|                high school|    standard|              completed|        69|           76|           74|    7|\n",
      "|  male|       group C|                high school|    standard|              completed|        69|           58|           53|    7|\n",
      "|female|       group D|                high school|    standard|                   none|        69|           77|           73|    7|\n",
      "|female|       group D|                high school|    standard|              completed|        69|           77|           78|    7|\n",
      "|  male|       group D|                high school|    standard|                   none|        69|           75|           71|    7|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        69|           70|           67|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           70|           65|    7|\n",
      "|  male|       group E|                high school|    standard|                   none|        70|           55|           56|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           56|           51|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           74|           71|    7|\n",
      "|  male|       group B|                high school|    standard|                   none|        70|           65|           60|    7|\n",
      "|  male|       group D|                high school|    standard|                   none|        70|           70|           70|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           79|           71|    8|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           66|           65|    8|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           60|           61|    8|\n",
      "|  male|       group A|                high school|    standard|                   none|        71|           74|           64|    8|\n",
      "|female|       group B|                high school|free/reduced|                   none|        71|           87|           82|    8|\n",
      "|  male|       group A|                high school|    standard|              completed|        72|           73|           74|    8|\n",
      "|female|       group C|                high school|    standard|                   none|        72|           80|           75|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        72|           65|           68|    8|\n",
      "|  male|       group A|                high school|free/reduced|              completed|        72|           67|           65|    8|\n",
      "|  male|       group C|                high school|    standard|              completed|        72|           67|           64|    8|\n",
      "|female|       group C|                high school|    standard|                   none|        72|           80|           83|    8|\n",
      "|  male|       group D|                high school|    standard|                   none|        72|           66|           66|    8|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        73|           68|           66|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        73|           71|           68|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        73|           69|           68|    8|\n",
      "|female|       group D|                high school|free/reduced|                   none|        73|           92|           84|    8|\n",
      "|  male|       group E|                high school|    standard|                   none|        73|           64|           57|    8|\n",
      "|female|       group E|                high school|    standard|                   none|        74|           81|           71|    8|\n",
      "|female|       group E|                high school|    standard|                   none|        74|           76|           73|    8|\n",
      "|female|       group E|                high school|    standard|              completed|        74|           79|           80|    9|\n",
      "|female|       group B|                high school|    standard|                   none|        74|           72|           72|    9|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        74|           70|           69|    9|\n",
      "|female|       group E|                high school|    standard|                   none|        75|           86|           79|    9|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        75|           74|           66|    9|\n",
      "|  male|       group C|                high school|    standard|              completed|        75|           69|           68|    9|\n",
      "|female|       group C|                high school|    standard|                   none|        75|           88|           85|    9|\n",
      "|female|       group A|                high school|    standard|              completed|        75|           82|           79|    9|\n",
      "|  male|       group C|                high school|    standard|                   none|        75|           81|           71|    9|\n",
      "|  male|       group D|                high school|    standard|                   none|        76|           73|           68|    9|\n",
      "|female|       group C|                high school|    standard|                   none|        76|           76|           74|    9|\n",
      "|  male|       group B|                high school|    standard|              completed|        76|           62|           60|    9|\n",
      "|female|       group B|                high school|free/reduced|              completed|        76|           85|           82|    9|\n",
      "|female|       group A|                high school|free/reduced|              completed|        77|           88|           85|    9|\n",
      "|female|       group B|                high school|    standard|              completed|        77|           82|           89|    9|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        78|           77|           80|    9|\n",
      "|female|       group D|                high school|    standard|                   none|        78|           81|           80|    9|\n",
      "|  male|       group B|                high school|    standard|                   none|        79|           60|           65|    9|\n",
      "|  male|       group E|                high school|    standard|                   none|        80|           76|           65|    9|\n",
      "|  male|       group E|                high school|    standard|              completed|        81|           80|           76|   10|\n",
      "|female|       group B|                high school|    standard|                   none|        81|           91|           89|   10|\n",
      "|female|       group C|                high school|    standard|                   none|        81|           84|           82|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        81|           66|           64|   10|\n",
      "|  male|       group C|                high school|    standard|              completed|        82|           84|           82|   10|\n",
      "|  male|       group B|                high school|    standard|                   none|        82|           82|           80|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        84|           77|           74|   10|\n",
      "|  male|       group E|                high school|    standard|                   none|        84|           73|           69|   10|\n",
      "|  male|       group C|                high school|    standard|              completed|        86|           81|           80|   10|\n",
      "|  male|       group E|                high school|free/reduced|              completed|        86|           81|           75|   10|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|   10|\n",
      "|  male|       group E|                high school|    standard|              completed|        87|           91|           81|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        88|           89|           86|   10|\n",
      "|  male|       group D|                high school|    standard|                   none|        88|           78|           75|   10|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|   10|\n",
      "|  male|       group D|                high school|    standard|                   none|        89|           87|           79|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        90|           75|           69|   10|\n",
      "|  male|       group E|                high school|    standard|                   none|        94|           73|           71|   10|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|   10|\n",
      "|female|       group D|            master's degree|free/reduced|                   none|        40|           59|           54|    1|\n",
      "|female|       group C|            master's degree|free/reduced|                   none|        40|           58|           54|    1|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        45|           56|           54|    1|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        46|           42|           46|    1|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        47|           58|           67|    1|\n",
      "|  male|       group B|            master's degree|free/reduced|                   none|        49|           53|           52|    1|\n",
      "|female|       group A|            master's degree|    standard|                   none|        50|           53|           58|    2|\n",
      "|female|       group A|            master's degree|free/reduced|                   none|        50|           67|           73|    2|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        52|           70|           62|    2|\n",
      "|female|       group C|            master's degree|free/reduced|                   none|        52|           65|           61|    2|\n",
      "|female|       group D|            master's degree|    standard|                   none|        53|           61|           68|    2|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        54|           59|           50|    2|\n",
      "|female|       group D|            master's degree|    standard|                   none|        54|           60|           63|    3|\n",
      "|female|       group C|            master's degree|    standard|              completed|        54|           64|           67|    3|\n",
      "|female|       group D|            master's degree|    standard|                   none|        55|           64|           70|    3|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        56|           72|           65|    3|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        58|           76|           78|    3|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        61|           67|           66|    3|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        61|           71|           78|    4|\n",
      "|female|       group D|            master's degree|    standard|                   none|        62|           70|           75|    4|\n",
      "|female|       group E|            master's degree|    standard|                   none|        62|           68|           68|    4|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        62|           68|           75|    4|\n",
      "|female|       group D|            master's degree|    standard|                   none|        64|           63|           66|    4|\n",
      "|female|       group C|            master's degree|free/reduced|              completed|        65|           81|           81|    4|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        67|           57|           59|    5|\n",
      "|female|       group C|            master's degree|    standard|              completed|        69|           84|           85|    5|\n",
      "|female|       group D|            master's degree|    standard|              completed|        70|           71|           74|    5|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        71|           67|           67|    5|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        72|           66|           72|    5|\n",
      "|  male|       group A|            master's degree|free/reduced|                   none|        73|           74|           72|    5|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        73|           70|           75|    6|\n",
      "|female|       group C|            master's degree|    standard|                   none|        73|           78|           74|    6|\n",
      "|female|       group D|            master's degree|    standard|                   none|        74|           79|           82|    6|\n",
      "|female|       group D|            master's degree|    standard|              completed|        77|           82|           91|    6|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|    6|\n",
      "|female|       group B|            master's degree|    standard|                   none|        77|           90|           84|    6|\n",
      "|female|       group D|            master's degree|    standard|                   none|        78|           91|           96|    7|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        79|           81|           71|    7|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        79|           78|           77|    7|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        79|           72|           69|    7|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        79|           77|           75|    7|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        80|           80|           72|    7|\n",
      "|female|       group E|            master's degree|    standard|                   none|        81|           92|           91|    8|\n",
      "|female|       group C|            master's degree|    standard|              completed|        81|           91|           87|    8|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        81|           81|           84|    8|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        81|           86|           87|    8|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        82|           82|           74|    8|\n",
      "|  male|       group D|            master's degree|free/reduced|              completed|        84|           89|           90|    8|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|    9|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        85|           84|           89|    9|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|    9|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|    9|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        89|           84|           82|    9|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|    9|\n",
      "|  male|       group E|            master's degree|    standard|                   none|        90|           85|           84|   10|\n",
      "|  male|       group C|            master's degree|    standard|              completed|        91|           85|           85|   10|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|   10|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100|   10|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        95|           81|           84|   10|\n",
      "|female|       group B|               some college|    standard|                   none|        19|           38|           32|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        22|           39|           33|    1|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        28|           23|           19|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        32|           39|           33|    1|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        35|           28|           27|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        35|           44|           43|    1|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        40|           43|           39|    1|\n",
      "|  male|       group D|               some college|    standard|                   none|        40|           42|           38|    1|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        41|           39|           34|    1|\n",
      "|female|       group E|               some college|free/reduced|              completed|        42|           55|           54|    1|\n",
      "|female|       group C|               some college|free/reduced|              completed|        42|           66|           69|    1|\n",
      "|  male|       group D|               some college|    standard|                   none|        44|           54|           53|    1|\n",
      "|female|       group B|               some college|free/reduced|                   none|        45|           53|           55|    1|\n",
      "|female|       group C|               some college|free/reduced|              completed|        45|           73|           70|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        46|           64|           66|    1|\n",
      "|  male|       group B|               some college|    standard|                   none|        47|           43|           41|    1|\n",
      "|female|       group B|               some college|free/reduced|              completed|        48|           56|           58|    1|\n",
      "|female|       group A|               some college|free/reduced|                   none|        49|           65|           55|    1|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        49|           57|           46|    1|\n",
      "|  male|       group E|               some college|free/reduced|              completed|        49|           52|           51|    1|\n",
      "|female|       group D|               some college|free/reduced|                   none|        49|           58|           60|    1|\n",
      "|female|       group D|               some college|free/reduced|                   none|        49|           65|           61|    1|\n",
      "|  male|       group A|               some college|free/reduced|              completed|        50|           47|           54|    1|\n",
      "|  male|       group C|               some college|free/reduced|              completed|        50|           48|           53|    2|\n",
      "|female|       group B|               some college|    standard|              completed|        50|           64|           66|    2|\n",
      "|female|       group D|               some college|    standard|                   none|        51|           58|           54|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        52|           58|           58|    2|\n",
      "|female|       group D|               some college|free/reduced|              completed|        52|           59|           65|    2|\n",
      "|  male|       group E|               some college|    standard|                   none|        53|           55|           48|    2|\n",
      "|  male|       group C|               some college|    standard|                   none|        53|           44|           42|    2|\n",
      "|  male|       group A|               some college|    standard|                   none|        53|           43|           43|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        53|           62|           56|    2|\n",
      "|  male|       group C|               some college|    standard|                   none|        53|           39|           37|    2|\n",
      "|female|       group E|               some college|free/reduced|                   none|        53|           58|           57|    2|\n",
      "|female|       group B|               some college|free/reduced|              completed|        53|           66|           73|    2|\n",
      "|  male|       group B|               some college|    standard|                   none|        54|           52|           51|    2|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        54|           54|           45|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        54|           48|           52|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        54|           64|           65|    2|\n",
      "|female|       group A|               some college|    standard|                   none|        54|           63|           67|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        55|           69|           65|    2|\n",
      "|female|       group D|               some college|free/reduced|                   none|        55|           71|           69|    2|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        55|           55|           47|    2|\n",
      "|  male|       group D|               some college|    standard|                   none|        55|           58|           52|    2|\n",
      "|female|       group A|               some college|    standard|                   none|        56|           58|           64|    2|\n",
      "|  male|       group D|               some college|    standard|              completed|        58|           59|           58|    2|\n",
      "|female|       group D|               some college|free/reduced|              completed|        58|           63|           73|    3|\n",
      "|female|       group A|               some college|    standard|                   none|        58|           70|           67|    3|\n",
      "|female|       group D|               some college|free/reduced|                   none|        58|           67|           62|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        58|           67|           72|    3|\n",
      "|female|       group B|               some college|free/reduced|                   none|        58|           61|           66|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        58|           49|           42|    3|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        58|           57|           54|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        58|           59|           66|    3|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        58|           60|           57|    3|\n",
      "|  male|       group B|               some college|    standard|                   none|        58|           50|           45|    3|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        59|           65|           66|    3|\n",
      "|female|       group C|               some college|free/reduced|                   none|        59|           62|           64|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        59|           41|           42|    3|\n",
      "|  male|       group E|               some college|    standard|                   none|        59|           51|           43|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        59|           71|           70|    3|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        59|           62|           61|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        59|           60|           58|    3|\n",
      "|female|       group D|               some college|free/reduced|              completed|        59|           78|           76|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        60|           72|           74|    3|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        60|           60|           60|    3|\n",
      "|  male|       group D|               some college|    standard|                   none|        60|           63|           59|    3|\n",
      "|female|       group D|               some college|free/reduced|                   none|        60|           66|           70|    3|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        60|           62|           60|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        61|           61|           62|    4|\n",
      "|female|       group B|               some college|free/reduced|                   none|        61|           68|           66|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        61|           47|           56|    4|\n",
      "|  male|       group A|               some college|    standard|              completed|        61|           51|           52|    4|\n",
      "|female|       group E|               some college|    standard|                   none|        61|           64|           62|    4|\n",
      "|female|       group A|               some college|free/reduced|                   none|        61|           60|           57|    4|\n",
      "|female|       group E|               some college|    standard|                   none|        62|           73|           70|    4|\n",
      "|  male|       group B|               some college|    standard|                   none|        62|           61|           57|    4|\n",
      "|female|       group C|               some college|free/reduced|                   none|        62|           67|           62|    4|\n",
      "|female|       group B|               some college|    standard|                   none|        62|           67|           67|    4|\n",
      "|  male|       group B|               some college|    standard|              completed|        62|           66|           68|    4|\n",
      "|female|       group C|               some college|free/reduced|                   none|        62|           72|           70|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        62|           57|           62|    4|\n",
      "|female|       group C|               some college|    standard|                   none|        62|           69|           69|    4|\n",
      "|female|       group D|               some college|    standard|                   none|        62|           70|           72|    4|\n",
      "|female|       group B|               some college|    standard|                   none|        63|           65|           61|    4|\n",
      "|  male|       group D|               some college|    standard|              completed|        63|           55|           63|    4|\n",
      "|female|       group E|               some college|    standard|              completed|        63|           72|           70|    4|\n",
      "|female|       group C|               some college|    standard|              completed|        63|           78|           80|    4|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        63|           61|           54|    4|\n",
      "|female|       group C|               some college|    standard|                   none|        63|           74|           74|    4|\n",
      "|female|       group D|               some college|free/reduced|              completed|        63|           80|           80|    4|\n",
      "|female|       group C|               some college|free/reduced|              completed|        63|           73|           71|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        63|           61|           60|    5|\n",
      "|female|       group D|               some college|    standard|                   none|        63|           64|           67|    5|\n",
      "|  male|       group C|               some college|    standard|                   none|        63|           63|           60|    5|\n",
      "|female|       group C|               some college|free/reduced|              completed|        64|           85|           85|    5|\n",
      "|female|       group D|               some college|free/reduced|                   none|        64|           74|           75|    5|\n",
      "|female|       group C|               some college|    standard|              completed|        64|           82|           77|    5|\n",
      "|female|       group B|               some college|free/reduced|              completed|        65|           75|           70|    5|\n",
      "|  male|       group D|               some college|    standard|              completed|        65|           77|           74|    5|\n",
      "|female|       group D|               some college|free/reduced|                   none|        65|           81|           77|    5|\n",
      "|female|       group D|               some college|    standard|                   none|        65|           70|           71|    5|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        65|           58|           49|    5|\n",
      "|  male|       group E|               some college|    standard|                   none|        66|           57|           52|    5|\n",
      "|female|       group E|               some college|    standard|              completed|        66|           74|           73|    5|\n",
      "|  male|       group C|               some college|    standard|                   none|        66|           59|           52|    5|\n",
      "|  male|       group B|               some college|    standard|                   none|        66|           65|           60|    5|\n",
      "|female|       group C|               some college|free/reduced|              completed|        67|           75|           70|    5|\n",
      "|female|       group E|               some college|    standard|                   none|        67|           76|           75|    5|\n",
      "|female|       group C|               some college|    standard|              completed|        67|           81|           79|    5|\n",
      "|  male|       group D|               some college|    standard|                   none|        67|           64|           70|    5|\n",
      "|  male|       group C|               some college|free/reduced|              completed|        67|           74|           70|    5|\n",
      "|female|       group D|               some college|free/reduced|              completed|        67|           86|           83|    5|\n",
      "|  male|       group D|               some college|    standard|                   none|        68|           59|           62|    5|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        68|           68|           61|    5|\n",
      "|  male|       group E|               some college|    standard|                   none|        68|           60|           59|    6|\n",
      "|  male|       group E|               some college|    standard|                   none|        68|           72|           65|    6|\n",
      "|female|       group E|               some college|    standard|                   none|        68|           70|           66|    6|\n",
      "|female|       group D|               some college|    standard|              completed|        68|           78|           77|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        69|           90|           88|    6|\n",
      "|  male|       group B|               some college|    standard|                   none|        69|           54|           55|    6|\n",
      "|female|       group D|               some college|    standard|                   none|        69|           74|           74|    6|\n",
      "|  male|       group A|               some college|    standard|                   none|        69|           67|           69|    6|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        69|           66|           60|    6|\n",
      "|female|       group D|               some college|    standard|                   none|        69|           77|           77|    6|\n",
      "|  male|       group D|               some college|free/reduced|              completed|        69|           60|           63|    6|\n",
      "|female|       group D|               some college|    standard|              completed|        69|           79|           81|    6|\n",
      "|female|       group C|               some college|    standard|                   none|        69|           78|           76|    6|\n",
      "|  male|       group C|               some college|    standard|                   none|        69|           64|           68|    6|\n",
      "|  male|       group B|               some college|    standard|              completed|        69|           77|           77|    6|\n",
      "|  male|       group E|               some college|    standard|                   none|        69|           60|           54|    6|\n",
      "|female|       group D|               some college|free/reduced|                   none|        69|           65|           74|    6|\n",
      "|female|       group A|               some college|    standard|                   none|        69|           84|           82|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        70|           89|           88|    6|\n",
      "|female|       group B|               some college|    standard|                   none|        70|           75|           78|    6|\n",
      "|female|       group D|               some college|free/reduced|              completed|        70|           78|           78|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        70|           72|           76|    6|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        70|           63|           58|    6|\n",
      "|  male|       group D|               some college|    standard|              completed|        71|           61|           69|    7|\n",
      "|female|       group D|               some college|free/reduced|                   none|        71|           83|           83|    7|\n",
      "|female|       group E|               some college|free/reduced|                   none|        71|           76|           70|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        71|           81|           80|    7|\n",
      "|female|       group E|               some college|    standard|                   none|        71|           70|           76|    7|\n",
      "|  male|       group D|               some college|    standard|                   none|        71|           49|           52|    7|\n",
      "|  male|       group B|               some college|    standard|              completed|        71|           75|           70|    7|\n",
      "|female|       group C|               some college|    standard|              completed|        71|           71|           80|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        72|           72|           71|    7|\n",
      "|female|       group A|               some college|    standard|              completed|        72|           79|           82|    7|\n",
      "|  male|       group D|               some college|    standard|                   none|        72|           57|           58|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        73|           80|           82|    7|\n",
      "|  male|       group C|               some college|    standard|                   none|        73|           74|           61|    7|\n",
      "|female|       group E|               some college|    standard|              completed|        73|           78|           76|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        73|           76|           78|    7|\n",
      "|female|       group B|               some college|free/reduced|                   none|        74|           81|           76|    7|\n",
      "|female|       group D|               some college|    standard|                   none|        74|           89|           84|    7|\n",
      "|female|       group D|               some college|    standard|              completed|        74|           75|           79|    7|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        74|           77|           73|    7|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        74|           77|           76|    7|\n",
      "|female|       group C|               some college|    standard|              completed|        75|           81|           84|    7|\n",
      "|female|       group E|               some college|free/reduced|              completed|        75|           88|           85|    7|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        75|           81|           74|    8|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        75|           68|           65|    8|\n",
      "|female|       group D|               some college|    standard|              completed|        75|           77|           83|    8|\n",
      "|  male|       group C|               some college|    standard|                   none|        76|           78|           75|    8|\n",
      "|female|       group C|               some college|free/reduced|                   none|        76|           83|           88|    8|\n",
      "|  male|       group D|               some college|    standard|              completed|        76|           83|           79|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        76|           64|           66|    8|\n",
      "|  male|       group E|               some college|    standard|                   none|        76|           67|           67|    8|\n",
      "|  male|       group E|               some college|    standard|                   none|        76|           71|           72|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        76|           71|           73|    8|\n",
      "|female|       group E|               some college|    standard|                   none|        76|           78|           80|    8|\n",
      "|  male|       group D|               some college|    standard|              completed|        77|           62|           62|    8|\n",
      "|female|       group D|               some college|    standard|                   none|        77|           68|           77|    8|\n",
      "|female|       group C|               some college|free/reduced|                   none|        77|           90|           91|    8|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        77|           62|           64|    8|\n",
      "|female|       group D|               some college|free/reduced|                   none|        77|           86|           86|    8|\n",
      "|  male|       group A|               some college|    standard|              completed|        78|           72|           70|    8|\n",
      "|female|       group A|               some college|    standard|              completed|        78|           87|           91|    8|\n",
      "|female|       group B|               some college|    standard|                   none|        79|           86|           92|    8|\n",
      "|  male|       group B|               some college|    standard|                   none|        79|           67|           67|    8|\n",
      "|female|       group D|               some college|    standard|                   none|        79|           86|           81|    8|\n",
      "|  male|       group C|               some college|    standard|              completed|        79|           79|           78|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        79|           73|           67|    9|\n",
      "|female|       group D|               some college|free/reduced|                   none|        79|           89|           86|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        79|           84|           91|    9|\n",
      "|female|       group D|               some college|    standard|                   none|        80|           90|           89|    9|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        80|           64|           66|    9|\n",
      "|  male|       group A|               some college|free/reduced|              completed|        81|           78|           81|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        81|           74|           71|    9|\n",
      "|  male|       group D|               some college|    standard|                   none|        81|           82|           84|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        82|           90|           94|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|    9|\n",
      "|female|       group B|               some college|    standard|                   none|        82|           85|           87|    9|\n",
      "|  male|       group D|               some college|    standard|              completed|        82|           82|           88|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        83|           80|           73|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        83|           83|           90|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        84|           77|           71|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        84|           83|           78|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        84|           87|           91|    9|\n",
      "|  male|       group C|               some college|    standard|                   none|        84|           87|           81|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        85|           86|           98|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        85|           75|           68|    9|\n",
      "|  male|       group D|               some college|    standard|              completed|        85|           81|           85|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        86|           76|           74|    9|\n",
      "|female|       group E|               some college|    standard|              completed|        86|           85|           91|   10|\n",
      "|  male|       group E|               some college|free/reduced|              completed|        87|           74|           70|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        87|           89|           94|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        87|           84|           86|   10|\n",
      "|female|       group E|               some college|    standard|                   none|        87|           85|           93|   10|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|   10|\n",
      "|  male|       group D|               some college|    standard|                   none|        88|           73|           78|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        88|           85|           76|   10|\n",
      "|  male|       group D|               some college|    standard|                   none|        88|           77|           77|   10|\n",
      "|  male|       group C|               some college|    standard|                   none|        91|           74|           76|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|   10|\n",
      "|  male|       group E|               some college|free/reduced|                   none|        93|           90|           83|   10|\n",
      "|  male|       group C|               some college|    standard|              completed|        93|           84|           90|   10|\n",
      "|  male|       group E|               some college|    standard|                   none|        97|           87|           82|   10|\n",
      "|  male|       group C|               some college|    standard|              completed|        98|           86|           90|   10|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|   10|\n",
      "|  male|       group E|               some college|    standard|              completed|        99|           87|           81|   10|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97|   10|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86|   10|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99|   10|\n",
      "|female|       group C|           some high school|free/reduced|                   none|         0|           17|           10|    1|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        18|           32|           28|    1|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        24|           38|           27|    1|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        27|           34|           32|    1|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        29|           40|           44|    1|\n",
      "|  male|       group E|           some high school|    standard|                   none|        30|           26|           22|    1|\n",
      "|female|       group B|           some high school|    standard|              completed|        32|           51|           44|    1|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        32|           34|           38|    1|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        35|           55|           60|    1|\n",
      "|female|       group B|           some high school|    standard|                   none|        37|           46|           46|    1|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        38|           49|           45|    1|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        38|           43|           43|    1|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        39|           39|           34|    1|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        40|           65|           64|    1|\n",
      "|female|       group B|           some high school|    standard|                   none|        41|           55|           51|    1|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        43|           53|           53|    1|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        44|           50|           51|    1|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        44|           64|           58|    1|\n",
      "|female|       group C|           some high school|    standard|              completed|        44|           51|           55|    2|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        44|           45|           45|    2|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        45|           37|           37|    2|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        45|           52|           49|    2|\n",
      "|  male|       group A|           some high school|    standard|              completed|        46|           41|           43|    2|\n",
      "|  male|       group A|           some high school|    standard|              completed|        47|           49|           49|    2|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        47|           59|           50|    2|\n",
      "|female|       group C|           some high school|    standard|                   none|        47|           54|           53|    2|\n",
      "|  male|       group B|           some high school|free/reduced|                   none|        48|           52|           45|    2|\n",
      "|female|       group A|           some high school|    standard|                   none|        48|           66|           65|    2|\n",
      "|female|       group D|           some high school|    standard|                   none|        48|           58|           54|    2|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        48|           58|           52|    2|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        48|           56|           51|    2|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        48|           54|           53|    2|\n",
      "|  male|       group C|           some high school|    standard|                   none|        49|           49|           41|    2|\n",
      "|female|       group C|           some high school|    standard|                   none|        49|           63|           56|    2|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        49|           58|           55|    2|\n",
      "|  male|       group B|           some high school|free/reduced|              completed|        49|           50|           52|    2|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        50|           64|           59|    3|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        50|           60|           60|    3|\n",
      "|female|       group D|           some high school|    standard|                   none|        51|           63|           61|    3|\n",
      "|  male|       group B|           some high school|    standard|              completed|        51|           54|           41|    3|\n",
      "|  male|       group C|           some high school|    standard|                   none|        51|           52|           44|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        51|           56|           53|    3|\n",
      "|  male|       group A|           some high school|    standard|                   none|        51|           31|           36|    3|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        52|           67|           72|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        53|           37|           40|    3|\n",
      "|  male|       group A|           some high school|    standard|                   none|        53|           54|           48|    3|\n",
      "|female|       group B|           some high school|    standard|              completed|        54|           61|           62|    3|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        55|           65|           62|    3|\n",
      "|  male|       group D|           some high school|free/reduced|              completed|        55|           59|           59|    3|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        55|           46|           43|    3|\n",
      "|  male|       group D|           some high school|    standard|                   none|        55|           47|           44|    3|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        56|           54|           52|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        56|           61|           60|    3|\n",
      "|female|       group B|           some high school|    standard|                   none|        57|           67|           72|    3|\n",
      "|  male|       group C|           some high school|    standard|                   none|        57|           61|           54|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           58|           59|    4|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        59|           69|           65|    4|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        59|           42|           41|    4|\n",
      "|female|       group C|           some high school|    standard|              completed|        59|           54|           67|    4|\n",
      "|female|       group A|           some high school|    standard|              completed|        59|           85|           80|    4|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        59|           73|           69|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           67|           61|    4|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        59|           63|           64|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           72|           80|    4|\n",
      "|female|       group B|           some high school|    standard|              completed|        60|           70|           70|    4|\n",
      "|  male|       group D|           some high school|    standard|                   none|        60|           59|           54|    4|\n",
      "|female|       group B|           some high school|    standard|              completed|        60|           70|           74|    4|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        61|           57|           56|    4|\n",
      "|female|       group D|           some high school|    standard|              completed|        61|           74|           72|    4|\n",
      "|  male|       group B|           some high school|    standard|              completed|        61|           56|           56|    4|\n",
      "|  male|       group A|           some high school|free/reduced|              completed|        61|           62|           61|    4|\n",
      "|  male|       group A|           some high school|    standard|              completed|        62|           67|           69|    4|\n",
      "|  male|       group D|           some high school|    standard|                   none|        62|           67|           61|    5|\n",
      "|  male|       group D|           some high school|    standard|              completed|        62|           66|           68|    5|\n",
      "|female|       group B|           some high school|    standard|                   none|        62|           64|           66|    5|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        62|           49|           52|    5|\n",
      "|  male|       group C|           some high school|    standard|                   none|        62|           64|           55|    5|\n",
      "|  male|       group C|           some high school|    standard|              completed|        63|           60|           57|    5|\n",
      "|female|       group C|           some high school|    standard|                   none|        63|           73|           68|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        63|           67|           67|    5|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        63|           78|           79|    5|\n",
      "|female|       group D|           some high school|    standard|              completed|        64|           60|           74|    5|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        64|           79|           77|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        64|           53|           57|    5|\n",
      "|  male|       group A|           some high school|    standard|                   none|        64|           50|           43|    5|\n",
      "|  male|       group C|           some high school|    standard|                   none|        64|           58|           51|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        65|           66|           62|    5|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        65|           86|           80|    5|\n",
      "|female|       group C|           some high school|    standard|              completed|        65|           74|           77|    5|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        65|           59|           53|    5|\n",
      "|female|       group D|           some high school|    standard|                   none|        65|           82|           81|    6|\n",
      "|female|       group C|           some high school|    standard|                   none|        65|           69|           76|    6|\n",
      "|female|       group D|           some high school|    standard|              completed|        65|           78|           82|    6|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        65|           76|           75|    6|\n",
      "|female|       group B|           some high school|    standard|              completed|        65|           82|           78|    6|\n",
      "|female|       group B|           some high school|    standard|                   none|        66|           69|           68|    6|\n",
      "|female|       group D|           some high school|    standard|              completed|        66|           78|           78|    6|\n",
      "|  male|       group A|           some high school|    standard|              completed|        66|           68|           64|    6|\n",
      "|  male|       group B|           some high school|    standard|                   none|        67|           64|           61|    6|\n",
      "|female|       group B|           some high school|    standard|                   none|        67|           89|           82|    6|\n",
      "|female|       group C|           some high school|    standard|              completed|        67|           74|           77|    6|\n",
      "|  male|       group C|           some high school|    standard|              completed|        67|           73|           68|    6|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        67|           84|           84|    6|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        68|           72|           64|    6|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        68|           63|           54|    6|\n",
      "|  male|       group E|           some high school|    standard|              completed|        68|           51|           57|    6|\n",
      "|female|       group D|           some high school|    standard|                   none|        68|           71|           75|    6|\n",
      "|  male|       group B|           some high school|    standard|                   none|        68|           54|           53|    6|\n",
      "|female|       group C|           some high school|    standard|                   none|        69|           75|           78|    7|\n",
      "|female|       group C|           some high school|    standard|                   none|        69|           73|           73|    7|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        69|           71|           65|    7|\n",
      "|  male|       group D|           some high school|    standard|                   none|        69|           66|           61|    7|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        69|           86|           81|    7|\n",
      "|female|       group B|           some high school|    standard|                   none|        70|           64|           72|    7|\n",
      "|female|       group C|           some high school|    standard|              completed|        70|           82|           76|    7|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        71|           84|           87|    7|\n",
      "|female|       group A|           some high school|    standard|                   none|        71|           83|           77|    7|\n",
      "|  male|       group A|           some high school|    standard|                   none|        71|           62|           50|    7|\n",
      "|  male|       group D|           some high school|    standard|              completed|        71|           69|           68|    7|\n",
      "|  male|       group B|           some high school|    standard|                   none|        72|           68|           67|    7|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        72|           81|           79|    7|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        72|           79|           77|    7|\n",
      "|female|       group D|           some high school|    standard|                   none|        73|           86|           82|    7|\n",
      "|female|       group D|           some high school|    standard|                   none|        73|           84|           85|    7|\n",
      "|  male|       group E|           some high school|free/reduced|              completed|        73|           67|           59|    7|\n",
      "|  male|       group C|           some high school|    standard|                   none|        73|           66|           66|    7|\n",
      "|  male|       group D|           some high school|    standard|                   none|        73|           66|           62|    8|\n",
      "|female|       group B|           some high school|    standard|                   none|        73|           79|           79|    8|\n",
      "|  male|       group C|           some high school|    standard|                   none|        73|           66|           63|    8|\n",
      "|  male|       group D|           some high school|    standard|              completed|        74|           71|           78|    8|\n",
      "|  male|       group D|           some high school|    standard|                   none|        74|           74|           72|    8|\n",
      "|  male|       group B|           some high school|    standard|                   none|        74|           63|           57|    8|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        74|           90|           88|    8|\n",
      "|  male|       group E|           some high school|    standard|              completed|        74|           64|           60|    8|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        74|           74|           72|    8|\n",
      "|female|       group C|           some high school|    standard|                   none|        74|           75|           82|    8|\n",
      "|  male|       group D|           some high school|    standard|                   none|        75|           74|           69|    8|\n",
      "|  male|       group C|           some high school|    standard|                   none|        75|           72|           62|    8|\n",
      "|female|       group D|           some high school|    standard|                   none|        76|           72|           71|    8|\n",
      "|  male|       group D|           some high school|    standard|              completed|        76|           70|           69|    8|\n",
      "|female|       group C|           some high school|    standard|              completed|        76|           87|           85|    8|\n",
      "|  male|       group C|           some high school|    standard|              completed|        76|           80|           73|    8|\n",
      "|  male|       group E|           some high school|    standard|              completed|        77|           76|           77|    8|\n",
      "|female|       group E|           some high school|    standard|                   none|        77|           79|           80|    8|\n",
      "|female|       group C|           some high school|    standard|              completed|        77|           90|           85|    9|\n",
      "|female|       group C|           some high school|    standard|                   none|        77|           91|           88|    9|\n",
      "|  male|       group D|           some high school|    standard|              completed|        77|           68|           69|    9|\n",
      "|  male|       group E|           some high school|free/reduced|              completed|        78|           83|           80|    9|\n",
      "|  male|       group D|           some high school|    standard|              completed|        78|           81|           86|    9|\n",
      "|  male|       group C|           some high school|    standard|              completed|        78|           72|           69|    9|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        79|           82|           73|    9|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        79|           76|           65|    9|\n",
      "|  male|       group B|           some high school|    standard|              completed|        79|           85|           86|    9|\n",
      "|  male|       group D|           some high school|free/reduced|              completed|        80|           79|           79|    9|\n",
      "|female|       group D|           some high school|    standard|                   none|        80|           90|           82|    9|\n",
      "|female|       group D|           some high school|    standard|              completed|        80|           92|           88|    9|\n",
      "|female|       group E|           some high school|    standard|              completed|        80|           85|           85|    9|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|    9|\n",
      "|  male|       group D|           some high school|    standard|                   none|        81|           78|           78|    9|\n",
      "|  male|       group E|           some high school|    standard|              completed|        81|           75|           76|    9|\n",
      "|female|       group B|           some high school|    standard|                   none|        82|           82|           80|    9|\n",
      "|  male|       group E|           some high school|    standard|                   none|        82|           67|           61|    9|\n",
      "|  male|       group B|           some high school|    standard|              completed|        84|           83|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        84|           84|           80|   10|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|   10|\n",
      "|  male|       group B|           some high school|    standard|              completed|        85|           84|           78|   10|\n",
      "|female|       group A|           some high school|    standard|              completed|        85|           90|           92|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        86|           80|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        86|           73|           70|   10|\n",
      "|  male|       group E|           some high school|    standard|              completed|        87|           84|           76|   10|\n",
      "|  male|       group B|           some high school|    standard|                   none|        88|           84|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|              completed|        88|           74|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|              completed|        89|           88|           82|   10|\n",
      "|  male|       group E|           some high school|    standard|              completed|        89|           84|           77|   10|\n",
      "|  male|       group E|           some high school|    standard|                   none|        92|           87|           78|   10|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|   10|\n",
      "|  male|       group E|           some high school|    standard|                   none|        94|           88|           78|   10|\n",
      "|  male|       group B|           some high school|    standard|              completed|        94|           86|           87|   10|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100|   10|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"ntile\", ntile(10).over(w)).show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb01513-b583-47ba-9016-df6a33b0a181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.percent_rank() → pyspark.sql.column.Column\n",
    "Window function: returns the relative rank (i.e. percentile) of rows within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cbc75042-a995-4d53-9fc5-4299f27b5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| pr|\n",
      "+-----+---+\n",
      "|    1|0.0|\n",
      "|    1|0.0|\n",
      "|    2|0.4|\n",
      "|    3|0.6|\n",
      "|    3|0.6|\n",
      "|    4|1.0|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"pr\", percent_rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f6ffe69f-5971-4bd6-a8d5-45c3c54faf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|math score|                  pr|\n",
      "+----------+--------------------+\n",
      "|        44| 0.05128205128205128|\n",
      "|        49| 0.08547008547008547|\n",
      "|        58| 0.27586206896551724|\n",
      "|        77|                0.76|\n",
      "|        90|  0.9095022624434389|\n",
      "|        53|  0.1452991452991453|\n",
      "|        79|  0.6379310344827587|\n",
      "|        60| 0.28444444444444444|\n",
      "|        59|  0.3089887640449438|\n",
      "|        35|  0.0449438202247191|\n",
      "|        59| 0.37435897435897436|\n",
      "|        92|  0.9775280898876404|\n",
      "|        61| 0.23076923076923078|\n",
      "|        41| 0.07865168539325842|\n",
      "|        51| 0.12669683257918551|\n",
      "|        57| 0.24434389140271492|\n",
      "|        63| 0.49743589743589745|\n",
      "|        69|  0.6461538461538462|\n",
      "|        53|  0.1724137931034483|\n",
      "|        69|  0.5288888888888889|\n",
      "|        58| 0.28054298642533937|\n",
      "|        29|                 0.0|\n",
      "|        60|  0.3595505617977528|\n",
      "|        61|   0.334841628959276|\n",
      "|        95|  0.9683257918552036|\n",
      "|        74|                 0.8|\n",
      "|        88|  0.9550561797752809|\n",
      "|        75|  0.7022222222222222|\n",
      "|        97|  0.9733333333333334|\n",
      "|        24|0.011235955056179775|\n",
      "|        53| 0.24719101123595505|\n",
      "|        54|  0.2512820512820513|\n",
      "|        90|  0.9897435897435898|\n",
      "|        81|  0.7241379310344828|\n",
      "|        71|  0.6460674157303371|\n",
      "|        63|  0.3891402714932127|\n",
      "|        82|  0.7918552036199095|\n",
      "|        85|  0.8376068376068376|\n",
      "|        88|  0.9692307692307692|\n",
      "|        99|                 1.0|\n",
      "|        44| 0.04888888888888889|\n",
      "|        74|  0.6068376068376068|\n",
      "|        54| 0.19909502262443438|\n",
      "|        64|  0.4222222222222222|\n",
      "|        79|  0.7194570135746606|\n",
      "|        98|  0.9909502262443439|\n",
      "|        78|  0.6206896551724138|\n",
      "|        91|  0.9482758620689655|\n",
      "|        50| 0.16923076923076924|\n",
      "|        83|  0.8280542986425339|\n",
      "|        37|0.008547008547008548|\n",
      "|        55|  0.1623931623931624|\n",
      "|        19|                 0.0|\n",
      "|        39| 0.00904977375565611|\n",
      "|       100|  0.9911111111111112|\n",
      "|        77|  0.8769230769230769|\n",
      "|        40|                 0.0|\n",
      "|        88|  0.8793103448275862|\n",
      "|        88|  0.8803418803418803|\n",
      "|        41| 0.07692307692307693|\n",
      "|        27|0.010256410256410256|\n",
      "|        28|0.008888888888888889|\n",
      "|        71|  0.6133333333333333|\n",
      "|        45| 0.11235955056179775|\n",
      "|        82|  0.9282051282051282|\n",
      "|        67| 0.49321266968325794|\n",
      "|        52|  0.1282051282051282|\n",
      "|        75|  0.8256410256410256|\n",
      "|        79|  0.8974358974358975|\n",
      "|        48| 0.07111111111111111|\n",
      "|        66|  0.5337078651685393|\n",
      "|        65|  0.5384615384615384|\n",
      "|        32|0.013333333333333334|\n",
      "|        61|  0.4358974358974359|\n",
      "|        41|0.035555555555555556|\n",
      "|        62|   0.398876404494382|\n",
      "|        68|  0.5203619909502263|\n",
      "|        64|  0.3793103448275862|\n",
      "|        87|  0.8620689655172413|\n",
      "|        95|                 1.0|\n",
      "|        41| 0.03167420814479638|\n",
      "|        79|  0.8426966292134831|\n",
      "|        43| 0.04072398190045249|\n",
      "|        74|  0.6289592760180995|\n",
      "|        66|   0.558974358974359|\n",
      "|        45| 0.05333333333333334|\n",
      "|        96|  0.9572649572649573|\n",
      "|        36| 0.05128205128205128|\n",
      "|        64|  0.4550561797752809|\n",
      "|        71|  0.7128205128205128|\n",
      "|        55| 0.17777777777777778|\n",
      "|        50| 0.20224719101123595|\n",
      "|        47| 0.14358974358974358|\n",
      "|        44|  0.0898876404494382|\n",
      "|        89|  0.9662921348314607|\n",
      "|        54| 0.15384615384615385|\n",
      "|        81|  0.7948717948717948|\n",
      "|        72|  0.7384615384615385|\n",
      "|        70|  0.6348314606741573|\n",
      "|        38|0.056179775280898875|\n",
      "+----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"pr\", percent_rank().over(w)).select(\"math score\", \"pr\").distinct().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec764c-f6da-4048-89eb-d4a1298ed9d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.rank() → pyspark.sql.column.Column\n",
    "Window function: returns the rank of rows within a window partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca76cb0-c3b9-42a7-8e58-f51c74223a21",
   "metadata": {},
   "source": [
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "This is equivalent to the RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5859a360-aa81-494b-876f-d75ccd15565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|drank|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    1|    1|\n",
      "|    2|    3|\n",
      "|    3|    4|\n",
      "|    3|    4|\n",
      "|    4|    6|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"drank\", rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "78c713f8-defb-4916-9a49-883118d94a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|math score|drank|\n",
      "+----------+-----+\n",
      "|        68|   57|\n",
      "|        73|  152|\n",
      "|        47|   24|\n",
      "|        72|  131|\n",
      "|        84|  187|\n",
      "|        90|  202|\n",
      "|        92|  211|\n",
      "|        72|   69|\n",
      "|        64|   96|\n",
      "|        58|   67|\n",
      "|        68|  116|\n",
      "|        78|  148|\n",
      "|        89|  106|\n",
      "|        42|   10|\n",
      "|        76|  168|\n",
      "|        64|  102|\n",
      "|        81|  169|\n",
      "|        66|   48|\n",
      "|        48|   30|\n",
      "|        50|   37|\n",
      "|        41|    8|\n",
      "|        55|   20|\n",
      "|        98|  221|\n",
      "|        81|   94|\n",
      "|        39|    3|\n",
      "|        88|  200|\n",
      "|        60|   80|\n",
      "|        69|   61|\n",
      "|        76|  148|\n",
      "|        27|    4|\n",
      "|        99|  223|\n",
      "|        52|   27|\n",
      "|        62|   32|\n",
      "|        70|   27|\n",
      "|        68|  114|\n",
      "|        39|   13|\n",
      "|        53|   18|\n",
      "|        64|   41|\n",
      "|        78|   84|\n",
      "|        75|  137|\n",
      "|        51|   14|\n",
      "|        75|   75|\n",
      "|        87|   51|\n",
      "|        66|  106|\n",
      "|        53|   38|\n",
      "|        90|   54|\n",
      "|        84|  163|\n",
      "|        71|   66|\n",
      "|        92|  109|\n",
      "|        69|   26|\n",
      "|        61|   70|\n",
      "|        63|   85|\n",
      "|        86|  168|\n",
      "|        29|    4|\n",
      "|        34|    7|\n",
      "|        65|  106|\n",
      "|        79|  176|\n",
      "|        70|   63|\n",
      "|        46|   13|\n",
      "|        63|   87|\n",
      "|        78|  174|\n",
      "|        58|   46|\n",
      "|        60|   65|\n",
      "|        49|   11|\n",
      "|        50|   12|\n",
      "|        23|    2|\n",
      "|        41|   16|\n",
      "|        77|  152|\n",
      "|        69|  120|\n",
      "|        75|  144|\n",
      "|        87|  102|\n",
      "|        46|    4|\n",
      "|        76|   79|\n",
      "|        32|    4|\n",
      "|        59|   56|\n",
      "|        80|   91|\n",
      "|        71|  140|\n",
      "|        86|  186|\n",
      "|        54|   36|\n",
      "|        84|  184|\n",
      "|        54|   19|\n",
      "|        74|   72|\n",
      "|        46|   27|\n",
      "|        48|   27|\n",
      "|        49|   33|\n",
      "|        81|  158|\n",
      "|        85|  165|\n",
      "|        26|    1|\n",
      "|        52|    9|\n",
      "|        97|  220|\n",
      "|        70|  125|\n",
      "|        54|   50|\n",
      "|       100|  224|\n",
      "|        81|   43|\n",
      "|        51|   39|\n",
      "|        88|  190|\n",
      "|        47|    5|\n",
      "|        68|  122|\n",
      "|        19|    1|\n",
      "|        48|   17|\n",
      "+----------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"drank\", rank().over(w)).select(\"math score\", \"drank\").distinct().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3e18c-67ca-402c-bfee-11e5c38e0cc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.row_number() → pyspark.sql.column.Column\n",
    "Window function: returns a sequential number starting at 1 within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9384c2aa-fe24-493a-9c2e-5492a29ce288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|desc_order|\n",
      "+---+----------+\n",
      "|  2|         1|\n",
      "|  1|         2|\n",
      "|  0|         3|\n",
      "+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(3)\n",
    "w = Window.orderBy(df.id.desc())\n",
    "df.withColumn(\"desc_order\", row_number().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bdbfea4a-ecd4-4853-9752-7ae4bc54b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|math score|desc_order|\n",
      "+----------+----------+\n",
      "|        32|         8|\n",
      "|        47|        24|\n",
      "|        53|        49|\n",
      "|        68|        57|\n",
      "|        68|        60|\n",
      "|        69|        62|\n",
      "|        72|        69|\n",
      "|        61|        77|\n",
      "|        62|        78|\n",
      "|        63|        81|\n",
      "|        60|        84|\n",
      "|        80|        93|\n",
      "|        69|       112|\n",
      "|        72|       131|\n",
      "|        73|       152|\n",
      "|        73|       153|\n",
      "|        84|       185|\n",
      "|        84|       187|\n",
      "|        90|       202|\n",
      "|        92|       211|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"desc_order\", row_number().over(w)).select(\"math score\", \"desc_order\").distinct().limit(20).orderBy(asc(\"desc_order\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02006d2-6938-453c-aaa1-6e1cc5b7f8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Sort Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#sort-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd37ae-43f2-43d3-a5e6-0c1f9309cb8c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "36119302-a8fc-4cd4-90b4-ffb423e4b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(5)\n",
    "df = df.sort(desc(\"id\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7a981d3f-becb-4610-9142-82c8a39fe34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(asc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "44c15f16-f19b-4ae9-94c5-0cb8fde17158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>male</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender race/ethnicity parental level of education         lunch  \\\n",
       "0    female        group C            some high school  free/reduced   \n",
       "1    female        group B                 high school  free/reduced   \n",
       "2    female        group B            some high school  free/reduced   \n",
       "3    female        group B                some college      standard   \n",
       "4    female        group C                some college  free/reduced   \n",
       "..      ...            ...                         ...           ...   \n",
       "995  female        group E           bachelor's degree      standard   \n",
       "996    male        group A                some college      standard   \n",
       "997    male        group D                some college      standard   \n",
       "998    male        group E           bachelor's degree      standard   \n",
       "999  female        group E          associate's degree      standard   \n",
       "\n",
       "    test preparation course  math score  reading score  writing score  \n",
       "0                      none           0             17             10  \n",
       "1                      none           8             24             23  \n",
       "2                      none          18             32             28  \n",
       "3                      none          19             38             32  \n",
       "4                      none          22             39             33  \n",
       "..                      ...         ...            ...            ...  \n",
       "995                    none         100            100            100  \n",
       "996               completed         100             96             86  \n",
       "997               completed         100             97             99  \n",
       "998               completed         100            100            100  \n",
       "999                    none         100            100            100  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.orderBy(asc(\"math score\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fe147-2b19-43a3-af86-c8e85fb40f48",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc_nulls_first(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8bc2aecf-6e02-43a9-ab8f-3c908ecb82bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  0| NULL|\n",
      "|  2|Alice|\n",
      "|  1|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Bob\"),\n",
    "                             (0, None),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(asc_nulls_first(df1.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078833e-bcfb-4237-b206-771019b6b438",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc_nulls_last(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2f0eeb43-f6ec-4a45-9a2c-c18d8df3e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "|  1|  Bob|\n",
      "|  0| NULL|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(asc_nulls_last(df1.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08264c03-7481-476e-9e38-733497570554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------------+--------------------+-----+\n",
      "|     Player Name|Season|           Statistic|            Variable|Value|\n",
      "+----------------+------+--------------------+--------------------+-----+\n",
      "|   Jordan Spieth|  2018| Par 4 Eagle Leaders|Par 4 Eagle Leade...| NULL|\n",
      "|   Gary Woodland|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|Brendon de Jonge|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|      Jon Curran|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "| Charley Hoffman|  2017|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "| David Lingmerth|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Charlie Wi|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|    Patrick Reed|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|    Bubba Watson|  2013|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|    Zach Johnson|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Paul Casey|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|  Billy Horschel|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|       Luke List|  2017|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|      Paul Casey|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Ryan Moore|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|   Brooks Koepka|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|    Lee Westwood|  2014|Driving Pct. 240-...|Driving Pct. 240-...| NULL|\n",
      "|    Webb Simpson|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|     Sean O'Hair|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|        Kevin Na|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "+----------------+------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(asc(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "072ddcff-f89c-40e4-bce8-0db556ec3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------------------+--------------------+----------+\n",
      "|   Player Name|Season|           Statistic|            Variable|     Value|\n",
      "+--------------+------+--------------------+--------------------+----------+\n",
      "|   Chris Riley|  2010|Percentage of Ava...|Percentage of Ava...|$1,001,580|\n",
      "|   Chris Riley|  2010|Percentage of pot...|Percentage of pot...|$1,001,581|\n",
      "|   Chris Riley|  2010|Money per Event L...|Money per Event L...|$1,001,581|\n",
      "|Steve Stricker|  2017|      Official Money|Official Money - ...|$1,002,036|\n",
      "|Steve Stricker|  2017|Percentage of Ava...|Percentage of Ava...|$1,002,036|\n",
      "|Steve Stricker|  2017|Money per Event L...|Money per Event L...|$1,002,036|\n",
      "|Steve Stricker|  2017|Percentage of pot...|Percentage of pot...|$1,002,036|\n",
      "|  Robert Streb|  2016|Percentage of Ava...|Percentage of Ava...|$1,003,359|\n",
      "|  Robert Streb|  2016|Percentage of pot...|Percentage of pot...|$1,003,362|\n",
      "|  Robert Streb|  2016|Money per Event L...|Money per Event L...|$1,003,362|\n",
      "|  Robert Streb|  2016|Total Money (Offi...|Total Money (Offi...|$1,003,363|\n",
      "|  Robert Streb|  2016|      Official Money|Official Money - ...|$1,003,363|\n",
      "|      Jon Rahm|  2016|Percentage of Ava...|Percentage of Ava...|$1,004,033|\n",
      "|      Jon Rahm|  2016|Percentage of pot...|Percentage of pot...|$1,004,034|\n",
      "|      Jon Rahm|  2016|Money per Event L...|Money per Event L...|$1,004,034|\n",
      "|      Jon Rahm|  2016|Total Money (Offi...|Total Money (Offi...|$1,004,035|\n",
      "|      Jon Rahm|  2016|      Official Money|Official Money - ...|$1,004,035|\n",
      "|Brendan Steele|  2013|Percentage of Ava...|Percentage of Ava...|$1,004,159|\n",
      "|Brendan Steele|  2013|Percentage of pot...|Percentage of pot...|$1,004,160|\n",
      "|Brendan Steele|  2013|Money per Event L...|Money per Event L...|$1,004,160|\n",
      "+--------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(asc_nulls_last(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0115a-be99-4225-9735-016e2a8c5340",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b1a6930e-6106-45a0-8cab-8249173a009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).orderBy(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b22e20b3-ecf6-4509-a810-e76278c23ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|       Player Name|Season|           Statistic|            Variable|               Value|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|    Soren Kjeldsen|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Jason Dufner|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Vijay Singh|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|     Scott Gregory|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Ryan Ruffels|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Andrew Dorn|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|         John Hahn|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Kenny Perry|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Harry Ellis|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Andrew Svoboda|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|        Chris Kirk|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|     Justin Thomas|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|      Will Claxton|  2012|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|     Gary Woodland|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|Charles Howell III|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|      Jason Dufner|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|   Robert Karlsson|  2011|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Justin Leonard|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|Charles Howell III|  2013|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(desc(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959cc72-7c75-4333-a9c5-d01837d06608",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc_nulls_first(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0b57cc6a-4ad3-4101-87ea-602f4b4013f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  0| NULL|\n",
      "|  1|  Bob|\n",
      "|  2|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(desc_nulls_first(df1.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4521b41-ce7b-46d6-9e13-1bc41ed451be",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc_nulls_last(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3e2c40a7-1b60-4c47-8399-1d394f0edc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|  Bob|\n",
      "|  2|Alice|\n",
      "|  0| NULL|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(desc_nulls_last(df1.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "35ebf309-69f0-403c-8c4c-67d939738b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|       Player Name|Season|           Statistic|            Variable|               Value|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|      Jason Dufner|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Vijay Singh|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Soren Kjeldsen|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Andrew Dorn|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|     Scott Gregory|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Kenny Perry|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Ryan Ruffels|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Harry Ellis|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|         John Hahn|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Andrew Svoboda|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Troy Matteson|  2011|      Longest Drives|Longest Drives - ...|the Memorial Tour...|\n",
      "|     Justin Thomas|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|      Chad Collins|  2010|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Gary Woodland|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Kevin Stadler|  2011|      Longest Drives|Longest Drives - ...|the Memorial Tour...|\n",
      "|      Jason Dufner|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|Charles Howell III|  2013|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2011|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|        Adam Scott|  2010|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(desc_nulls_last(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916483f-91a2-43b5-b624-6fa9eb4103ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [String Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#string-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a405d4-40c7-472d-9c0b-7553c0ce6460",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ascii(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the numeric value of the first character of the string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "73586068-bf91-49a4-85ce-94922b67a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|ascii(value)|\n",
      "+------------+\n",
      "|          83|\n",
      "|          80|\n",
      "|          80|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(ascii(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080cdbd-2dbd-491a-92bf-f0290cbe4838",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.base64(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the BASE64 encoding of a binary column and returns it as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8fe35e49-cfbf-457b-a0b9-9ed984f619e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   base64(value)|\n",
      "+----------------+\n",
      "|        U3Bhcms=|\n",
      "|    UHlTcGFyaw==|\n",
      "|UGFuZGFzIEFQSQ==|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(base64(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d40bb5-5986-4a99-b6dd-a6681fde784a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the bit length for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "01b90731-32f1-4a4e-acec-ee620bc1e496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bit_length(cat)=24), Row(bit_length(cat)=32), Row(bit_length(cat)=48)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('cat',), ( '🐈',), ( 'кіт',)], ['cat'])\n",
    "df.select(bit_length('cat')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d43294-5110-478c-afce-293c89b0ed2b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.btrim(str: ColumnOrName, trim: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Remove the leading and trailing trim characters from str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "33a4e53e-785f-4771-a96c-b07198c874ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='parkSQ')]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"SSparkSQLS\", \"SL\", )], ['a', 'b'])\n",
    "df.select(btrim(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "48c4d90a-b841-459d-9748-c2d1d66ec62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='SparkSQL')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"    SparkSQL   \",)], ['a'])\n",
    "df.select(btrim(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829f7c1-82b2-4dc6-ae96-58d29460aeb0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.char(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the ASCII character having the binary equivalent to col. If col is larger than 256 the result is equivalent to char(col % 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "044c35c3-5ce2-4345-be9f-01290390d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|char(65)|\n",
      "+--------+\n",
      "|       A|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(char(lit(65))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cedd1c-90dd-4cf2-9a7f-6be9e6eb1035",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.character_length(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d8753ce7-eb44-4895-abf6-1a2efacce3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|character_length(SparkSQL)|\n",
      "+--------------------------+\n",
      "|                         8|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sf.character_length(sf.lit(\"SparkSQL\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d4380-cae7-4e82-ac1d-1c768e82ac8d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.char_length(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fa04afee-6f0c-41c5-97ac-599fddde280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|char_length(SparkSQL)|\n",
      "+---------------------+\n",
      "|                    8|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(char_length(lit(\"SparkSQL\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30741cf4-ab9c-4be3-b048-ab7f4486d3ce",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.concat_ws(sep: str, *cols: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "45d34ecb-ff0e-4e95-ae5b-de9533e9cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd-123')]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat_ws('-', df.s, df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b0ab0-1f90-4de6-9f46-964b291b0bd7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.contains(left: ColumnOrName, right: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if right is found inside left. Returns NULL if either input expression is NULL. Otherwise, returns False. Both left or right must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c55aba-132a-45eb-8fa6-e798192d070f",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- left: Column or str | \n",
    "The input column or strings to check, may be NULL.\n",
    "- right: Column or str | \n",
    "The input column or strings to find, may be NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1564d5fa-a8ea-4df8-b426-cbf9fad8ac66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n",
    "df.select(contains(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4a85e6da-3c7d-4ac2-b410-13cf8bed2f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c: binary (nullable = true)\n",
      " |-- d: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"c\", \"d\"])\n",
    "df = df.select(to_binary(\"c\").alias(\"c\"), to_binary(\"d\").alias(\"d\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "38ebaf26-377d-494a-98f2-104995c18d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|contains(c, d)|contains(d, c)|\n",
      "+--------------+--------------+\n",
      "|          true|         false|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(contains(\"c\", \"d\"), contains(\"d\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4637f-74d9-45ab-a514-42db41e8547f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.decode(col: ColumnOrName, charset: str) → pyspark.sql.column.Column\n",
    "Computes the first argument into a string from a binary using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "28b1f67a-f0de-4be0-8af9-9aa6c26113ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|decode(a, UTF-8)|\n",
      "+----------------+\n",
      "|            abcd|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['a'])\n",
    "df.select(decode(\"a\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "136d04b4-e011-48b9-930a-962b114d5225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|decode(a, UTF-8)|\n",
      "+----------------+\n",
      "| кіт кит дельфін|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('кіт кит дельфін',)], ['a'])\n",
    "df.select(decode(\"a\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece79dd-cba9-4485-8429-34e477135a9e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.elt(*inputs: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the n-th input, e.g., returns input2 when n is 2. The function returns NULL if the index exceeds the length of the array and spark.sql.ansi.enabled is set to false. If spark.sql.ansi.enabled is set to true, it throws ArrayIndexOutOfBoundsException for invalid indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "13c64c3e-0dbd-4bc4-ae90-63fdaf6b82e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='scala')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1ff60cb8-63c2-46e6-9228-1cd9dbbf5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='scala')]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "53df6d43-0903-4dff-8e01-cd11d62b139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='java')]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ggg\", \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(lit(2), df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0400b2e2-df93-42b5-bedb-f0b19ca765c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None, r=None)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(elt(lit(0), df.b, df.c).alias('r'), elt(lit(3), df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ca776-3273-4460-8036-944df318a445",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.encode(col: ColumnOrName, charset: str) → pyspark.sql.column.Column\n",
    "Computes the first argument into a binary from a string using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e490f32d-91fb-4754-9fac-c9d9da2bf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|encode(c, UTF-8)|\n",
      "+----------------+\n",
      "|   [61 62 63 64]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['c'])\n",
    "df.select(encode(\"c\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3039ed6e-a197-44d0-93c5-9553e5f2ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------+\n",
      "|encode                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|[FE FF 04 3A 04 56 04 42 00 20 04 3A 04 38 04 42 00 20 04 34 04 35 04 3B 04 4C 04 44 04 56 04 3D]|\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+----------------------+\n",
      "|decode(encode, UTF-16)|\n",
      "+----------------------+\n",
      "|       кіт кит дельфін|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('кіт кит дельфін',)], ['c'])\n",
    "df = df.select(encode(\"c\", \"UTF-16\").alias(\"encode\"))\n",
    "df.show(truncate=False)\n",
    "df.select(decode(\"encode\", \"UTF-16\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0c692-c3fa-4d58-a870-7de76b1351b5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.endswith(str: ColumnOrName, suffix: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if str ends with suffix. Returns NULL if either input expression is NULL. Otherwise, returns False. Both str or suffix must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "75af7500-b568-4731-898f-4673f54a205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=False)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
    "df.select(endswith(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "841a72ea-55fc-4483-8521-91611dd622cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- e: binary (nullable = true)\n",
      " |-- f: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"e\", \"f\"])\n",
    "df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "867dd8a0-8fde-4fcb-8850-c70d341ec9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|endswith(e, f)|endswith(f, e)|\n",
      "+--------------+--------------+\n",
      "|          true|         false|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(endswith(\"e\", \"f\"), endswith(\"f\", \"e\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc654ba-6b28-4190-a8bc-658ce4748b0a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.find_in_set(str: ColumnOrName, str_array: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the index (1-based) of the given string (str) in the comma-delimited list (strArray). Returns 0, if the string was not found or if the given string (str) contains a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ad8dcbe7-c0b4-437e-8c80-3a7591ab3459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ab\", \"abc,b,ab,c,def\")], ['a', 'b'])\n",
    "df.select(find_in_set(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80de42-ec13-4e69-9b2a-2e802fa3bc85",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.format_number(col: ColumnOrName, d: int) → pyspark.sql.column.Column\n",
    "Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "40e5bea4-8a70-44fc-ac4c-e89fd2174a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5.0000')]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c05005-8dea-4a04-bce7-134f7df519f6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.format_string(format: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Formats the arguments in printf-style and returns the result as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b192f663-9989-4d6d-bc3f-4090c9b61aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5 hello')]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
    "df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879684b5-dcc9-4556-a60f-d721c949534c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ilike(str: ColumnOrName, pattern: ColumnOrName, escapeChar: Optional[Column] = None) → pyspark.sql.column.Column\n",
    "Returns true if str matches pattern with escape case-insensitively, null if any arguments are null, false otherwise. The default escape character is the ‘’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862249fe-4179-4d8b-b60d-7160829d8597",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- str: Column or str | \n",
    "A string.\n",
    "- pattern: Column or str |\n",
    "A string. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order to match “\u0007bc”, the pattern should be “abc”. When SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back to Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the pattern to match “\u0007bc” should be “\u0007bc”.\n",
    "- escape: Column |\n",
    "An character added since Spark 3.0. The default escape character is the ‘’. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "dae24b4f-cfa4-4eb0-92e5-5d1a83d78477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
    "df.select(ilike(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ce69036d-6c93-40a6-9bb8-3c8641490ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
    "    ['a', 'b']\n",
    ")\n",
    "df.select(ilike(df.a, df.b, lit('/')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdefa7-2306-4d5d-8bd8-6805bf9eb43e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.initcap(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Translate the first letter of each word to upper case in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c96ee3e8-fc22-4a3f-aa12-68b7fc79936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='Ab Cd')]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e97d88-94ad-40c7-b63e-901a7eca6e8a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.instr(str: ColumnOrName, substr: str) → pyspark.sql.column.Column\n",
    "Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a594be7-6755-4620-ab82-fafabfd3c82e",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9700e872-b8fb-4de3-aa60-cbe527dedf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=2)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(instr(df.s, 'b').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cce506-89e8-4b8e-8d6c-e95b822617e8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lcase(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns str with all characters changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8c59d7d6-287b-4330-8957-48bfcb247e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|lcase(Spark)|\n",
      "+------------+\n",
      "|       spark|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sf.lcase(sf.lit(\"Spark\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949877f-77e8-4ab9-9582-1e84d2b9c56d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "cf6a1d99-6636-4f2d-80c4-8a7ea28045aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(length=4)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feecca4-5abb-48ba-97b0-3751fd6333ea",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.like(str: ColumnOrName, pattern: ColumnOrName, escapeChar: Optional[Column] = None) → pyspark.sql.column.Column\n",
    "Returns true if str matches pattern with escape, null if any arguments are null, false otherwise. The default escape character is the ‘’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a80e2b-0b3b-4ff5-88bb-d267287d48ec",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str |\n",
    "A string.\n",
    "- pattern: Column or str | \n",
    "A string. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order to match “\u0007bc”, the pattern should be “abc”. When SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back to Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the pattern to match “\u0007bc” should be “\u0007bc”.\n",
    "- escape: Column |\n",
    "An character added since Spark 3.0. The default escape character is the ‘’. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "59498fc2-a2de-4296-8281-5f0998fafa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
    "df.select(like(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1410c45c-0c5b-48f7-b8a1-71066e0f39fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
    "    ['a', 'b']\n",
    ")\n",
    "df.select(like(df.a, df.b, lit('/')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993e112-61e8-4a70-80f5-9ad3a1c4aaa8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lower(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Converts a string expression to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9fe4a1fa-d3ad-42e6-a2aa-196bedf0c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|lower(value)|\n",
      "+------------+\n",
      "|       spark|\n",
      "|     pyspark|\n",
      "|  pandas api|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(lower(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b82f3-c660-41fc-96b0-dc76f602be9f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.left(str: ColumnOrName, len: ColumnOrName) → pyspark.sql.column.Column\n",
    "    Returns the leftmost len`(`len can be string type) characters from the string str, if len is less or equal than 0 the result is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c0414682-1823-4f44-8390-e586be41db21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spa')]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
    "df.select(left(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a15683-c93f-48a6-94b5-44b6bb4a50dd",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.levenshtein(left: ColumnOrName, right: ColumnOrName, threshold: Optional[int] = None) → pyspark.sql.column.Column\n",
    "Computes the Levenshtein distance of the two given strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdb7f8-b658-45b8-b0b5-0f2f58af2490",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- leftColumn or str\n",
    "first column value.\n",
    "- right: Column or str | \n",
    "second column value.\n",
    "- threshold: int, optional | \n",
    "if set when the levenshtein distance of the two given strings less than or equal to a given threshold then return result distance, or -1\n",
    "\n",
    "Returns – Column | Levenshtein distance as integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "83e8e60c-2c32-460a-a9d6-18793f63917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
    "df0.select(levenshtein('l', 'r').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "323e1cc6-bcae-4ca4-994d-d5eafd9ae2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=-1)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.select(levenshtein('l', 'r', 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f0cd-520c-4615-ae10-dab27bd60fc7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.locate(substr: str, str: ColumnOrName, pos: int = 1) → pyspark.sql.column.Column\n",
    "Locate the position of the first occurrence of substr in a string column, after position pos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996d059-2e65-4226-bb39-aee1b7661186",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a342a11e-071e-45b0-aff8-7574bd2a4ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=2)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(locate('b', df.s, 1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad19392-f4cc-481e-bd99-5756381f7f11",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lpad(col: ColumnOrName, len: int, pad: str) → pyspark.sql.column.Column\n",
    "Left-pad the string column to width len with pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "53307981-37ac-4288-99a5-dcead0a24ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='##abcd')]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(lpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4af9e7a0-76ff-4ed5-99ad-115536fe20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   s|\n",
      "+--------------------+\n",
      "|~~~~~Robert Garrigus|\n",
      "|~~~~~~~~Bubba Watson|\n",
      "|~~~~~~Dustin Johnson|\n",
      "|~~~~~Brett Wetterich|\n",
      "|~~~~~~~~~J.B. Holmes|\n",
      "|~~~~~~~~~~~John Daly|\n",
      "|~~~~~~~Graham DeLaet|\n",
      "|~~~~~~~Angel Cabrera|\n",
      "|~~~~~~Charles Warren|\n",
      "|~~~~~~~~~D.J. Trahan|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tour.limit(10).select(lpad(tour[\"Player name\"], 20, '~').alias('s')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97ad6f-a2eb-4515-bafc-16086375f3e8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ltrim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from left end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d3807d84-622b-4be6-8432-6e3181554a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|      r|length|\n",
      "+-------+------+\n",
      "|  Spark|     5|\n",
      "|Spark  |     7|\n",
      "|  Spark|     5|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15580a-701b-400c-935d-5b5db5fc399c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.mask(col: ColumnOrName, upperChar: Optional[ColumnOrName] = None, lowerChar: Optional[ColumnOrName] = None, digitChar: Optional[ColumnOrName] = None, otherChar: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Masks the given string value. This can be useful for creating copies of tables with sensitive information removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728a271-feed-4ae0-b9ef-869712aed9b4",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "    col: :class:`~pyspark.sql.Column` or str\n",
    "        target column to compute on.\n",
    "\n",
    "    upperChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace upper-case characters with. Specify NULL to retain original character.\n",
    "\n",
    "    lowerChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace lower-case characters with. Specify NULL to retain original character.\n",
    "\n",
    "    digitChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace digit characters with. Specify NULL to retain original character.\n",
    "\n",
    "    otherChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace all other characters with. Specify NULL to retain original character.\n",
    "\n",
    "Returns | Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8cbc4fc2-29a8-431c-ab17-350f70958128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='XxXXnnn-@$#'), Row(r='xxxx-XXXX-nnnn-nnnn')]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"AbCD123-@$#\",), (\"abcd-EFGH-8765-4321\",)], ['data'])\n",
    "df.select(mask(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "47f7dd92-985c-476b-b2e9-13970d45b1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YxYYnnn-@$#'), Row(r='xxxx-YYYY-nnnn-nnnn')]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e4baca6d-305c-4f26-b660-b3a59087b0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYnnn-@$#'), Row(r='yyyy-YYYY-nnnn-nnnn')]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "610d2584-dc8f-47f2-b9b1-26e7965b0bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYddd-@$#'), Row(r='yyyy-YYYY-dddd-dddd')]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y'), lit('d')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "7dfbe8d3-e6e1-4dc7-8f92-9c9c47d72cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYddd****'), Row(r='yyyy*YYYY*dddd*dddd')]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y'), lit('d'), lit('*')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "edef701c-5bdb-4fec-bacd-ee14e2d9d066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='XxXXnnn-@$#'),\n",
       " Row(r='xxxx-XXXX-nnnn-nnnn'),\n",
       " Row(r='nnnn-nnnn-nnnn-nnnn'),\n",
       " Row(r='nnnnnnnn'),\n",
       " Row(r='Xxxxx Xxxxxxxx'),\n",
       " Row(r=\"Xxxx_nnnn_!'№;_xxxx\"),\n",
       " Row(r='x[XXn&[&xxxx')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"AbCD123-@$#\",),\n",
    "                            (\"abcd-EFGH-8765-4321\",),\n",
    "                            (\"1234-5678-8765-4321\",),\n",
    "                            (\"12345678\",),\n",
    "                            (\"Абаба Галамага\",),\n",
    "                            (\"Тест_1234_!'№;_test\",),\n",
    "                            (\"k[MY0&[&jwts\",),\n",
    "                           ], ['data'])\n",
    "df.select(mask(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fff05c-b57e-4e23-8059-304f0d079c98",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.octet_length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the byte length for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "41fc574c-6706-4af5-a468-2590f58b72ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(octet_length(cat)=3), Row(octet_length(cat)=4)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
    "     .select(octet_length('cat')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894a794-bab5-4022-b667-9eeb5c095cf0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.parse_url(url: ColumnOrName, partToExtract: ColumnOrName, key: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Extracts a part from a URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361734c-1c8e-410c-ad78-e0573b19356f",
   "metadata": {},
   "source": [
    "    'HOST': Домен (наприклад, www.example.com).\n",
    "    'PATH': Шлях (наприклад, /page1).\n",
    "    'QUERY': Рядок запиту (наприклад, ?param1=value1&param2=value2).\n",
    "    'SCHEME': Протокол (наприклад, http).\n",
    "    'FILE': Шлях до файлу.\n",
    "    'REF': Фрагмент (якщо він вказаний у URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1d3725eb-0074-40cf-bc38-1e85c21c2948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='1')]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"http://spark.apache.org/path?query=1\", \"QUERY\", \"query\",)],\n",
    "    [\"a\", \"b\", \"c\"]\n",
    ")\n",
    "df.select(parse_url(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "0834cf30-875a-449d-b2eb-bf87b91c814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='query=1')]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "0de29889-dff4-4680-a0c2-e57ef3d99022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='/path')]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, lit(\"PATH\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "6f736315-4716-4c5a-b5fc-0888803966a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='spark.apache.org')]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, lit(\"HOST\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f153ee-8f2f-4ee1-9701-f709ee490f42",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.position(substr: ColumnOrName, str: ColumnOrName, start: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Returns the position of the first occurrence of substr in str after position start. The given start and return value are 1-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b495d2bd-fbc1-4969-ac96-0be848cf21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|position(a, b, c)|\n",
      "+-----------------+\n",
      "|                7|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
    ").select(position(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "4e7f65b6-e687-45dd-aec4-64c3af8897cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|position(a, b, 1)|\n",
      "+-----------------+\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
    ").select(position(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137f345-f98c-46f9-bf55-c79ffcf501f7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.printf(format: ColumnOrName, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Formats the arguments in printf-style and returns the result as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6581d0f1-d978-4dbe-bb9d-055bfd0018d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|printf(a, b, c)|\n",
      "+---------------+\n",
      "|     aa123ccqwe|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"aa%d%sqwe\", 123, \"cc\",)], [\"a\", \"b\", \"c\"]\n",
    ").select(printf(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90846aa1-a290-4429-8ddd-0aa6a9820ce2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rlike(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f517d203-9f4e-4496-9617-0572d8228cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=True)]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"])\n",
    "df.select(rlike('str', lit(r'(\\d+)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "f03c3ddd-5dbe-45f0-b6ea-edec9eb406b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=False)]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(rlike('str', lit(r'\\d{2}b')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ca8b8061-ac33-4802-832f-fc8b65dba6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=True)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(rlike(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810f694-381a-497b-b0ff-9e8ba9d80e8f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606c633-30db-4ee3-8298-f90e3d82fca9",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str | \n",
    "target column to work on.\n",
    "- regexp: Column or str | \n",
    "regex pattern to apply.\n",
    "\n",
    "Returns – Column | true if str matches a Java regex, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "9d13a52e-7692-4f95-81eb-6b57025729cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|REGEXP(str, (\\d+))|\n",
      "+------------------+\n",
      "|              true|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', lit(r'(\\d+)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6fc92011-17fe-4adf-b473-a636f4393ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|REGEXP(str, \\d{2}b)|\n",
      "+-------------------+\n",
      "|              false|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', lit(r'\\d{2}b'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4def6e69-1013-405a-aa18-f085e09ba5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|REGEXP(str, regexp)|\n",
      "+-------------------+\n",
      "|               true|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', col(\"regexp\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa87da-100c-4408-9738-033e1917586a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_like(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e35e7b-50e1-4ff3-8aed-cf31c4d28cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|REGEXP_LIKE(str, (\\d+))|\n",
      "+-----------------------+\n",
      "|                   true|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', lit(r'(\\d+)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8452d170-08f8-4ec7-971c-9fc6965e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|REGEXP_LIKE(str, \\d{2}b)|\n",
      "+------------------------+\n",
      "|                   false|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', lit(r'\\d{2}b'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b945518-74d4-4e11-8b83-56f471c73ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|REGEXP_LIKE(str, regexp)|\n",
      "+------------------------+\n",
      "|                    true|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', col(\"regexp\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f9139-cb66-4bc0-8677-31b91e465a61",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_count(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a count of the number of times that the Java regex pattern regexp is matched in the string str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6243a3-8aa2-4951-9a87-50229013a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_count('str', lit(r'\\d+')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dbe0135-6ee0-4762-be03-2f65bc94e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_count('str', lit(r'mmm')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470ae498-5132-4ee7-9796-56ab41fb4b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_count(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec13ad-a251-4e33-ae93-a1dff8c91f17",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_extract(str: ColumnOrName, pattern: str, idx: int) → pyspark.sql.column.Column\n",
    "Extract a specific group matched by the Java regex regexp, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e7b9e2-755f-4994-afe8-badc906c942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='100')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e50677-fee7-40fc-804a-2c4c25ea0ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('foo',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574c49fd-32ea-46ea-8991-da5180555079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('aaaac',)], ['str'])\n",
    "df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4502b-92f1-4e76-a7e4-537406085683",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_extract_all(str: ColumnOrName, regexp: ColumnOrName, idx: Union[int, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Extract all strings in the str that match the Java regex regexp and corresponding to the regex group index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3239181-99eb-43aa-92dc-bcd2ed0dfb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"100-200, 300-400\", r\"(\\d+)-(\\d+)\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f39e61-1677-4842-ab0f-c2a7c66225b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 1).alias('d')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1980467f-c500-4859-93b0-86fd70918a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['200', '400'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 2).alias('d')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06af0978-da79-4a36-a06c-096bb216f8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b130c-7b7c-48ca-a68f-68f9e61c5293",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_replace(string: ColumnOrName, pattern: Union[str, pyspark.sql.column.Column], replacement: Union[str, pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Replace all substrings of the specified string value that match regexp with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a57c72-13d8-4eb1-bc60-5e700fb8b91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='-----')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"100-200\", r\"(\\d+)\", \"--\")], [\"str\", \"pattern\", \"replacement\"])\n",
    "df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7401b823-c26f-4235-8334-e7c70964ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='-----')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_replace(\"str\", col(\"pattern\"), col(\"replacement\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06719ad5-0f72-411b-b492-fb158136cca7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_substr(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the substring that matches the Java regex regexp within the string str. If the regular expression is not found, the result is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c78866-f468-4847-a06e-b7f652946501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='1')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_substr('str', lit(r'\\d+')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a659e121-222d-42ae-b66b-af0b258a58f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_substr('str', lit(r'mmm')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877f7520-2cfa-40cf-b211-c9f73d03e6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='1')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_substr(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4b3d9-f611-40f6-83de-d82e1580c997",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_instr(str: ColumnOrName, regexp: ColumnOrName, idx: Union[int, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Extract all strings in the str that match the Java regex regexp and corresponding to the regex group index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645aaa2a-abe7-459c-a23c-ace91f5a8157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+(a|b|m)\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335a05a4-3391-4d70-8555-e049ff69bcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55752538-79e8-4678-ba32-b41a0be96bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8333d08f-b9eb-4908-968f-29da15cd39d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e7adc-6798-4b72-a2da-449d67cfd878",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.replace(src: ColumnOrName, search: ColumnOrName, replace: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Replaces all occurrences of search with replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4875b5fd-9579-4b2d-af35-52414e19272f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='ABCDEF')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ABCabc\", \"abc\", \"DEF\",)], [\"a\", \"b\", \"c\"])\n",
    "df.select(replace(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d35333d-647c-41b9-82be-7a73307d3d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='ABC')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(replace(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007d0a5-cef8-4f14-a794-9f86d4ad97c3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.right(str: ColumnOrName, len: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "    Returns the rightmost len`(`len can be string type) characters from the string str, if len is less or equal than 0 the result is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fe076ed-4707-4c27-869c-0a676d5fec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='SQL')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
    "df.select(right(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07983af-5996-4566-b638-ce9a329ed12a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ucase(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns str with all characters changed to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea206ba2-d64f-401f-8af2-f4ebf51a7236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|ucase(Spark)|\n",
      "+------------+\n",
      "|       SPARK|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(ucase(lit(\"Spark\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c164206-b7a7-4e81-8e0f-ad52613735e4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.unbase64(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Decodes a BASE64 encoded string column and returns it as a binary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4da8fef0-7843-4e5f-b301-4c5aea759e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     unbase64(value)|\n",
      "+--------------------+\n",
      "|    [53 70 61 72 6B]|\n",
      "|[50 79 53 70 61 7...|\n",
      "|[50 61 6E 64 61 7...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"U3Bhcms=\",\n",
    "                            \"UHlTcGFyaw==\",\n",
    "                            \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n",
    "df.select(unbase64(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71262f5-94c8-4ee0-bcc4-74b924f9d054",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rpad(col: ColumnOrName, len: int, pad: str) → pyspark.sql.column.Column\n",
    "Right-pad the string column to width len with pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77d9b861-625d-47fc-8d6f-55c858e31e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd##')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(rpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37306412-bf4a-4f4d-ab85-36549aca8912",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.repeat(col: ColumnOrName, n: int) → pyspark.sql.column.Column\n",
    "Repeats a string column n times, and returns it as a new string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e46ff79f-3d02-47e8-b701-a843e833cdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='ababab')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ab',)], ['s',])\n",
    "df.select(repeat(df.s, 3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c3cc-41d3-4adb-b886-072bfd32a41e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rtrim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from right end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b82fbf3b-c244-4db2-bbdf-485ac6151035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|       r|length|\n",
      "+--------+------+\n",
      "|   Spark|     8|\n",
      "|   Spark|     5|\n",
      "|   Spark|     6|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afb40e-9407-4975-b6c4-0af5c233ca04",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.soundex(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the SoundEx encoding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c034022e-842d-47b7-a67f-bcfedb6bdc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(soundex='P362'), Row(soundex='U612')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
    "df.select(soundex(df.name).alias(\"soundex\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa79fa-31a2-4356-8498-a2a75cba85ed",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.split(str: ColumnOrName, pattern: str, limit: int = - 1) → pyspark.sql.column.Column\n",
    "Splits str around matches of the given pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d9afe-47c8-462a-a034-1c51c3f9f423",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str | \n",
    "a string expression to split\n",
    "- pattern: str | \n",
    "a string representing a regular expression. The regex string should be a Java regular expression.\n",
    "- limit: int, optional | \n",
    "an integer which controls the number of times pattern is applied.\n",
    "    * limit > 0: The resulting array’s length will not be more than limit, and the\n",
    "resulting array’s last entry will contain all input beyond the last matched pattern.\n",
    "    * limit <= 0: pattern will be applied as many times as possible, and the resulting\n",
    "array can be of any size.\n",
    "\n",
    "Returns – Column | \n",
    "array of separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f804cd54-e963-4b77-aaf1-1fde2b97aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=['one', 'twoBthreeC'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
    "df.select(split(df.s, '[ABC]', 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d041b1dd-90d3-4e54-8652-ddf2496468c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=['one', 'two', 'three', ''])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(split(df.s, '[ABC]', -1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d44796-6125-4a38-82d4-7a8014c066fc",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.split_part(src: ColumnOrName, delimiter: ColumnOrName, partNum: ColumnOrName) → pyspark.sql.column.Column\n",
    "Splits str by delimiter and return requested part of the split (1-based). If any input is null, returns null. if partNum is out of range of split parts, returns empty string. If partNum is 0, throws an error. If partNum is negative, the parts are counted backward from the end of the string. If the delimiter is an empty string, the str is not split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5473c9df-58f1-4a21-a2fa-5be85ae11dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='13')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"11.12.13\", \".\", 3,)], [\"a\", \"b\", \"c\"])\n",
    "df.select(split_part(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648894-8944-409e-989c-eb153d06d6f1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.startswith(str: ColumnOrName, prefix: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if str starts with prefix. Returns NULL if either input expression is NULL. Otherwise, returns False. Both str or prefix must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43e7b20a-0c20-4501-b28b-dcacd17bbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
    "df.select(startswith(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64d0b253-cfc9-4b22-a402-4db19335351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- e: binary (nullable = true)\n",
      " |-- f: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4142\",)], [\"e\", \"f\"])\n",
    "df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfa1de02-e176-42cf-8146-59655f2e51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|startswith(e, f)|startswith(f, e)|\n",
      "+----------------+----------------+\n",
      "|            true|           false|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(startswith(\"e\", \"f\"), startswith(\"f\", \"e\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41689e-2bf5-43d4-859c-2c3032697fc4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substr(str: ColumnOrName, pos: ColumnOrName, len: Optional[ColumnOrName] = None) → pyspark.sql.column.Column¶\n",
    "Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceed9698-2d17-49b2-9f54-a70916ace338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|substr(a, b, c)|\n",
      "+---------------+\n",
      "|              k|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
    ").select(substr(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea66e77d-e119-4429-b6f7-6a93e426a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|substr(a, b, 2147483647)|\n",
      "+------------------------+\n",
      "|                   k SQL|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
    ").select(substr(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec889031-9f24-4224-bd7a-0936dd48039a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substring(str: ColumnOrName, pos: int, len: int) → pyspark.sql.column.Column\n",
    "Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71f041-333f-4431-a308-cb7ab79dfa31",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "104469ad-d73d-47a5-a292-d0e774e24b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='ab')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(substring(df.s, 1, 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d554ab-5d03-4c87-bb0b-e17c9036c7a0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substring_index(str: ColumnOrName, delim: str, count: int) → pyspark.sql.column.Column\n",
    "Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25bbfadf-8ae6-4112-8ade-daf7ea281c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='a.b')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
    "df.select(substring_index(df.s, '.', 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60a08534-c4ed-4283-9c24-1559063f70f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='b.c.d')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(substring_index(df.s, '.', -3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ec10b-0c37-450f-a087-8a1fa56187fe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.overlay(src: ColumnOrName, replace: ColumnOrName, pos: Union[ColumnOrName, int], len: Union[ColumnOrName, int] = - 1) → pyspark.sql.column.Column\n",
    "Overlay the specified portion of src with replace, starting from byte position pos of src and proceeding for len bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92deee66-5920-48b4-8454-e5206215ae79",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- src: Column or str | \n",
    "column name or column containing the string that will be replaced\n",
    "- replace: Column or str | \n",
    "column name or column containing the substitution string\n",
    "- pos: Column or str or int | \n",
    "column name, column, or int containing the starting position in src\n",
    "- len: Column or str or int, optional | \n",
    "column name, column, or int containing the number of bytes to replace in src string by ‘replace’ defaults to -1, which represents the length of the ‘replace’ string\n",
    "\n",
    "Returns – Column | \n",
    "string with replaced values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "405ccb20-e097-43ef-80c9-f77f5f0458d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_CORE')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
    "df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79ee9b01-8673-4d1b-b91a-2578549da599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_CORESQL')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(overlay(\"x\", \"y\", 7, 0).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3916e7ae-020f-4716-a0d2-bf82e8d4d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_COREL')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(overlay(\"x\", \"y\", 7, 2).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7973acb-becf-4a8d-b764-b35321bee353",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sentences(string: ColumnOrName, language: Optional[ColumnOrName] = None, country: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Splits a string into arrays of sentences, where each sentence is an array of words. The ‘language’ and ‘country’ arguments are optional, and if omitted, the default locale is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c65828df-a257-479c-82fb-b49866152874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|sentences(string, en, US)          |\n",
      "+-----------------------------------+\n",
      "|[[This, is, an, example, sentence]]|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\n",
    "df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd7c12ba-44f0-415e-b66e-03a2eb994ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|sentences(s, , )                 |\n",
      "+---------------------------------+\n",
      "|[[Hello, world], [How, are, you]]|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"Hello world. How are you?\"]], [\"s\"])\n",
    "df.select(sentences(\"s\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffd68a-d53d-4448-a08c-fe4c442cc8d9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_binary(col: ColumnOrName, format: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Converts the input col to a binary value based on the supplied format. The format can be a case-insensitive string literal of “hex”, “utf-8”, “utf8”, or “base64”. By default, the binary format for conversion is “hex” if format is omitted. The function returns NULL if at least one of the input parameters is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a772c500-632f-4b43-89ee-1aadd70ea24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'abc'))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
    "df.select(to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ffd1901-6d77-4ea8-b056-5574dec752ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'ABC'))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
    "df.select(to_binary(df.e).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf579e9d-e707-4177-8fe6-ea183747ff95",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_char(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Convert col to a string based on the format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d281f0ef-794d-475b-803d-939908f36888",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input value, generating a result string of the same length as the corresponding sequence in the format string. The result string is left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of the decimal value, starts with 0, and is before the decimal point. Otherwise, it is padded with spaces. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive values but ‘MI’ prints a space. ‘PR’: Only allowed at the end of the format string; specifies that the result string will be wrapped by angle brackets if the input value is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5d72722-6217-4037-9578-e414f857dbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='$78.12')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
    "df.select(to_char(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe377-474e-4609-9d8a-9200330caead",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_number(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Convert string ‘col’ to a number based on the string format ‘format’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdddf0-537a-4a95-943d-350a55286434",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input string. If the 0/9 sequence starts with 0 and is before the decimal point, it can only match a digit sequence of the same size. Otherwise, if the sequence starts with 9 or is after the decimal point, it can match a digit sequence that has the same or smaller size. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘col’ must match the grouping separator relevant for the size of the number. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ allows ‘-‘ but ‘MI’ does not. ‘PR’: Only allowed at the end of the format string; specifies that ‘col’ indicates a negative number with wrapping angled brackets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ff7f611-8757-4757-b415-d448bde0ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=Decimal('78.12'))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
    "df.select(to_number(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5a23c-5e0e-4ff7-ac42-8e24517d9b46",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_varchar(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column\n",
    "Convert col to a string based on the format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96757825-b2bc-44d6-9eeb-42be11f8bc2b",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input value, generating a result string of the same length as the corresponding sequence in the format string. The result string is left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of the decimal value, starts with 0, and is before the decimal point. Otherwise, it is padded with spaces. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive values but ‘MI’ prints a space. ‘PR’: Only allowed at the end of the format string; specifies that the result string will be wrapped by angle brackets if the input value is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "054bb93d-97ff-41f9-a8b3-d05e7a82347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='$78.12')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
    "df.select(to_varchar(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca88d41-d0dd-4ac7-a92e-6243d25f9d5d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.translate(srcCol: ColumnOrName, matching: str, replace: str) → pyspark.sql.column.Column\n",
    "A function translate any character in the srcCol by a character in matching. The characters in replace is corresponding to the characters in matching. Translation will happen whenever any character in the string is matching with the character in the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9a58a99-be4a-492f-b0db-f79550c6676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='1a2s3ae')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('translate',)], ['a']) \\\n",
    "     .select(translate('a', \"rnlt\", \"123\") \\\n",
    "     .alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfb0ff-32d0-4836-98b3-c15c10df39bf",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.trim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from both ends for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c03403e-d0ae-48f7-b3ea-6b0e873a6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|    r|length|\n",
      "+-----+------+\n",
      "|Spark|     5|\n",
      "|Spark|     5|\n",
      "|Spark|     5|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f73ae-250d-41a5-8add-ff3173337bb4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.upper(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Converts a string expression to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5924cae-fd19-46c2-82df-3b05ba89d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|upper(value)|\n",
      "+------------+\n",
      "|       SPARK|\n",
      "|     PYSPARK|\n",
      "|  PANDAS API|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(upper(\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c5b66ce-9a63-4586-b95c-952e2c3ea817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    Player name|upper(Player name)|\n",
      "+---------------+------------------+\n",
      "|Robert Garrigus|   ROBERT GARRIGUS|\n",
      "|   Bubba Watson|      BUBBA WATSON|\n",
      "| Dustin Johnson|    DUSTIN JOHNSON|\n",
      "|Brett Wetterich|   BRETT WETTERICH|\n",
      "|    J.B. Holmes|       J.B. HOLMES|\n",
      "|      John Daly|         JOHN DALY|\n",
      "|  Graham DeLaet|     GRAHAM DELAET|\n",
      "|  Angel Cabrera|     ANGEL CABRERA|\n",
      "| Charles Warren|    CHARLES WARREN|\n",
      "|    D.J. Trahan|       D.J. TRAHAN|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tour.limit(10).select(\"Player name\", upper(\"Player name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b68a9-1a74-4fce-8471-74af4ceaf1e3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.url_decode(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Decodes a str in ‘application/x-www-form-urlencoded’ format using a specific encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20eb61a0-0282-494f-9a66-722b613a20cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='https://spark.apache.org')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"a\"])\n",
    "df.select(url_decode(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90464bab-c420-4f64-be1f-063daf047065",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.url_encode(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Translates a string into ‘application/x-www-form-urlencoded’ format using a specific encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9717cf5-372d-47f0-89da-e38778ec8938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='https%3A%2F%2Fspark.apache.org')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"https://spark.apache.org\",)], [\"a\"])\n",
    "df.select(url_encode(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ec5e6-d7d7-44e3-8c79-6d2d9ba87156",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Bitwise Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#bitwise-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752813bb-5b08-49eb-8db7-dde98ada0870",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_count(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bc9f6bb0-504d-40d1-8d7d-187db4293adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|bit_count(c)|\n",
      "+------------+\n",
      "|           1|\n",
      "|           1|\n",
      "|           1|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(bit_count(\"c\")).show()\n",
    "# Returns Column | the number of bits that are set in the argument expr as\n",
    "# an unsigned 64-bit integer, or NULL if the argument is NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77ae61-4b0f-4781-9131-2850c57455fb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_get(col: ColumnOrName, pos: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the value of the bit (0 or 1) at the specified position. The positions are numbered from right to left, starting at zero. The position argument cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "426c3261-e5e0-4351-a88b-090e61be4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  2|  1|  0|\n",
      "+---+---+---+\n",
      "|  0|  0|  1|\n",
      "|  0|  0|  1|\n",
      "|  0|  1|  0|\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  1|  0|  1|\n",
      "|  1|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2],[3],[4],[5],[6],], [\"c\"])\n",
    "df.select(bit_get(\"c\", lit(2)).alias(\"2\"), bit_get(\"c\", lit(1)).alias(\"1\"), bit_get(\"c\", lit(0)).alias(\"0\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc915872-e8b9-46c1-a8a9-83704a71414a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.getbit(col: ColumnOrName, pos: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the value of the bit (0 or 1) at the specified position. The positions are numbered from right to left, starting at zero. The position argument cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "101dbab4-bdeb-4f84-a56c-39a89bd79c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|getbit(c, 1)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "spark.createDataFrame(\n",
    "    [[1], [1], [2]], [\"c\"]\n",
    ").select(sf.getbit(\"c\", sf.lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6b785-7ad8-433f-848e-3e62ac39c717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Call Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#call-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f3a09-27d0-48f9-b541-d525653b4ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.call_function(funcName: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Call a SQL function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a276f-b9da-4a84-8dfe-4def5c417595",
   "metadata": {},
   "source": [
    "`print(call_function.__doc__)`\n",
    "\n",
    "    Call a SQL function.\n",
    "\n",
    "    .. versionadded:: 3.5.0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    funcName : str\n",
    "        function name that follows the SQL identifier syntax (can be quoted, can be qualified)\n",
    "    cols : :class:`~pyspark.sql.Column` or str\n",
    "        column names or :class:`~pyspark.sql.Column`\\s to be used in the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`~pyspark.sql.Column`\n",
    "        result of executed function.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from pyspark.sql.functions import call_udf, col\n",
    "    >>> from pyspark.sql.types import IntegerType, StringType\n",
    "    >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "    >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "    >>> df.select(call_function(\"intX2\", \"id\")).show()\n",
    "    +---------+\n",
    "    |intX2(id)|\n",
    "    +---------+\n",
    "    |        2|\n",
    "    |        4|\n",
    "    |        6|\n",
    "    +---------+\n",
    "    >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "    >>> df.select(call_function(\"strX2\", col(\"name\"))).show()\n",
    "    +-----------+\n",
    "    |strX2(name)|\n",
    "    +-----------+\n",
    "    |         aa|\n",
    "    |         bb|\n",
    "    |         cc|\n",
    "    +-----------+\n",
    "    >>> df.select(call_function(\"avg\", col(\"id\"))).show()\n",
    "    +-------+\n",
    "    |avg(id)|\n",
    "    +-------+\n",
    "    |    2.0|\n",
    "    +-------+\n",
    "    >>> _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
    "    ... # doctest: +SKIP\n",
    "    >>> df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
    "    ... # doctest: +SKIP\n",
    "    +------------------------------------+\n",
    "    |spark_catalog.default.custom_avg(id)|\n",
    "    +------------------------------------+\n",
    "    |                               102.0|\n",
    "    +------------------------------------+\n",
    "    >>> df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()\n",
    "    ... # doctest: +SKIP\n",
    "    +------------------------------------+\n",
    "    |spark_catalog.default.custom_avg(id)|\n",
    "    +------------------------------------+\n",
    "    |                               102.0|\n",
    "    +------------------------------------+\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88647d6e-2fd0-4e1f-bc72-badce98a45f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|intX2(id)|\n",
      "+---------+\n",
      "|        2|\n",
      "|        4|\n",
      "|        6|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "_ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "df.select(call_function(\"intX2\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d5aa02-e975-469b-9234-cfe647c048ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|strX2(name)|\n",
      "+-----------+\n",
      "|         aa|\n",
      "|         bb|\n",
      "|         cc|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "df.select(call_function(\"strX2\", col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58db1daa-4852-4c82-945e-aae629c3e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    2.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(call_function(\"avg\", col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7528abf4-96b4-47b6-bdea-1ac66ab9e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
    "# df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
    "# df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b86237-19e5-4f92-8f04-99e5dcab40af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.call_udf(udfName: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Call an user-defined function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8f898-feec-4991-8c3c-6545d2c57894",
   "metadata": {},
   "source": [
    "`print(call_udf.__doc__)`\n",
    "\n",
    "    Call an user-defined function.\n",
    "\n",
    "    .. versionadded:: 3.4.0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    udfName : str\n",
    "        name of the user defined function (UDF)\n",
    "    cols : :class:`~pyspark.sql.Column` or str\n",
    "        column names or :class:`~pyspark.sql.Column`\\s to be used in the UDF\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`~pyspark.sql.Column`\n",
    "        result of executed udf.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from pyspark.sql.functions import call_udf, col\n",
    "    >>> from pyspark.sql.types import IntegerType, StringType\n",
    "    >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "    >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "    >>> df.select(call_udf(\"intX2\", \"id\")).show()\n",
    "    +---------+\n",
    "    |intX2(id)|\n",
    "    +---------+\n",
    "    |        2|\n",
    "    |        4|\n",
    "    |        6|\n",
    "    +---------+\n",
    "    >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "    >>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n",
    "    +-----------+\n",
    "    |strX2(name)|\n",
    "    +-----------+\n",
    "    |         aa|\n",
    "    |         bb|\n",
    "    |         cc|\n",
    "    +-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e29366c-fa71-4bf9-8091-04160381b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|intX2(math score)|\n",
      "+-----------------+\n",
      "|              144|\n",
      "|              138|\n",
      "|              180|\n",
      "|               94|\n",
      "|              152|\n",
      "|              142|\n",
      "|              176|\n",
      "|               80|\n",
      "|              128|\n",
      "|               76|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"intX1.5\", lambda i: i * 1.5, IntegerType())\n",
    "students.limit(10).select(call_udf(\"intX2\", \"math score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6623311b-2b1a-420e-9d44-837623a9fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 16:03:15 WARN SimpleFunctionRegistry: The function strx2 replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|strX2(Player name)            |\n",
      "+------------------------------+\n",
      "|Robert GarrigusRobert Garrigus|\n",
      "|Bubba WatsonBubba Watson      |\n",
      "|Dustin JohnsonDustin Johnson  |\n",
      "|Brett WetterichBrett Wetterich|\n",
      "|J.B. HolmesJ.B. Holmes        |\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "tour.limit(5).select(call_udf(\"strX2\", col(\"Player name\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee1340-23c1-46f1-8913-a34df8877ddb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)\n",
    "Creates a pandas user defined function (a.k.a. vectorized user defined function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa0e67-c938-4e22-b727-876298cd7763",
   "metadata": {},
   "source": [
    "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "Parameters:\n",
    "- f: function, optional |\n",
    "user-defined function. A python function if used as a standalone function\n",
    "- return: Typepyspark.sql.types.DataType or str, optional | \n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "- function: Typeint, optional | \n",
    "an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR. This parameter exists for compatibility. Using Python type hints is encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fac94-278b-4a7c-a49d-808e8213c7cd",
   "metadata": {},
   "source": [
    "[why] Get same errors with all code snippets. Probably unmatching versions of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e06563e5-4c24-4323-a1ed-ab5d4127a4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\n@pandas_udf(IntegerType())\\ndef slen(s: pd.Series) -> pd.Series:\\n    return s.str.len()\\n\\ndf = tour.select(\"Player name\")\\ndf.withColumn(\"Name_Length\", slen(df[\"Player name\"])).show()\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()\n",
    "\n",
    "df = tour.select(\"Player name\")\n",
    "df.withColumn(\"Name_Length\", slen(df[\"Player name\"])).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b33e61a4-7b73-44ee-ae7b-586edf3c0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "60c1988a-27f8-40b3-9677-0b54cdeab610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cd4dfb6f-98dd-4615-a9bc-70a49e834b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f8985236-40b5-4d47-a588-ac011bc08e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"string\")\\ndef to_upper(s: pd.Series) -> pd.Series:\\n    return s.str.upper()\\n\\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\\ndf.select(to_upper(\"name\")).show()\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(to_upper(\"name\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c95c1c9-c1dd-4670-935f-e1b1f35bc33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"first string, last string\")\\ndef split_expand(s: pd.Series) -> pd.DataFrame:\\n    return s.str.split(expand=True)\\n\\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\\ndf.select(split_expand(\"name\")).show()\\n'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"first string, last string\")\n",
    "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
    "    return s.str.split(expand=True)\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(split_expand(\"name\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a594f1d3-9425-4829-9cd5-53a2bddf7a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator\\n\\n@pandas_udf(\"long\")\\ndef calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\\n    # Do some expensive initialization with a state\\n    state = very_expensive_initialization()\\n    for x in iterator:\\n        # Use that state for whole iterator.\\n        yield calculate_with_state(x, state)\\n\\ndf.select(calculate(\"value\")).show()\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Do some expensive initialization with a state\n",
    "    state = very_expensive_initialization()\n",
    "    for x in iterator:\n",
    "        # Use that state for whole iterator.\n",
    "        yield calculate_with_state(x, state)\n",
    "\n",
    "df.select(calculate(\"value\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "16c65ced-99e9-455c-8d32-65066c727a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator\\n@pandas_udf(\"long\")\\ndef plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\\n    for s in iterator:\\n        yield s + 1\\n\\ndf = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\\ndf.select(plus_one(df.v)).show()\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for s in iterator:\n",
    "        yield s + 1\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.select(plus_one(df.v)).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "95ddeba3-ea31-4402-994e-2f84b2886818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator, Tuple\\nfrom pyspark.sql.functions import struct, col\\n@pandas_udf(\"long\")\\ndef multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\\n    for s1, df in iterator:\\n        yield s1 * df.v\\n\\ndf = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\\ndf.withColumn(\\'output\\', multiply(col(\"v\"), struct(col(\"v\")))).show()\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql.functions import struct, col\n",
    "@pandas_udf(\"long\")\n",
    "def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n",
    "    for s1, df in iterator:\n",
    "        yield s1 * df.v\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "daba8baa-4fbd-452e-ab77-de0a314373b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"double\")\\ndef mean_udf(v: pd.Series) -> float:\\n    return v.mean()\\n\\ndf = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\\ndf.groupby(\"id\").agg(mean_udf(df[\\'v\\'])).show()\\n'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "550f0721-977f-45ea-b08c-506a5ca642b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import Window\\n@pandas_udf(\"double\")\\ndef mean_udf(v: pd.Series) -> float:\\n    return v.mean()\\n\\ndf = spark.createDataFrame(\\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\\nw = Window.partitionBy(\\'id\\').orderBy(\\'v\\').rowsBetween(-1, 0)\\ndf.withColumn(\\'mean_v\\', mean_udf(\"v\").over(w)).show()\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql import Window\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n",
    "df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7b2a3e76-1c6f-4fe5-a4c3-64d7f99c82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Creates a pandas user defined function (a.k.a. vectorized user defined function).\n",
      "\n",
      "    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer\n",
      "    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF\n",
      "    is defined using the `pandas_udf` as a decorator or to wrap the function, and no\n",
      "    additional configuration is required. A Pandas UDF behaves as a regular PySpark function\n",
      "    API in general.\n",
      "\n",
      "    .. versionadded:: 2.3.0\n",
      "\n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    f : function, optional\n",
      "        user-defined function. A python function if used as a standalone function\n",
      "    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "        the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    functionType : int, optional\n",
      "        an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n",
      "        Default: SCALAR. This parameter exists for compatibility.\n",
      "        Using Python type hints is encouraged.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    In order to use this API, customarily the below are imported:\n",
      "\n",
      "    >>> import pandas as pd\n",
      "    >>> from pyspark.sql.functions import pandas_udf\n",
      "\n",
      "    From Spark 3.0 with Python 3.6+, `Python type hints <https://www.python.org/dev/peps/pep-0484>`_\n",
      "    detect the function types as below:\n",
      "\n",
      "    >>> @pandas_udf(IntegerType())\n",
      "    ... def slen(s: pd.Series) -> pd.Series:\n",
      "    ...     return s.str.len()\n",
      "\n",
      "    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:\n",
      "\n",
      "    >>> from pyspark.sql.functions import PandasUDFType\n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> @pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
      "    ... def slen(s):\n",
      "    ...     return s.str.len()\n",
      "\n",
      "    It is preferred to specify type hints for the pandas UDF instead of specifying pandas UDF\n",
      "    type via `functionType` which will be deprecated in the future releases.\n",
      "\n",
      "    Note that the type hint should use `pandas.Series` in all cases but there is one variant\n",
      "    that `pandas.DataFrame` should be used for its input or output type hint instead when the input\n",
      "    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows\n",
      "    a Pandas UDF which takes long column, string column and struct column, and outputs a struct\n",
      "    column. It requires the function to specify the type hints of `pandas.Series` and\n",
      "    `pandas.DataFrame` as below:\n",
      "\n",
      "    >>> @pandas_udf(\"col1 string, col2 long\")\n",
      "    >>> def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
      "    ...     s3['col2'] = s1 + s2.str.len()\n",
      "    ...     return s3\n",
      "    ...\n",
      "    >>> # Create a Spark DataFrame that has three columns including a struct column.\n",
      "    ... df = spark.createDataFrame(\n",
      "    ...     [[1, \"a string\", (\"a nested string\",)]],\n",
      "    ...     \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
      "    >>> df.printSchema()\n",
      "    root\n",
      "    |-- long_column: long (nullable = true)\n",
      "    |-- string_column: string (nullable = true)\n",
      "    |-- struct_column: struct (nullable = true)\n",
      "    |    |-- col1: string (nullable = true)\n",
      "    >>> df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()\n",
      "    |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      "    |    |-- col1: string (nullable = true)\n",
      "    |    |-- col2: long (nullable = true)\n",
      "\n",
      "    In the following sections, it describes the combinations of the supported type hints. For\n",
      "    simplicity, `pandas.DataFrame` variant is omitted.\n",
      "\n",
      "    * Series to Series\n",
      "        `pandas.Series`, ... -> `pandas.Series`\n",
      "\n",
      "        The function takes one or more `pandas.Series` and outputs one `pandas.Series`.\n",
      "        The output of the function should always be of the same length as the input.\n",
      "\n",
      "        >>> @pandas_udf(\"string\")\n",
      "        ... def to_upper(s: pd.Series) -> pd.Series:\n",
      "        ...     return s.str.upper()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
      "        >>> df.select(to_upper(\"name\")).show()\n",
      "        +--------------+\n",
      "        |to_upper(name)|\n",
      "        +--------------+\n",
      "        |      JOHN DOE|\n",
      "        +--------------+\n",
      "\n",
      "        >>> @pandas_udf(\"first string, last string\")\n",
      "        ... def split_expand(s: pd.Series) -> pd.DataFrame:\n",
      "        ...     return s.str.split(expand=True)\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
      "        >>> df.select(split_expand(\"name\")).show()\n",
      "        +------------------+\n",
      "        |split_expand(name)|\n",
      "        +------------------+\n",
      "        |       [John, Doe]|\n",
      "        +------------------+\n",
      "\n",
      "        .. note:: The length of the input is not that of the whole input column, but is the\n",
      "            length of an internal batch used for each call to the function.\n",
      "\n",
      "    * Iterator of Series to Iterator of Series\n",
      "        `Iterator[pandas.Series]` -> `Iterator[pandas.Series]`\n",
      "\n",
      "        The function takes an iterator of `pandas.Series` and outputs an iterator of\n",
      "        `pandas.Series`. In this case, the created pandas UDF instance requires one input\n",
      "        column when this is called as a PySpark column. The length of the entire output from\n",
      "        the function should be the same length of the entire input; therefore, it can\n",
      "        prefetch the data from the input iterator as long as the lengths are the same.\n",
      "\n",
      "        It is also useful when the UDF execution\n",
      "        requires initializing some states although internally it works identically as\n",
      "        Series to Series case. The pseudocode below illustrates the example.\n",
      "\n",
      "        .. highlight:: python\n",
      "        .. code-block:: python\n",
      "\n",
      "            @pandas_udf(\"long\")\n",
      "            def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
      "                # Do some expensive initialization with a state\n",
      "                state = very_expensive_initialization()\n",
      "                for x in iterator:\n",
      "                    # Use that state for whole iterator.\n",
      "                    yield calculate_with_state(x, state)\n",
      "\n",
      "            df.select(calculate(\"value\")).show()\n",
      "\n",
      "        >>> from typing import Iterator\n",
      "        >>> @pandas_udf(\"long\")\n",
      "        ... def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
      "        ...     for s in iterator:\n",
      "        ...         yield s + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
      "        >>> df.select(plus_one(df.v)).show()\n",
      "        +-----------+\n",
      "        |plus_one(v)|\n",
      "        +-----------+\n",
      "        |          2|\n",
      "        |          3|\n",
      "        |          4|\n",
      "        +-----------+\n",
      "\n",
      "        .. note:: The length of each series is the length of a batch internally used.\n",
      "\n",
      "    * Iterator of Multiple Series to Iterator of Series\n",
      "        `Iterator[Tuple[pandas.Series, ...]]` -> `Iterator[pandas.Series]`\n",
      "\n",
      "        The function takes an iterator of a tuple of multiple `pandas.Series` and outputs an\n",
      "        iterator of `pandas.Series`. In this case, the created pandas UDF instance requires\n",
      "        input columns as many as the series when this is called as a PySpark column.\n",
      "        Otherwise, it has the same characteristics and restrictions as Iterator of Series\n",
      "        to Iterator of Series case.\n",
      "\n",
      "        >>> from typing import Iterator, Tuple\n",
      "        >>> from pyspark.sql.functions import struct, col\n",
      "        >>> @pandas_udf(\"long\")\n",
      "        ... def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n",
      "        ...     for s1, df in iterator:\n",
      "        ...         yield s1 * df.v\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
      "        >>> df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()\n",
      "        +---+------+\n",
      "        |  v|output|\n",
      "        +---+------+\n",
      "        |  1|     1|\n",
      "        |  2|     4|\n",
      "        |  3|     9|\n",
      "        +---+------+\n",
      "\n",
      "        .. note:: The length of each series is the length of a batch internally used.\n",
      "\n",
      "    * Series to Scalar\n",
      "        `pandas.Series`, ... -> `Any`\n",
      "\n",
      "        The function takes `pandas.Series` and returns a scalar value. The `returnType`\n",
      "        should be a primitive data type, and the returned scalar can be either a python primitive\n",
      "        type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64.\n",
      "        `Any` should ideally be a specific scalar type accordingly.\n",
      "\n",
      "        >>> @pandas_udf(\"double\")\n",
      "        ... def mean_udf(v: pd.Series) -> float:\n",
      "        ...     return v.mean()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
      "        >>> df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
      "        +---+-----------+\n",
      "        | id|mean_udf(v)|\n",
      "        +---+-----------+\n",
      "        |  1|        1.5|\n",
      "        |  2|        6.0|\n",
      "        +---+-----------+\n",
      "\n",
      "        This UDF can also be used as window functions as below:\n",
      "\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> @pandas_udf(\"double\")\n",
      "        ... def mean_udf(v: pd.Series) -> float:\n",
      "        ...     return v.mean()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
      "        >>> w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n",
      "        >>> df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()\n",
      "        +---+----+------+\n",
      "        | id|   v|mean_v|\n",
      "        +---+----+------+\n",
      "        |  1| 1.0|   1.0|\n",
      "        |  1| 2.0|   1.5|\n",
      "        |  2| 3.0|   3.0|\n",
      "        |  2| 5.0|   4.0|\n",
      "        |  2|10.0|   7.5|\n",
      "        +---+----+------+\n",
      "\n",
      "        .. note:: For performance reasons, the input series to window functions are not copied.\n",
      "            Therefore, mutating the input series is not allowed and will cause incorrect results.\n",
      "            For the same reason, users should also not rely on the index of the input series.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The user-defined functions do not support conditional expressions or short circuiting\n",
      "    in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "    can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "\n",
      "    The user-defined functions do not take keyword arguments on the calling side.\n",
      "\n",
      "    The data type of returned `pandas.Series` from the user-defined functions should be\n",
      "    matched with defined `returnType` (see :meth:`types.to_arrow_type` and\n",
      "    :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do\n",
      "    conversion on returned data. The conversion is not guaranteed to be correct and results\n",
      "    should be checked for accuracy by users.\n",
      "\n",
      "    Currently,\n",
      "    :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and\n",
      "    nested :class:`pyspark.sql.types.StructType`\n",
      "    are currently not supported as output types.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    pyspark.sql.GroupedData.agg\n",
      "    pyspark.sql.DataFrame.mapInPandas\n",
      "    pyspark.sql.GroupedData.applyInPandas\n",
      "    pyspark.sql.PandasCogroupedOps.applyInPandas\n",
      "    pyspark.sql.UDFRegistration.register\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pandas_udf.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ceb27-7e72-41eb-96a6-78ae62d03478",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.udf(f: Union[Callable[[…], Any], DataTypeOrString, None] = None, returnType: DataTypeOrString = StringType(), *, useArrow: Optional[bool] = None) → Union[UserDefinedFunctionLike, Callable[[Callable[[…], Any]], UserDefinedFunctionLike]]\n",
    "Creates a user defined function (UDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d38e8d-2d77-4c71-9fa4-eb96568a49b4",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- f: function |\n",
    "python function if used as a standalone function\n",
    "- return: Typepyspark.sql.types.DataType or str | \n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "- use: Arrowbool or None | \n",
    "whether to use Arrow to optimize the (de)serialization. When it is None, the Spark config “spark.sql.execution.pythonUDF.arrow.enabled” takes effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3bb37122-2bb1-44ee-882b-8d2bb4e436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4d721c43-118b-43aa-b656-c2ffb218319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|Random int|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        72|           72|           74|         2|\n",
      "|female|       group C|               some college|    standard|              completed|        69|           90|           88|        69|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|        38|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        47|           57|           44|         3|\n",
      "|  male|       group C|               some college|    standard|                   none|        76|           78|           75|        21|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.limit(5).withColumn(\"Random int\", random_udf()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8bc7cf27-73c6-453d-9c92-9d6381a3591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------+\n",
      "|slen(name)|to_upper(name)|add_one(age)|\n",
      "+----------+--------------+------------+\n",
      "|         8|      JOHN DOE|          22|\n",
      "+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "@udf\n",
    "def to_upper(s):\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def add_one(x):\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b2099-4ad6-46bd-a9b4-5b7dedc25aae",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.udtf(cls: Optional[Type] = None, *, returnType: Union[pyspark.sql.types.StructType, str], useArrow: Optional[bool] = None) → Union[pyspark.sql.udtf.UserDefinedTableFunction, Callable[[Type], pyspark.sql.udtf.UserDefinedTableFunction]]\n",
    "Creates a user defined table function (UDTF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4330c0-b9bc-4461-8d12-0f4df5975b86",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- cls: class | \n",
    "the Python user-defined table function handler class.\n",
    "- return: Typepyspark.sql.types.StructType or str | \n",
    "the return type of the user-defined table function. The value can be either a pyspark.sql.types.StructType object or a DDL-formatted struct type string.\n",
    "- use: Arrowbool or None, optional | \n",
    "whether to use Arrow to optimize the (de)serializations. When it’s set to None, the Spark config “spark.sql.execution.pythonUDTF.arrow.enabled” is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c84969ec-d27b-4a36-8a46-435de8054211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlusOne:\n",
    "    def eval(self, a: int):\n",
    "        yield a + 1,\n",
    "plus_one = udtf(PlusOne, returnType=\"r: int\").asDeterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "23cf4934-04c4-4883-9195-eb33184096ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   c1|   c2|\n",
      "+-----+-----+\n",
      "|hello|world|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TestUDTF:\n",
    "    def eval(self, *args: Any):\n",
    "        yield \"hello\", \"world\"\n",
    "\n",
    "test_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")\n",
    "test_udtf().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b0668c5-9015-49ca-bd94-21c5635ca7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udtf(returnType=\"c1: int, c2: int\")\n",
    "class PlusOne:\n",
    "    def eval(self, x: int):\n",
    "        yield x, x + 1\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "PlusOne(lit(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "506c7837-9869-493f-92d7-8c5cbabef662",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udtf(returnType=\"c1: int, c2: int\", useArrow=True)\n",
    "class ArrowPlusOne:\n",
    "    def eval(self, x: int):\n",
    "        yield x, x + 1\n",
    "\n",
    "# eval_type = read_int(infile) Error\n",
    "# ArrowPlusOne(lit(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2c821-b4da-46cc-827d-fa32db4c8c1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.unwrap_udt(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Unwrap UDT data type column into its underlying type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6707c52b-4c96-4db3-b82c-ddf1d41c6266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class MyUDT:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "data = [(MyUDT(42),)]\n",
    "schema = StructType([StructField(\"my_udt\", MyUDTType(), True)])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef56e7-b773-488c-89c3-9fd13c900d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "result_df = df.select(unwrap_udt(col(\"my_udt\")).alias(\"my_udt_value\"))\n",
    "result_df.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d00dc8f0-80f2-4ede-9805-176bb4762295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import UserDefinedType\n",
    "\n",
    "# Визначаємо користувацький тип даних MyUDT\n",
    "class MyUDT(UserDefinedType):\n",
    "\n",
    "    def __init__(self, value = None):\n",
    "        self.value = value\n",
    "        \n",
    "    def simpleString(self):\n",
    "        return \"MyUDT\"\n",
    "\n",
    "    def serialize(self, obj):\n",
    "        return str(obj).encode('utf-8')\n",
    "\n",
    "    def deserialize(self, datum):\n",
    "        return MyUDT(int(datum.decode('utf-8')))\n",
    "\n",
    "# Оголошуємо функцію unwrap_udt з використанням користувацького типу MyUDT\n",
    "@pandas_udf(MyUDT())\n",
    "def unwrap_udt(s: pd.Series) -> pd.Series:\n",
    "    return s\n",
    "\n",
    "# Створюємо DataFrame\n",
    "data = [(MyUDT(42),), (MyUDT(56),)]\n",
    "df = spark.createDataFrame(data, [\"my_udt\"])\n",
    "\n",
    "# Використовуємо unwrap_udt для розгортання користувацького типу MyUDT\n",
    "result_df = df.select(unwrap_udt(col(\"my_udt\")).alias(\"my_udt_value\"))\n",
    "\n",
    "# Виводимо результат\n",
    "result_df.show()\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617aa81-b5a3-4753-9eac-434351cd8b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Misc Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#misc-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb1036-6b3b-4e51-91ef-a62de46f12de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.aes_decrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9b7b-7225-46de-9397-b3795a0ff7b4",
   "metadata": {},
   "source": [
    "##### The Advanced Encryption Standard (AES) \n",
    "Returns a decrypted value of input using AES in mode with padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM.\n",
    "\n",
    "Parameters:\n",
    "- input: Column or str | The binary value to decrypt.\n",
    "- key: Column or str | The passphrase to use to decrypt the data.\n",
    "- mode: Column or str, optional | Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- aad: Column or str, optional | Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77a9d16-75fa-4f60-997d-4f6b136382e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3926a9-5cd2-47f9-a1c4-1155f04ddbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b226d44-c853-4de4-876b-5a7669a4a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfaa1d44-2239-41dd-b4de-85415090fe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    \"0000111122223333\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdaef3-1a32-479f-8ad4-2af582a99279",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.aes_encrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, iv: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf223f-ecc4-44af-be4d-d4bed5166f03",
   "metadata": {},
   "source": [
    "##### The Advanced Encryption Standard (AES) \n",
    "Returns an encrypted value of input using AES in given mode with the specified padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional initialization vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12 bytes for GCM. If not provided, a random vector will be generated and prepended to the output. Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM.\n",
    "\n",
    "Parameters: \n",
    "- input: Column or str | The binary value to encrypt.\n",
    "- key: Column or str | The passphrase to use to encrypt the data.\n",
    "- mode: Column or str, optional | Specifies which block cipher mode should be used to encrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- iv: Column or str, optional | Optional initialization vector. Only supported for CBC and GCM modes. Valid values: None or “”. 16-byte array for CBC mode. 12-byte array for GCM mode.\n",
    "- aad: Column or str, optional | Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c19dcbd-34d5-49c8-a5e7-5dbfd9e6fe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n",
    ")\n",
    "df.select(base64(aes_encrypt(\n",
    "    df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")), df.aad)\n",
    ").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d315bcca-5685-4524-ac60-3a0f7a357ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='AAAAAAAAAAAAAAAAQiYi+sRNYDAOTjdSEcYBFsAWPL1f')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(base64(aes_encrypt(\n",
    "    df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")))\n",
    ").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f29e7c10-72c2-4c82-bba6-d51a98f22163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark SQL'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"1234567890abcdef\", \"ECB\", \"PKCS\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode, df.padding),\n",
    "    df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea2a481-18c2-461b-80a1-f5886abb6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark SQL'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"0000111122223333\", \"ECB\",)],\n",
    "    [\"input\", \"key\", \"mode\"]\n",
    ")\n",
    "df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode),\n",
    "    df.key, df.mode).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36dd94c0-8591-46e8-8219-9ade838cf4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spark SQL')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"abcdefghijklmnop\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(base64(aes_encrypt(df.input, df.key))), df.key\n",
    ").cast(\"STRING\").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bfe752-457b-4de8-80ab-5f48c0ed641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_1 = \"0000111122223333\"\n",
    "key_2 = \"abcdefghijklmnop\"\n",
    "\n",
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    key_1,)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f36debe1-e01f-44e4-83f6-62ca8a7e2f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='!\\x196�:���\\x7f�4�]��hT�5\\x7fshm<\\x0b4wO\\x06�\\r��\\x10�1')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", key_2,)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "\n",
    "df_key_2_en = df.select(aes_encrypt(df.input, df.key).cast(\"STRING\").alias('r'))\n",
    "df_key_2_en.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b601bc8c-bf96-4bfd-b30c-c2d818da792c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spark SQL')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_key_2_de = df_key_2_en.select(aes_decrypt(\"r\", lit(key_2)).cast(\"STRING\").alias('r'))\n",
    "df_key_2_de.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fddb37-ec70-4cb9-ba05-19ca95c87188",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_bit_position(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the bit position for the given input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822427e2-76d7-4504-99a8-c34f57d02915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=122)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(123,)], [\"a\"])\n",
    "df.select(bitmap_bit_position(df.a).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9ede2b-25f9-42b0-a026-54cd2532e3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Value='71', bitmap_bit_position(Value)=70),\n",
       " Row(Value='77', bitmap_bit_position(Value)=76),\n",
       " Row(Value='83', bitmap_bit_position(Value)=82),\n",
       " Row(Value='54', bitmap_bit_position(Value)=53),\n",
       " Row(Value='100', bitmap_bit_position(Value)=99),\n",
       " Row(Value='63', bitmap_bit_position(Value)=62),\n",
       " Row(Value='88', bitmap_bit_position(Value)=87),\n",
       " Row(Value='64', bitmap_bit_position(Value)=63),\n",
       " Row(Value='64', bitmap_bit_position(Value)=63),\n",
       " Row(Value='92', bitmap_bit_position(Value)=91),\n",
       " Row(Value='75', bitmap_bit_position(Value)=74),\n",
       " Row(Value='54', bitmap_bit_position(Value)=53),\n",
       " Row(Value='76', bitmap_bit_position(Value)=75),\n",
       " Row(Value='94', bitmap_bit_position(Value)=93),\n",
       " Row(Value='82', bitmap_bit_position(Value)=81),\n",
       " Row(Value='85', bitmap_bit_position(Value)=84),\n",
       " Row(Value='79', bitmap_bit_position(Value)=78),\n",
       " Row(Value='89', bitmap_bit_position(Value)=88),\n",
       " Row(Value='88', bitmap_bit_position(Value)=87),\n",
       " Row(Value='91', bitmap_bit_position(Value)=90)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(\"Value\", bitmap_bit_position(\"Value\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "323045af-6af9-49c9-a27c-ca16ed024644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bitmap_bit_position(Value)=70),\n",
       " Row(bitmap_bit_position(Value)=76),\n",
       " Row(bitmap_bit_position(Value)=82),\n",
       " Row(bitmap_bit_position(Value)=53),\n",
       " Row(bitmap_bit_position(Value)=99),\n",
       " Row(bitmap_bit_position(Value)=62),\n",
       " Row(bitmap_bit_position(Value)=87),\n",
       " Row(bitmap_bit_position(Value)=63),\n",
       " Row(bitmap_bit_position(Value)=63),\n",
       " Row(bitmap_bit_position(Value)=91),\n",
       " Row(bitmap_bit_position(Value)=74),\n",
       " Row(bitmap_bit_position(Value)=53),\n",
       " Row(bitmap_bit_position(Value)=75),\n",
       " Row(bitmap_bit_position(Value)=93),\n",
       " Row(bitmap_bit_position(Value)=81),\n",
       " Row(bitmap_bit_position(Value)=84),\n",
       " Row(bitmap_bit_position(Value)=78),\n",
       " Row(bitmap_bit_position(Value)=88),\n",
       " Row(bitmap_bit_position(Value)=87),\n",
       " Row(bitmap_bit_position(Value)=90)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(bitmap_bit_position(tour.Value)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063fa2-fe3c-4f81-829d-f8a0a9d0fe47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_bucket_number(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the bucket number for the given input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f67e02f-8a98-4f67-8e65-c95916d0149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(123,)], [\"a\"])\n",
    "df.select(bitmap_bucket_number(df.a).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac5e5e48-3453-4986-8676-72dd4b82176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Value='71', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='77', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='83', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='54', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='100', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='63', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='88', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='64', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='64', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='92', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='75', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='54', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='76', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='94', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='82', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='85', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='79', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='89', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='88', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='91', bitmap_bucket_number(Value)=1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(\"Value\", bitmap_bucket_number(\"Value\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19960d-cfcb-46ae-be80-a2a10c9f6491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_construct_agg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a bitmap with the positions of the bits set from all the values from the input column. The input column will most likely be bitmap_bit_position()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4e93cd-e6be-450f-b5d6-f73a18f3652e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='070000')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,),(2,),(3,)], [\"a\"])\n",
    "df.select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(df.a))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e077dbf4-8d7d-40fe-9211-7a71a0bc7115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(math score=72, bitmap_bit_position(math score)=71),\n",
       " Row(math score=69, bitmap_bit_position(math score)=68),\n",
       " Row(math score=90, bitmap_bit_position(math score)=89),\n",
       " Row(math score=47, bitmap_bit_position(math score)=46),\n",
       " Row(math score=76, bitmap_bit_position(math score)=75),\n",
       " Row(math score=71, bitmap_bit_position(math score)=70),\n",
       " Row(math score=88, bitmap_bit_position(math score)=87),\n",
       " Row(math score=40, bitmap_bit_position(math score)=39),\n",
       " Row(math score=64, bitmap_bit_position(math score)=63),\n",
       " Row(math score=38, bitmap_bit_position(math score)=37)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(\"math score\", bitmap_bit_position(\"math score\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df7ab23-8541-46b8-a7ad-26068fb62807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='8100E6')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b0f6813-2667-46ad-bacb-7122fd162d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='8100E6BEFFFFFFFFFFFFFFFF0F000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    ").alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "589441f9-00e0-4eb1-a787-3e96db54964d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'\\x81\\x00\\xe6\\xbe\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(bitmap_construct_agg(bitmap_bit_position(\"math score\")).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4463abf8-c1dd-4086-bd02-9b30a928dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='000002')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(20).select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71f02cb0-c84f-4c43-a36b-3bf9d4df6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='00000200A0602282D128800200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(20).select(hex(bitmap_construct_agg(bitmap_bit_position(\"math score\"))).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa84701-bbb6-499b-841a-ade1bbd8ac2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_count(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of set bits in the input bitmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a8ae900-4b0b-4eb1-84c2-6ae03a3cecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=16)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"FFFF\",)], [\"a\"])\n",
    "df.select(bitmap_count(to_binary(df.a, lit(\"hex\"))).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "301f8af2-f324-48fc-818b-2b8eeb00c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=4),\n",
       " Row(r=4),\n",
       " Row(r=2),\n",
       " Row(r=4),\n",
       " Row(r=5),\n",
       " Row(r=4),\n",
       " Row(r=2),\n",
       " Row(r=1),\n",
       " Row(r=3),\n",
       " Row(r=3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(bitmap_count(to_binary(\"math score\", lit(\"hex\"))).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0ac3-7e0d-4976-a801-7f73041b4991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0ac126-8798-499b-a5e1-0134d4a0c19a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_or_agg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a bitmap that is the bitwise OR of all of the bitmaps from the input column. The input column should be bitmaps created from bitmap_construct_agg()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2cd91d2-cadf-4336-9def-7e539fce52f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='700000')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"10\",),(\"20\",),(\"40\",)], [\"a\"])\n",
    "df.select(substring(hex(\n",
    "    bitmap_or_agg(to_binary(df.a, lit(\"hex\")))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48348164-25f6-4c50-afb0-d66170ea6a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='FF0000')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(substring(hex(\n",
    "    bitmap_or_agg(to_binary(\"math score\", lit(\"hex\")))), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d7cb2-e512-4ab1-ae65-6b1c24f5e9a8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_catalog() → pyspark.sql.column.Column\n",
    "Returns the current catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d6ceb37-258b-4e2c-aa90-45971c8fbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|current_catalog()|\n",
      "+-----------------+\n",
      "|    spark_catalog|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_catalog()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc1199-b03d-4225-b43e-3de36bf35475",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_database() → pyspark.sql.column.Column[source]\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecae5548-5809-4eba-977a-c3ba5c13ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_database()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b481d65-dfb6-43b2-baeb-398f6add5c16",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_schema() → pyspark.sql.column.Column\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3641707d-b644-4be5-b52e-a10ca06936ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_schema()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646a071-cf2d-4ba2-8d9a-c0f4b513abba",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_user() → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d771bbd2-63fc-4ae1-af52-30fee4f24749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_user()|\n",
      "+--------------+\n",
      "|    zsavchenko|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_user()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a8814-72fd-446c-a233-64761813a90f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.input_file_block_length() → pyspark.sql.column.Column\n",
    "Returns the length of the block being read, or -1 if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c38f3e97-7cee-46fe-92f1-53df6819be4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=72036)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"students.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_length().alias('r')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bd12d6e-aa27-4df8-989b-407f0d3b9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=21305899)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"pga_tour_historical.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_length().alias('r')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02505ea-06b5-452a-b1a0-203568b1cbf6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.input_file_block_start() → pyspark.sql.column.Column\n",
    "Returns the start offset of the block being read, or -1 if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bd38b62-e59f-4705-95e6-e08da12a7081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"students.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_start().alias('r')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bff80f5-564b-4d54-980c-f19a5d4b6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"pga_tour_historical.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_start().alias('r')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ba148-cb39-41dd-b31c-787d219fed4c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.md5(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the MD5 digest and returns the value as a 32 character hex string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f16dc0ac-44e4-4ecf-a9ed-4447a1601c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe9baa-7c1e-470a-8bdb-4c44b3dbb468",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sha1 hash value as a hex string of the col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdac43de-f220-466c-9dd5-160457fe19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|sha(Spark)                              |\n",
      "+----------------------------------------+\n",
      "|85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sha(lit(\"Spark\"))).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8c840-46d6-4515-896b-f023a34a0a9e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha1(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the hex string result of SHA-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dfcedb7-ca9a-4c21-8a8b-0d38cecbd209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b5425-b887-473b-bc2b-313a307b94fe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha2(col: ColumnOrName, numBits: int) → pyspark.sql.column.Column\n",
    "Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The numBits indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997fd50-64de-4a37-9bc4-7cb1f19f3831",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- col: Column or str | target column to compute on.\n",
    "- num: Bitsint | the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
    "\n",
    "Returns – Column | the column for computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd43cd5d-0897-44e9-8c54-0b4caecb092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------+\n",
      "|name |sha2                                                            |\n",
      "+-----+----------------------------------------------------------------+\n",
      "|Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "|Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n",
      "+-----+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
    "df.withColumn(\"sha2\", sha2(df.name, 256)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079161c8-3978-4058-9e4b-ca8130fbdd27",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.crc32(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c530254b-b0bb-4218-b35b-5fb70a1d1e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(crc32=2743272264)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c38a2c-cbe1-4b86-af49-f742455504d9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hash(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the hash code of given columns, and returns the result as an int column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b22d5c4-3ee6-4a45-ab60-ea6a87a83052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      hash|\n",
      "+----------+\n",
      "|-757602832|\n",
      "+----------+\n",
      "\n",
      "+---------+\n",
      "|     hash|\n",
      "+---------+\n",
      "|599895104|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
    "\n",
    "df.select(hash('c1').alias('hash')).show()\n",
    "\n",
    "df.select(hash('c1', 'c2').alias('hash')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972efaf-ca1b-4617-a99e-f5f7adbd90e9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xxhash64(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm, and returns the result as a long column. The hash computation uses an initial seed of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49bc5bf7-c55f-415f-967d-4901a89f2efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               hash|\n",
      "+-------------------+\n",
      "|4105715581806190027|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|               hash|\n",
      "+-------------------+\n",
      "|3233247871021311208|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
    "\n",
    "df.select(xxhash64('c1').alias('hash')).show()\n",
    "\n",
    "df.select(xxhash64('c1', 'c2').alias('hash')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351565-e265-4924-b2d3-efde2e3dd939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.assert_true(col: ColumnOrName, errMsg: Union[pyspark.sql.column.Column, str, None] = None) → pyspark.sql.column.Column\n",
    "Returns null if the input column is true; throws an exception with the provided error message otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fae58-b408-4eb3-9c48-357e15147c7d",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- col: Column or str | column name or column that represents the input column to test.\n",
    "- errMsg: Column or str, optional | A Python string literal or column containing the error message.\n",
    "\n",
    "Returns – Column | null if the input column is true otherwise throws an error with specified message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97d50672-006c-4011-8b5b-3f6a8fa7b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
    "df.select(assert_true(df.a < df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c65d7b2-47c9-49ac-858a-b1a192923e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d92fd079-da85-4485-91ec-b77649798f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68c680f6-663a-4fe4-84a9-a8df5e9f4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:38 ERROR Executor: Exception in task 11.0 in stage 91.0 (TID 496)\n",
      "java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "23/10/22 15:48:38 WARN TaskSetManager: Lost task 11.0 in stage 91.0 (TID 496) (192.168.0.179 executor driver): java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "23/10/22 15:48:38 ERROR TaskSetManager: Task 11 in stage 91.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    "    df.select(assert_true(df.a > df.b, 'My error msg').alias('r')).collect()\n",
    "except Py4JJavaError as e:\n",
    "    print(\"My error msg\" in e.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd7a50-acf7-44b1-a9e3-1b497912fdf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.raise_error(errMsg: Union[pyspark.sql.column.Column, str]) → pyspark.sql.column.Column\n",
    "Throws an exception with the provided error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba93d4d6-8f15-48dd-8205-d008e069f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:38 ERROR Executor: Exception in task 11.0 in stage 92.0 (TID 508)\n",
      "java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "23/10/22 15:48:38 WARN TaskSetManager: Lost task 11.0 in stage 92.0 (TID 508) (192.168.0.179 executor driver): java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache."
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "df = spark.range(1)\n",
    "\n",
    "try:\n",
    "    df.select(raise_error(\"My error msg\")).show()\n",
    "except Py4JJavaError as e:\n",
    "    print(\"My error msg\" in e.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccbf72-dbc5-4a18-a90f-15b7da9be924",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.reflect(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calls a method with reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f8957-3846-4ecb-9813-018ba36aa626",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal string for the class name, and the second element should be a literal string for the method name, and the remaining are input arguments to the Java method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e347dba-f974-4fea-b06a-35b3c14cb5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "23/10/22 15:48:38 ERROR TaskSetManager: Task 11 in stage 92.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r='a5cf6c42-0c85-418f-af6c-3e4e5b1328f2')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\",)], [\"a\"])\n",
    "df.select(\n",
    "    reflect(lit(\"java.util.UUID\"), lit(\"fromString\"), df.a).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f1dd2-a71d-4c54-96a7-22fb509b1ef9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_sketch_estimate(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the estimated number of unique values given the binary representation of a Datasketches HllSketch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5efee-0d02-4235-9e5a-415b02858b1f",
   "metadata": {},
   "source": [
    "[why] why not use hll_sketch_estimate all time?\n",
    "\n",
    "Datasketches HyperLogLog (HLL) Sketch - це структура даних, призначена для наближеного підрахунку унікальних елементів у великих наборах даних. Вона використовує алгоритм HyperLogLog для оцінки кількості різних елементів у масиві, не зберігаючи всі ці елементи, а лише важливі характеристики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58d374b9-0bc2-4ac4-bd16-a69c14b0887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b759d3d-d30b-4674-bba1-195dc52a9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|      179970|\n",
      "+------------+\n",
      "\n",
      "execution_time - 1.1872360706329346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "tour.agg(hll_sketch_estimate(hll_sketch_agg(\"Value\")).alias(\"distinct_cnt\")).show()\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56daead7-e800-4eb2-9822-f5b3db88cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:====>                                                  (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180981\n",
      "execution_time - 1.5197079181671143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(tour.select(\"Value\").distinct().count())\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c53d04d2-4ccb-4b29-814c-865f0213bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|          81|\n",
      "+------------+\n",
      "\n",
      "execution_time - 0.07657694816589355\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "students.agg(hll_sketch_estimate(hll_sketch_agg(\"math score\")).alias(\"distinct_cnt\")).show()\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "305e2e78-a03d-49af-a39b-b6ebc870d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "execution_time - 0.23991703987121582\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(students.select(\"math score\").distinct().count())\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b61d03-1d9d-4d8b-a2f7-a2565c756506",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_union(col1: ColumnOrName, col2: ColumnOrName, allowDifferentLgConfigK: Optional[bool] = None) → pyspark.sql.column.Column\n",
    "Merges two binary representations of Datasketches HllSketch objects, using a Datasketches Union object. Throws an exception if sketches have different lgConfigK values and allowDifferentLgConfigK is unset or set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0934c466-1436-4dd8-9f6a-34b0adf08110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|             sketch1|             sketch2|distinct_cnt|\n",
      "+--------------------+--------------------+------------+\n",
      "|[02 01 07 0C 03 0...|[02 01 07 0C 03 0...|           6|\n",
      "+--------------------+--------------------+------------+\n",
      "\n",
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           6|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,4),(2,5),(2,5),(3,6)], \"struct<v1:int,v2:int>\")\n",
    "df = df.agg(hll_sketch_agg(\"v1\").alias(\"sketch1\"), hll_sketch_agg(\"v2\").alias(\"sketch2\"))\n",
    "df = df.withColumn(\"distinct_cnt\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
    "df.show()\n",
    "df.drop(\"sketch1\", \"sketch2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33b2f1a6-16e3-4acd-ad64-89a48ca42089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|             sketch1|             sketch2|             sketch3|distinct_cnt1|distinct_cnt2|distinct_cnt3|distinct_cnt4|\n",
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|[03 01 07 0C 07 0...|[03 01 07 0C 07 0...|[03 01 07 0C 07 0...|           83|           83|           82|           82|\n",
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|distinct_cnt1|distinct_cnt2|distinct_cnt3|distinct_cnt4|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|           83|           83|           82|           82|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = students.agg(hll_sketch_agg(\"math score\").alias(\"sketch1\"),\n",
    "                  hll_sketch_agg(\"reading score\").alias(\"sketch2\"),\n",
    "                  hll_sketch_agg(\"writing score\").alias(\"sketch3\"),)\n",
    "df = (df.withColumn(\"distinct_cnt1\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
    "        .withColumn(\"distinct_cnt2\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch3\")))\n",
    "        .withColumn(\"distinct_cnt3\", hll_sketch_estimate(hll_union(\"sketch2\", \"sketch3\")))\n",
    "        .withColumn(\"distinct_cnt4\", hll_sketch_estimate(hll_union(\"sketch3\", \"sketch2\")))\n",
    ")\n",
    "df.show()\n",
    "df.drop(\"sketch1\", \"sketch2\", \"sketch3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d19ba9-bd01-4661-b55a-90ddbeb8a98b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.java_method(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calls a method with reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63158e-79a2-4425-b624-4084bbce4a6a",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal string for the class name, and the second element should be a literal string for the method name, and the remaining are input arguments to the Java method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "504a9681-8976-4008-99a7-6a144a35ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(\n",
    "    java_method(\n",
    "        lit(\"java.util.UUID\"),\n",
    "        lit(\"fromString\"),\n",
    "        lit(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\")\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604ea34-8910-407a-bdaf-f8d6f75adf16",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.stack(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Separates col1, …, colk into n rows. Uses column names col0, col1, etc. by default unless specified otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a1d98-7500-4d70-b0b5-71578535fc32",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal int for the number of rows to be separated, and the remaining are input elements to be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f2b7dde-08b1-43fa-a9dc-0b9e979c36fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3)], [\"a\", \"b\", \"c\"])\n",
    "df.select(stack(lit(2), df.a, df.b, df.c)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebc7128a-f4f7-4ee6-b6ca-f6e81bdb7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3, 4), (1, 2, 3, 4)], [\"a\", \"b\", \"c\", \"d\"])\n",
    "df.select(stack(lit(2), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7772cdaf-2572-4f2a-ba0b-f271d7231dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|NULL|NULL|\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(3), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69751707-9a89-43f7-bae6-03672e8d638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|1   |2   |3   |4   |\n",
      "|1   |2   |3   |4   |\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(1), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce06de0f-d2fd-4597-a230-1804c2c0bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col0|\n",
      "+----+\n",
      "|1   |\n",
      "|2   |\n",
      "|3   |\n",
      "|4   |\n",
      "|1   |\n",
      "|2   |\n",
      "|3   |\n",
      "|4   |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(4), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397ae1b-3bb7-4ffb-8e84-2e7636f66e93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.try_aes_decrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "This is a special version of aes_decrypt that performs the same operation, but returns a NULL value instead of raising an error if the decryption cannot be performed. Returns a decrypted value of input using AES in mode with padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70324b9-c4fc-489c-be28-6c14a347820a",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- input: Column or str | \n",
    "The binary value to decrypt.\n",
    "- key: Column or str | \n",
    "The passphrase to use to decrypt the data.\n",
    "- mode: Column or str, optional |\n",
    "Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | \n",
    "Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- aad: Column or str, optional | \n",
    "Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6e0fc44-b5ba-4367-964f-c60b718e84d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcfb0a80-42dc-4b4d-9289-c988e71afbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6201239a-1135-44ba-af6e-bc26ca4c605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(try_aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14e54b59-6b29-4969-8e05-1e80fed28fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    \"0000111122223333\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d975d6-8971-4bcd-8439-290ff340eccd",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.typeof(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Return DDL-formatted type string for the data type of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "069b7f67-6c96-4969-97d7-501967481d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='bigint')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,)], [\"a\"])\n",
    "df.select(typeof(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8bb48-8bd1-4694-b8e3-a3e13a1a3da7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.user() → pyspark.sql.column.Column\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "479c39e6-573f-44a5-ace7-a48f2fc11eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_user()|\n",
      "+--------------+\n",
      "|    zsavchenko|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(user()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042fdb3-ba69-49b3-b2ae-84203e47bde9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.version() → pyspark.sql.column.Column\n",
    "Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26ccb10a-7c74-456a-b3f0-788cb2b2e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|version()                                     |\n",
      "+----------------------------------------------+\n",
      "|3.5.0 ce5ddad990373636e94071e7cef2f31021add07b|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(version()).show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4507ea8-f50f-41dd-88eb-051a2114ec55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Predicate Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#predicate-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c361b-47cd-4051-b8ac-9e4a9c5aa2e9",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* col1: Column or str\n",
    "* col2: Column or str\n",
    "* col3: Column or str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a923363-f56f-45b8-ad65-ebd5a8df13eb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.equal_null(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns same result as the EQUAL(=) operator for non-null operands, but returns true if both are null, false if one of the them is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "798b4cd6-59fe-4420-8a98-d2f742dbbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "|   1|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=True), Row(r=False), Row(r=True)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, None,), (1, 9,), (1, 1,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(equal_null(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e79d7-9bee-4163-806f-766c799854f1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ifnull(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is null, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79988c06-9cf8-4fb6-a20b-0569862aa42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   e|  e2|\n",
      "+----+----+\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "+----+----+\n",
      "\n",
      "+-------------+\n",
      "|ifnull(e, e2)|\n",
      "+-------------+\n",
      "|           12|\n",
      "|            1|\n",
      "|         NULL|\n",
      "|            1|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 12,), (1, None,), (None, None,), (1, 9,),], [\"e\", \"e2\"])\n",
    "df.show()\n",
    "df.select(ifnull(df.e, df.e2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10964946-795e-418d-913c-dac47d801a83",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.isnotnull(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if col is not null, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d1bdcc0-c87c-40cf-9be9-d9bc02af6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   e|\n",
      "+----+\n",
      "|NULL|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=False), Row(r=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
    "df.show()\n",
    "df.select(isnotnull(df.e).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080dd58-ae28-4198-9c16-777537be2a00",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nullif(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns null if col1 equals to col2, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed67a71d-cd09-4bdd-98c1-e68dfc71b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "|   1|   1|\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=None), Row(r=1), Row(r=None), Row(r=None), Row(r=1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, None,), (1, 9,), (1, 1,), (None, 12,), (1, None,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(nullif(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58b879-542e-43e7-a090-73e03059774b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nvl(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is null, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4c1308e-d9d9-4584-9370-518dbef38218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=12), Row(r=1), Row(r=None), Row(r=1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 12,), (1, None,), (None, None,), (1, 9,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(nvl(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb176098-667f-4240-964e-cd9bd3418fbe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nvl2(col1: ColumnOrName, col2: ColumnOrName, col3: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is not null, or col3 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b80f0a4-aab5-48f3-a979-e7f27165040d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=6), Row(r=8)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 8, 6,), (1, 8, 9,)], [\"a\", \"b\", \"c\"])\n",
    "df.select(nvl2(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bac89-15c5-4079-8210-67b136eb28df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Xml Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#xml-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18352f41-a717-4190-a59f-fe1ff5679e3f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a string array of values within the nodes of xml that match the XPath expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "451c17a2-2461-4d22-a41e-d139d85cb9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=['b1', 'b2', 'b3'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>',)], ['x'])\n",
    "df.select(xpath(df.x, lit('a/b/text()')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdbd1d-b27b-434c-98d1-6c7a72a5464f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_boolean(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns true if the XPath expression evaluates to true, or if a matching node is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ff0ad24-20b5-4b76-9707-6c2e53db3ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b></a>',)], ['x'])\n",
    "df.select(xpath_boolean(df.x, lit('a/b')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc0eaf-0b45-4db4-ab61-d25d2c83f79f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_double(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05760e61-c813-4698-ac9d-cb81a8b51889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_double(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a000a4-f517-4e2c-96d9-b2e0c9742dc0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_int(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ed11832-bdfa-40a4-9b35-89812fda2c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_int(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2dae5-2286-42dc-b000-89c0042614ca",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_long(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ccb0452-110f-4607-9baa-9387fa6ac07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_long(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35877bbb-436b-490c-b6f0-83abfe0400b3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_number(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "041c23c1-63f3-48f4-be41-13e99490554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|xpath_number(x, sum(a/b))|\n",
      "+-------------------------+\n",
      "|                      3.0|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x']).select(xpath_number('x', lit('sum(a/b)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddedb0b5-d2d9-4fb3-a182-f7a6a789cce5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_short(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24a2203c-95dd-4aa2-9bd6-5db6e8476a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_short(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f378ac0-b115-4656-8502-5b39dd7078ed",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_string(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the text contents of the first xml node that matches the XPath expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4e9aa4e-af79-4c51-8275-a33ae2ef2e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='cc')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>b</b><c>cc</c></a>',)], ['x'])\n",
    "df.select(xpath_string(df.x, lit('a/c')).alias('r')).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
