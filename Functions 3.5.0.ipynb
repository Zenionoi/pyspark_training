{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709bd626-fd3f-4f6c-829d-12024eb0d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/01 20:36:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.179:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Functions 3.5.0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1125b9190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find [why] for questions\n",
    "# why I get error with eval_type = read_int(infile)\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Functions 3.5.0\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaee414-52b3-436d-84a8-e6b68c5e0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------------+--------------------+-----+\n",
      "|    Player Name|Season|       Statistic|            Variable|Value|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "|Robert Garrigus|  2010|Driving Distance|Driving Distance ...|   71|\n",
      "|   Bubba Watson|  2010|Driving Distance|Driving Distance ...|   77|\n",
      "| Dustin Johnson|  2010|Driving Distance|Driving Distance ...|   83|\n",
      "|Brett Wetterich|  2010|Driving Distance|Driving Distance ...|   54|\n",
      "|    J.B. Holmes|  2010|Driving Distance|Driving Distance ...|  100|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/01 20:37:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "path = \"datasets/\"\n",
    "students = spark.read.csv(path+\"students.csv\", inferSchema=True, header=True)\n",
    "tour = spark.read.csv(path+\"pga_tour_historical.csv\", inferSchema=True, header=True)\n",
    "tour.limit(100).dropna().dropDuplicates().limit(5).show()\n",
    "students.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a4f2-8716-4107-995d-1b5cb6367cef",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72b5f0-c773-4bb2-9337-cbac3444af6f",
   "metadata": {},
   "source": [
    "## [Normal Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#normal-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46824170-7c48-4032-b3e9-45ad39faff0d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.col(col: str) → pyspark.sql.column.Column\n",
    "Returns a Column based on the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5015e644-e4d5-4603-8581-673df1449b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'math score'>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"math score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2955a90-5212-4a5a-820a-35dbf12f8bd5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.column(col: str) → pyspark.sql.column.Column¶\n",
    "Returns a Column based on the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3cf51a61-1d2b-4cbb-8132-c8c834bfa518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'math score'>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column(\"math score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703826d-a1c6-45b6-ade3-e9bf8152d206",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lit(col: Any) → pyspark.sql.column.Column¶\n",
    "Creates a Column of literal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1056d469-7bc1-48de-962f-c52c16507f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|height| id|\n",
      "+------+---+\n",
      "|     5|  0|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(lit(5).alias('height'), df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "28774020-f144-42b6-97c0-1c509465b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|array(1, 2, 3)|\n",
      "+--------------+\n",
      "|     [1, 2, 3]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(lit([1, 2, 3])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56d592-b13d-43fb-a3eb-217bef41117e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.broadcast(df: pyspark.sql.dataframe.DataFrame) → pyspark.sql.dataframe.DataFrame\n",
    "Marks a DataFrame as small enough for use in broadcast joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "58f69f1c-86a7-4f76-8481-96a184e5fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| id|\n",
      "+-----+---+\n",
      "|    1|  1|\n",
      "|    2|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 2, 3, 3, 4], IntegerType())\n",
    "df_small = spark.range(3)\n",
    "df_b = broadcast(df_small)\n",
    "df.join(df_b, df.value == df_small.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cc19d-1100-47a7-80cb-68e70637aabf",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.coalesce(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the first column that is not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b49007ee-6611-473b-a9f1-e74f00cb6473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|NULL|\n",
      "|NULL|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5f2278f0-8f89-4138-acd1-af953215a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f901023e-6f11-4ae4-a101-1fd1390ff58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------------+\n",
      "|   a|   b|coalesce(a, 0.0)|\n",
      "+----+----+----------------+\n",
      "|NULL|NULL|             0.0|\n",
      "|   1|NULL|             1.0|\n",
      "|NULL|   2|             0.0|\n",
      "+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03539dda-597d-4cbf-b3a2-09d4b84c68d0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.input_file_name() → pyspark.sql.column.Column\n",
    "Creates a string column for the file name of the current Spark task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "46708a73-80d9-4029-a77c-f71bbc5663f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\npath = os.path.abspath(__file__)\\ndf = spark.read.text(path)\\ndf.select(input_file_name()).first()\\n'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "path = os.path.abspath(__file__)\n",
    "df = spark.read.text(path)\n",
    "df.select(input_file_name()).first()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537921d2-2cb2-491f-8c41-d60fa74c70aa",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.isnan(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "An expression that returns true if the column is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d3b04-1750-44ec-9b50-5bca3c74d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(\"a\", \"b\", isnan(\"a\").alias(\"r1\"), isnan(df.b).alias(\"r2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391828e4-d391-4bc4-b244-ce8d30122ec5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.isnull(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "An expression that returns true if the column is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "26c0d713-e716-4187-8751-5d2cdec910a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+\n",
      "|   a|   b|   r1|   r2|\n",
      "+----+----+-----+-----+\n",
      "|   1|NULL|false| true|\n",
      "|NULL|   2| true|false|\n",
      "+----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
    "df.select(\"a\", \"b\", isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c257d7a-ae43-4796-a04e-d6d0ac81b92b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.monotonically_increasing_id() → pyspark.sql.column.Column\n",
    "A column that generates monotonically increasing 64-bit integers.\n",
    "\n",
    "The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b234c-83b8-413c-a355-5ac418c57783",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The function is non-deterministic because its result depends on partition IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ef082297-55ac-4531-b1e5-117558a3c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "|                   8589934592|\n",
      "|                   8589934593|\n",
      "|                   8589934594|\n",
      "|                   8589934595|\n",
      "|                   8589934596|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 1, 2).select(monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b48f92-a20d-4e95-8354-38b806b091b0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.named_struct(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Creates a struct with the given field names and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e1f4c96d-79c4-402e-91dd-94dd7ec35e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=Row(x=1, y=2))]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3)], ['a', 'b', 'c'])\n",
    "df.select(named_struct(lit('x'), df.a, lit('y'), df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be428a4-0974-4f39-8da6-54425fd29873",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nanvl(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
    "\n",
    "Both inputs should be floating point columns (DoubleType or FloatType)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9d07d3d1-1dc8-4c72-8d44-e53653fd384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8d44c-639f-480e-8b70-5fb4e5adf4fa",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rand(seed: Optional[int] = None) → pyspark.sql.column.Column\n",
    "Generates a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9990a4-7e4e-4f2c-a334-594b86e2f3ae",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The function is non-deterministic in general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7275af57-228c-496b-bde5-b31a8131c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              rand|\n",
      "+---+------------------+\n",
      "|  0|1.8575681106759028|\n",
      "|  1|1.5288056527339444|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 2, 1, 1).withColumn('rand', rand(seed=42) * 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167499a0-7384-45eb-8ebf-ae8a30dced08",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.randn(seed: Optional[int] = None) → pyspark.sql.column.Column\n",
    "Generates a column with independent and identically distributed (i.i.d.) samples from the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27f459-937b-4cff-bfb0-68d15b2d3b95",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The function is non-deterministic in general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d511e65f-f5d1-465d-81a7-ab39b6521a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|             randn|\n",
      "+---+------------------+\n",
      "|  0| 2.384479054241165|\n",
      "|  1|0.1920934041293524|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 2, 1, 1).withColumn('randn', randn(seed=42)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad8935-54ad-4f58-b1b0-b9e0a463fe01",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.spark_partition_id() → pyspark.sql.column.Column\n",
    "A column for partition ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e3cbb-0b96-47b4-ae67-e403f7f05720",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    This is non deterministic because it depends on data partitioning and task scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b3f3c10f-4fb8-4877-8284-c2fc04ecb6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pid=0), Row(pid=0)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(2)\n",
    "df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c412207-ed97-4dff-b892-d572afc1f122",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.when(condition: pyspark.sql.column.Column, value: Any) → pyspark.sql.column.Column\n",
    "Evaluates a list of conditions and returns one of multiple possible result expressions. If pyspark.sql.Column.otherwise() is not invoked, None is returned for unmatched conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1e9b0b2a-73bf-49b1-8074-f65ad015e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  4|\n",
      "|  4|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(3)\n",
    "df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "51a05500-d2e1-425c-b747-8fd4b016feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|NULL|\n",
      "|   3|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64143eed-78e5-4565-9975-1edb7326620d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bitwise_not(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Computes bitwise not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0e42e324-4eba-4418-b398-b66f0b43eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| ~0|\n",
      "+---+\n",
      "| -1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(bitwise_not(lit(0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "095f2c05-8642-4cc1-b72d-73eff70acbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| ~1|\n",
      "+---+\n",
      "| -2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(bitwise_not(lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d0c78-cf1f-4d57-a379-7cbf7e3d8f6c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bitwiseNOT(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes bitwise not.\n",
    "Deprecated since version 3.2.0: Use bitwise_not() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c231c-ca46-4f15-8b3b-1d0b1fcac9a6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.expr(str: str) → pyspark.sql.column.Column\n",
    "Parses the expression string into the column that it represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "99a01996-bfc7-4915-96c1-14b42c344928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| name|length(name)|\n",
      "+-----+------------+\n",
      "|Alice|           5|\n",
      "|  Bob|           3|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
    "df.select(\"name\", expr(\"length(name)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad63b90-0e70-41d9-a50e-5776ad8014ef",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.greatest(*cols: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null if all parameters are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "66e1e179-9751-411a-9f0a-fef99bbf657f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(greatest=4)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48abfbf-d992-47c6-a207-13a3f4396993",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.least(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null if all parameters are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f8dca10e-d921-4e51-8c61-bedada505a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(least=1)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda1812-1773-4bf1-95e0-b8d54a34cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f985e-91e6-45a1-9753-96ed52a393eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9593a2-91b9-406f-a3d0-ab3ce7c15feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af0c22d6-0d81-4337-84b2-987ae967b7a5",
   "metadata": {},
   "source": [
    "## Math Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833b2a1-bc8b-427d-9abb-64ce1791aefa",
   "metadata": {},
   "source": [
    "## [Datetime Functions]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a3096-c634-43f1-929f-360e3e8ade83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b5c70-3b70-4bac-b5ea-362e2c426c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9ecae-1961-4f5b-9cae-3ebcec909253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c76d5-ab02-4e5d-8aa7-38a4837a4341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2fe26-fefc-4509-946f-1cacee7208e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da92d14-c705-49bc-88e4-1be4d05a9ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4d787-de11-44e2-b9e1-0cbbaa85db5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb5f9a-b0e8-40a7-99bd-eb8fa7f44bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a06536-d0d4-4470-81c0-78092b1dbd37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Collection Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#collection-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1cd6b-72c4-4d26-ae47-ce0a20b68c4c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array(*cols: Union[ColumnOrName, List[ColumnOrName_], Tuple[ColumnOrName_, …]]) → pyspark.sql.column.Column\n",
    "Creates a new array column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75651b35-8c8e-4021-9e0a-0b7ea254eb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(arr=[2, 2]), Row(arr=[5, 5])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
    "df.select(array('age', 'age').alias(\"arr\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eaa294f-0965-4740-898e-581e5214b20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(arr=[2, 2]), Row(arr=[5, 5])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array([df.age, df.age]).alias(\"arr\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77d2f1d3-7c2b-425e-a5f2-989084bf6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array('age', 'age').alias(\"col\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe6393-c7a7-4a04-b924-c5c73e2da4c1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_contains(col: ColumnOrName, value: Any) → pyspark.sql.column.Column\n",
    "Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30336b66-3fbf-4a04-b6f7-6d7153e16e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
    "df.select(array_contains(df.data, \"a\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fb72bd2-8774-41ad-b4d0-ea787f3c757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array_contains(df.data, lit(\"a\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d05c4-a36b-455e-92d3-b9a0a151fe28",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.arrays_overlap(a1: ColumnOrName, a2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d48c337-3432-4a98-bc09-64a44eaa340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlap=True), Row(overlap=False)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
    "df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad68f8f-89ad-47ce-b8b4-6cfa83a69451",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_join(col: ColumnOrName, delimiter: str, null_replacement: Optional[str] = None) → pyspark.sql.column.Column\n",
    "Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7847f-f01f-4f41-9fac-dfe72663bcbf",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "target column to work on.\n",
    "- delimiter: str | \n",
    "delimiter used to concatenate elements\n",
    "- null_replacement: str, optional | \n",
    "if set then null values will be replaced by this value\n",
    "\n",
    "Returns – Column | \n",
    "a column of string type. Concatenated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a38b20c8-546b-48b0-8423-4103215c70c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(joined='a,b,c'), Row(joined='a')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
    "df.select(array_join(df.data, \",\").alias(\"joined\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19a94703-c926-44b6-9e8f-21e75fef9490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(joined='a,b,c'), Row(joined='a,NULL')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1211b9-21e7-44ca-b772-d33f8029cce6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.create_map(*cols: Union[ColumnOrName, List[ColumnOrName_], Tuple[ColumnOrName_, …]]) → pyspark.sql.column.Column\n",
    "Creates a new map column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fea31db8-9125-4f66-a7aa-72a9a049a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(map={'Alice': 2}), Row(map={'Bob': 5})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
    "df.select(create_map('name', 'age').alias(\"map\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e77dfa97-59f7-47d6-a75e-00d9ecbac993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(map={'Alice': 2}), Row(map={'Bob': 5})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(create_map([df.name, df.age]).alias(\"map\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56538c2-f82e-4973-817a-0e05fa0c70c4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.slice(x: ColumnOrName, start: Union[ColumnOrName, int], length: Union[ColumnOrName, int]) → pyspark.sql.column.Column\n",
    "Collection function: returns an array containing all the elements in x from index start (array indices start at 1, or from the end if start is negative) with the specified length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b7ad5-c11f-449e-a016-16634bd7960f",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- x: Column or str  | \n",
    "column name or column containing the array to be sliced\n",
    "- start: Column or str or int | \n",
    "column name, column, or int containing the starting index\n",
    "- length: Column or str or int | \n",
    "column name, column, or int containing the length of the slice\n",
    "\n",
    "Returns – Column | \n",
    "a column of array type. Subset of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "811fefdb-ca06-4df6-b56b-655d1a101902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sliced=[2, 3]), Row(sliced=[5])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
    "df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbc59e-0e2a-4bb3-999e-7b446dc8401f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.concat(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Concatenates multiple input columns together into a single column. The function works with strings, numeric, binary and compatible array columns.\n",
    "\n",
    "See also – pyspark.sql.functions.array_join() | \n",
    "to concatenate string columns with delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b0dfe58-69cb-4893-8d1f-8d60d21220b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd123')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df = df.select(concat(df.s, df.d).alias('s'))\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cf8078a-f842-4cfb-a4ec-ec4f819a88c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[s: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16b92baa-0f69-4393-99c3-dbbfdb92d832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
    "df = df.select(concat(df.a, df.b, df.c).alias(\"arr\"))\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1370eb24-9712-40fe-836e-c9ee7d34256d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arr: array<bigint>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160087b4-ab1c-42a4-8528-25e37836103c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_position(col: ColumnOrName, value: Any) → pyspark.sql.column.Column\n",
    "Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704989f-0630-414d-8bd1-b8599aa88061",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The position is not zero based, but 1 based index. Returns 0 if the given value could not be found in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9184b18a-a264-49b0-8fae-5da3e3b8dfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_position(data, a)=3), Row(array_position(data, a)=0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
    "df.select(array_position(df.data, \"a\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d024ca9-e3dc-41ef-a8fa-239e3bc9b1f8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.element_at(col: ColumnOrName, extraction: Any) → pyspark.sql.column.Column\n",
    "Collection function: Returns element of array at given index in extraction if col is array. Returns value for the given key in extraction if col is map. If position is negative then location of the element will start from end, if number is outside the array boundaries then None will be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0204d0-c55d-4aca-8a30-c266747e18f1",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The position is not zero based, but 1 based index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d9984a4-20e9-4cf8-9d37-086fd42fabe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(element_at(data, 1)='a')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
    "df.select(element_at(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b4a2120-7abd-4026-a51e-9bd77a0fe3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(element_at(data, -1)='c')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(element_at(df.data, -1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b0d012e-9c64-40a0-8bc6-92d92cba4647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(element_at(data, a)=1.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
    "df.select(element_at(df.data, lit(\"a\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9272475-cb87-4ba4-8262-9384d3a92c3b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_append(col: ColumnOrName, value: Any) → pyspark.sql.column.Column\n",
    "Collection function: returns an array of the elements in col1 along with the added element in col2 at the last of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce735414-0acb-47ec-a1ea-4ae93acf977e",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe7575b4-326b-48be-ba38-e0e91c12857f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_append(c1, c2)=['b', 'a', 'c', 'c'])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n",
    "df.select(array_append(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8ec37a1-b54c-4407-8581-0b3bc80d4c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_append(c1, x)=['b', 'a', 'c', 'x'])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array_append(df.c1, 'x')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5062260-6c1b-4bb3-8334-25b6633ae7b1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_size(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the total number of elements in the array. The function returns null for null input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0cf1f9b-4c6f-4e55-b397-e41d0b938906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3), Row(r=None)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, 3],), (None,)], ['data'])\n",
    "df.select(array_size(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6158bb-d7c1-4a62-9487-3d3067828091",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_sort(col: ColumnOrName, comparator: Optional[Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) → pyspark.sql.column.Column\n",
    "Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94b9d2-33a5-4192-ba5b-9ed12c787312",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- comparator: callable, optional | \n",
    "A binary (Column, Column) -> Column: .... The comparator will take two arguments representing two elements of the array. It returns a negative integer, 0, or a positive integer as the first element is less than, equal to, or greater than the second element. If the comparator function returns null, the function will fail and raise an error.\n",
    "\n",
    "Returns – Column | \n",
    "sorted array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f1b56f6-0bc5-4a44-9032-fdc2522d2ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
    "df.select(array_sort(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1766355e-7e0e-4865-b87d-7b785e07473a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=['foobar', 'foo', None, 'bar']), Row(r=['foo']), Row(r=[])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n",
    "df.select(array_sort(\n",
    "    \"data\",\n",
    "    lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n",
    ").alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1650b-f2d5-4528-b8cf-34a21e7621ad",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_insert(arr: ColumnOrName, pos: Union[ColumnOrName, int], value: Any) → pyspark.sql.column.Column\n",
    "Collection function: adds an item into a given array at a specified array index. Array indices start at 1, or start from the end if index is negative. Index above array size appends the array, or prepends the array if index is negative, with ‘null’ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd4b19-e245-4b56-b45b-41d48d0fe319",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- arr: Column or str | \n",
    "name of column containing an array\n",
    "- pos: Column or str or int | \n",
    "name of Numeric type column indicating position of insertion (starting at index 1, negative position is a start from the back of the array)\n",
    "- value :\n",
    "a literal value, or a Column expression.\n",
    "\n",
    "Returns – Column | \n",
    "an array of values, including the new specified value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf36952-5e2d-4a3d-bba4-6eb5fade46cb",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8764a4b-c296-46c1-b87f-355244c8137d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=['a', 'd', 'b', 'c']), Row(data=['c', 'b', 'd', 'a'])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(['a', 'b', 'c'], 2, 'd'), (['c', 'b', 'a'], -2, 'd')],\n",
    "    ['data', 'pos', 'val']\n",
    ")\n",
    "df.select(array_insert(df.data, df.pos.cast('integer'), df.val).alias('data')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cb49e1b-5d8c-4ae2-a3fb-582e0cf83993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=['a', 'b', 'c', None, 'hello']),\n",
       " Row(data=['c', 'b', 'a', None, 'hello'])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array_insert(df.data, 5, 'hello').alias('data')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c3709-34ee-4d7a-a569-3b179ced5ace",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_remove(col: ColumnOrName, element: Any) → pyspark.sql.column.Column\n",
    "Collection function: Remove all elements that equal to element from the given array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4deb3f5-0c2a-4491-a8fd-aa338e233863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
    "df.select(array_remove(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210f155-cabb-43e0-b396-485d13340066",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_prepend(col: ColumnOrName, value: Any) → pyspark.sql.column.Column\n",
    "Collection function: Returns an array containing element as well as all elements from array. The new element is positioned at the beginning of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0002db77-2ef5-4da2-bf75-7bef1a8e7250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_prepend(data, 1)=[1, 2, 3, 4]), Row(array_prepend(data, 1)=[1])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 3, 4],), ([],)], ['data'])\n",
    "df.select(array_prepend(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec9ff8-fa9d-45ab-b4e8-de10f9fa4356",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_distinct(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: removes duplicate values from the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfa9f46a-dc67-44ec-9e36-fb34d9c54881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
    "df.select(array_distinct(df.data)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19d68c-0033-413b-a051-4da29c455933",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_intersect(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e44b1c1-58a7-4dfa-a688-a68299180646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_intersect(c1, c2)=['a', 'c'])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_intersect(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0066a-1575-47a6-a6e8-898781931757",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_union(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Collection function: returns an array of the elements in the union of col1 and col2, without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d5216385-4d50-4046-9ed0-9f4358867de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_union(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94693b52-4f48-4988-aa9d-0e90194e91b8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_except(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns an array of the elements in col1 but not in col2, without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8926ff27-2fae-45e2-9a5c-d680e4a7db67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_except(c1, c2)=['b'])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_except(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996353d-5dfb-45c6-9ad3-dc579c8b3011",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_compact(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: removes null values from the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72987c57-7333-4078-b504-2dc6062adbb8",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae2e882e-adca-4f87-9c64-b81b1a081445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_compact(data)=[1, 2, 3]), Row(array_compact(data)=[4, 5, 4])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, None, 2, 3],), ([4, 5, None, 4],)], ['data'])\n",
    "df.select(array_compact(df.data)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa799d-bfb4-4e28-ab56-025ed668612f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.transform(col: ColumnOrName, f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) → pyspark.sql.column.Column\n",
    "Returns an array of elements after applying a transformation to each element in the input array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd6c55-cc52-4be7-a7c3-de582eb88a41",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "a function that is applied to each element of the input array. Can take one of the following forms:\n",
    "- * Unary (x: Column) -> Column: ...\n",
    "- * Binary (x: Column, i: Column) -> Column..., where the second argument is\n",
    "a 0-based index of the element.\n",
    "and can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "a new array of transformed elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ca70db3-d675-4479-bf2d-7454b8552bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     doubled|\n",
      "+------------+\n",
      "|[2, 4, 6, 8]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
    "df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18ca68d5-3951-401e-9880-cae13fb66cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    alternated|\n",
      "+--------------+\n",
      "|[1, -2, 3, -4]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def alternate(x, i):\n",
    "    return when(i % 2 == 0, x).otherwise(-x)\n",
    "\n",
    "df.select(transform(\"values\", alternate).alias(\"alternated\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d4d2b-3e9f-451e-a6d7-efa19630d3b3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.exists(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Returns whether a predicate holds for one or more elements in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eeed44-656f-4ce0-988e-846d88898206",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "(x: Column) -> Column: ... returning the Boolean expression. Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "True if “any” element of an array evaluates to True when passed as an argument to given function and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07c5bb98-cad6-4bdf-abb8-b01f0d202dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|any_negative|\n",
      "+------------+\n",
      "|       false|\n",
      "|        true|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
    "df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6affa-844c-4c33-b588-274fe5d8932a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.forall(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Returns whether a predicate holds for every element in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866cbeae-8418-42d3-a40f-8fd9f59bd34c",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "(x: Column) -> Column: ... returning the Boolean expression. Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "True if “all” elements of an array evaluates to True when passed as an argument to given function and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec92f9b9-9e85-40a6-bf1d-dac468b57b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|all_foo|\n",
      "+-------+\n",
      "|  false|\n",
      "|  false|\n",
      "|   true|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a695c99-ce22-484a-9a67-37b66ff257dc",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.filter(col: ColumnOrName, f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) → pyspark.sql.column.Column\n",
    "Returns an array of elements for which a predicate holds in a given array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab4f82-84f3-435c-9c75-33ba271e9637",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "A function that returns the Boolean expression. Can take one of the following forms:\n",
    "- * Unary (x: Column) -> Column: ...\n",
    "- * Binary (x: Column, i: Column) -> Column..., where the second argument is\n",
    "a 0-based index of the element.\n",
    "and can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "filtered array of elements where given function evaluated to True when passed as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cac8417-43b6-4aba-9542-68d5b41ae06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|after_second_quarter    |\n",
      "+------------------------+\n",
      "|[2018-09-20, 2019-07-01]|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "def after_second_quarter(x):\n",
    "    return month(to_date(x)) > 6\n",
    "\n",
    "df.select(\n",
    "    filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f580330-b5bb-4e76-8d56-fff005bf7104",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.aggregate(col: ColumnOrName, initialValue: ColumnOrName, merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) → pyspark.sql.column.Column\n",
    "Applies a binary operator to an initial state and all elements in the array, and reduces this to a single state. The final state is converted into the final result by applying a finish function.\n",
    "\n",
    "Both functions can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce538c8-50bc-46b0-a252-7f31db928a6b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- initial: ValueColumn or str | \n",
    "initial value. Name of column or expression\n",
    "- merge: function | \n",
    "a binary function (acc: Column, x: Column) -> Column... returning expression of the same type as zero\n",
    "- finish: function | \n",
    "an optional unary function (x: Column) -> Column: ... used to convert accumulated value.\n",
    "\n",
    "Returns – Column | \n",
    "final value after aggregate function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c4e2073-8462-423e-8f23-60e0a50247ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| sum|\n",
      "+----+\n",
      "|42.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
    "df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0350a37c-2992-4b01-a129-7388bbc777f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 8.4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge(acc, x):\n",
    "    count = acc.count + 1\n",
    "    sum = acc.sum + x\n",
    "    return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
    "\n",
    "df.select(\n",
    "    aggregate(\n",
    "        \"values\",\n",
    "        struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
    "        merge,\n",
    "        lambda acc: acc.sum / acc.count,\n",
    "    ).alias(\"mean\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdca2d1-5234-4db2-a697-44d1ffdd9c3c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.zip_with(left: ColumnOrName, right: ColumnOrName, f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Merge two given arrays, element-wise, into a single array using a function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07e3cf-a34e-4a85-af21-a573fd4aa0b0",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- left: Column or str | \n",
    "name of the first column or expression\n",
    "- right: Column or str | \n",
    "name of the second column or expression\n",
    "- f: function | \n",
    "a binary function (x1: Column, x2: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "array of calculated values derived by applying given function to each pair of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1dcaefe-633f-4439-b275-b4ad1b4a3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|powers                     |\n",
      "+---------------------------+\n",
      "|[1.0, 9.0, 625.0, 262144.0]|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
    "df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45cc392a-bf71-4a3d-add0-5dbdb586a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            xs_ys|\n",
      "+-----------------+\n",
      "|[foo_1, bar_2, 3]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
    "df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6cf7d-70b5-4891-b02b-4e68b45f3b63",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.transform_keys(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Applies a function to every key-value pair in a map and returns a map with the results of those applications as the new keys for the pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b7c02-e210-4401-b41b-8df8a44cdc71",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "a new map of enties where new keys were calculated by applying given function to each key value argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8e76b51-9da9-4118-b714-776c94a63037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BAR', 2.0), ('FOO', -2.0)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
    "row = df.select(transform_keys(\n",
    "    \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
    ").head()\n",
    "sorted(row[\"data_upper\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03557751-241f-4f22-b1a3-d387c102a995",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.transform_values(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column¶\n",
    "Applies a function to every key-value pair in a map and returns a map with the results of those applications as the new values for the pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134266-6eb7-441f-9b35-9612aa9d8521",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "a new map of enties where new values were calculated by applying given function to each key value argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f71e1270-38b9-4b08-969f-9395c3a770bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', 20.0), ('OPS', 34.0), ('SALES', 2.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
    "row = df.select(transform_values(\n",
    "    \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
    ").alias(\"new_data\")).head()\n",
    "sorted(row[\"new_data\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747bd45-f81a-4567-9164-853516d04572",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_filter(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Returns a map whose key-value pairs satisfy a predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dda782-93c1-4605-9f3f-dd3b5aa024ff",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- f: function | \n",
    "a binary function (k: Column, v: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns– Column | \n",
    "filtered map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69d8debb-c44b-46fd-aa7d-520ab8b28bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baz', 32.0), ('foo', 42.0)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
    "row = df.select(map_filter(\n",
    "    \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
    ").head()\n",
    "sorted(row[\"data_filtered\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb2ba9-13b9-4d57-8bb6-4db7367f331b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_from_arrays(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Creates a new map from two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba45ae5c-f7cb-458a-ac26-08d1f974b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             col|\n",
      "+----------------+\n",
      "|{2 -> a, 5 -> b}|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
    "df = df.select(map_from_arrays(df.k, df.v).alias(\"col\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "145cf291-07d1-4caf-81ad-c2b7a91dd856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: map (nullable = true)\n",
      " |    |-- key: long\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d483ca-67ed-489f-bb05-baf75a5d62e1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_zip_with(col1: ColumnOrName, col2: ColumnOrName, f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Merge two given maps, key-wise into a single map using a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b26e23-2f15-4e41-92ef-7037fbbe3783",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col1: Column or str | \n",
    "name of the first column or expression\n",
    "- col2: Column or str | \n",
    "name of the second column or expression\n",
    "- f: function | \n",
    "a ternary function (k: Column, v1: Column, v2: Column) -> Column... Can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052).\n",
    "\n",
    "Returns – Column | \n",
    "zipped map where entries are calculated by applying given function to each pair of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "280960b0-9052-405d-8148-456f04c9b047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', 48.0), ('SALES', 16.8)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
    "    (\"id\", \"base\", \"ratio\")\n",
    ")\n",
    "row = df.select(map_zip_with(\n",
    "    \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
    ").head()\n",
    "sorted(row[\"updated_data\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284d071-c022-4c06-a28d-6460985d9daa",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.explode(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a058df1-9050-4401-99ed-019fc1ebe1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(anInt=1), Row(anInt=2), Row(anInt=3)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "df.select(explode(df.intlist).alias(\"anInt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67b5d7a3-27ce-4661-8048-09b48119825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(df.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b52f9-3a03-468e-93a6-dc13a30ad951",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.explode_outer(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19696c6e-ec2d-47ff-ae6a-fb89f86e9ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+\n",
      "| id|  an_array| key|value|\n",
      "+---+----------+----+-----+\n",
      "|  1|[foo, bar]|   x|  1.0|\n",
      "|  2|        []|NULL| NULL|\n",
      "|  3|      NULL|NULL| NULL|\n",
      "+---+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a3d256d-1367-44c1-88a1-babc17ed2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+\n",
      "| id|     a_map| col|\n",
      "+---+----------+----+\n",
      "|  1|{x -> 1.0}| foo|\n",
      "|  1|{x -> 1.0}| bar|\n",
      "|  2|        {}|NULL|\n",
      "|  3|      NULL|NULL|\n",
      "+---+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e424dd-a875-4044-a3ad-4b4faa2cdf55",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.posexplode(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new row for each element with position in the given array or map. Uses the default column name pos for position, and col for elements in the array and key and value for elements in the map unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8b961ad-4de1-45ee-85f9-9fc639ba1848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "df.select(posexplode(df.intlist)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "03e37c1a-5d84-4c22-80f5-b42aaf883983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|pos|key|value|\n",
      "+---+---+-----+\n",
      "|  0|  a|    b|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(posexplode(df.mapfield)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b707fb7-af68-4244-9ca0-7cc45624eb14",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.posexplode_outer(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new row for each element with position in the given array or map. Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced. Uses the default column name pos for position, and col for elements in the array and key and value for elements in the map unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d27c08dc-9522-49aa-a983-f0384138b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+----+-----+\n",
      "| id|  an_array| pos| key|value|\n",
      "+---+----------+----+----+-----+\n",
      "|  1|[foo, bar]|   0|   x|  1.0|\n",
      "|  2|        []|NULL|NULL| NULL|\n",
      "|  3|      NULL|NULL|NULL| NULL|\n",
      "+---+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7393ac6-7e7f-48ff-9b0e-15758e2d6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+----+\n",
      "| id|     a_map| pos| col|\n",
      "+---+----------+----+----+\n",
      "|  1|{x -> 1.0}|   0| foo|\n",
      "|  1|{x -> 1.0}|   1| bar|\n",
      "|  2|        {}|NULL|NULL|\n",
      "|  3|      NULL|NULL|NULL|\n",
      "+---+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0fe74-c2bb-416c-acf4-39170c8d2dcb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.inline(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Explodes an array of structs into a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3455419-3116-4607-84b4-1d96e23e1acd",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "422e6aca-97c0-4b19-8a6e-9f8654ed7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(structlist=[Row(a=1, b=2), Row(a=3, b=4)])])\n",
    "df.select(inline(df.structlist)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db590989-7425-4315-abf6-3378a1c46aac",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.inline_outer(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Explodes an array of structs into a table. Unlike inline, if the array is null or empty then null is produced for each nested column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2da19-78de-46cc-8c51-389e214ba704",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6449e998-d216-423f-ba0f-a1b84809dedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|   a|   b|\n",
      "+---+----+----+\n",
      "|  1|   1|   2|\n",
      "|  1|   3|   4|\n",
      "|  2|NULL|NULL|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(id=1, structlist=[Row(a=1, b=2), Row(a=3, b=4)]),\n",
    "    Row(id=2, structlist=[])\n",
    "])\n",
    "df.select('id', inline_outer(df.structlist)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffccee-b1dd-4540-9207-c91c46718ee9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.get(col: ColumnOrName, index: Union[ColumnOrName, int]) → pyspark.sql.column.Column\n",
    "Collection function: Returns element of array at given (0-based) index. If the index points outside of the array boundaries, then this function returns NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3695cd8-67e6-487c-9c86-2a0def68c353",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The position is not 1 based, but 0 based index. Supports Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b15d7fa2-2970-4b53-9763-92d500a38453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|get(data, 1)|\n",
      "+------------+\n",
      "|           b|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 1)], ['data', 'index'])\n",
    "df.select(get(df.data, 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99ac792b-f083-4c3d-b4ba-c42082852b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|get(data, -1)|\n",
      "+-------------+\n",
      "|         NULL|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(get(df.data, -1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "549fb57d-3ba0-47da-9de6-ff0c9c03cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|get(data, 3)|\n",
      "+------------+\n",
      "|        NULL|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(get(df.data, 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6dcc75ad-d10c-4110-a87b-09b2eb873a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|get(data, index)|\n",
      "+----------------+\n",
      "|               b|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(get(df.data, \"index\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15a8c98f-334f-4d1a-925d-f59dce19e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|get(data, (index - 1))|\n",
      "+----------------------+\n",
      "|                     a|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(get(df.data, col(\"index\") - 1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f6403-e0e3-45fe-bdf4-2e0c5984460d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.get_json_object(col: ColumnOrName, path: str) → pyspark.sql.column.Column\n",
    "Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "259ccccd-b718-46c8-a9db-5054a094f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
    "                  get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860b226-23b4-453a-93ea-1930e0140f45",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.json_tuple(col: ColumnOrName, *fields: str) → pyspark.sql.column.Column\n",
    "Creates a new row for a json column according to the given field names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496561d-a3d2-4aaf-94da-efc8f28e326d",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "string column in json format\n",
    "- fields: str | \n",
    "a field or fields to extract\n",
    "\n",
    "Returns – Column | \n",
    "a new row for each given field value from json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fb59a22-05f4-45a9-a59b-5ceeb850e143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ata = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e51a98-ccb3-441b-aab2-3a40a4c10823",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.from_json(col: ColumnOrName, schema: Union[pyspark.sql.types.ArrayType, pyspark.sql.types.StructType, pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column\n",
    "Parses a column containing a JSON string into a MapType with StringType as keys type, StructType or ArrayType with the specified schema. Returns null, in the case of an unparseable string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071629a-3736-4fc9-9e62-334ba6e0a8f9",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "a column or column name in JSON format\n",
    "- schema: DataType or str | \n",
    "a StructType, ArrayType of StructType or Python string literal with a DDL-formatted string to use when parsing the json column\n",
    "- options: dict, optional | \n",
    "options to control parsing. accepts the same options as the json datasource. See Data Source Option for the version you use.\n",
    "\n",
    "Returns – Column | \n",
    "a new column of complex type from given JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "05030b1e-799c-439c-af30-f1cad32ebc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=Row(a=1))]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, '''{\"a\": 1}''')]\n",
    "schema = StructType([StructField(\"a\", IntegerType())])\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aee3d569-fb2c-4d1c-ac78-b5cac489dc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=Row(a=1))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4e9919fb-2a23-4142-9c26-d7e61e411463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json={'a': 1})]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3fd85b4e-b7b6-4e2e-a0fc-4550d66ae930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=[Row(a=1)])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, '''[{\"a\": 1}]''')]\n",
    "schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "55c9ee23-1184-48cc-9098-8f066c90ca15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=Row(a=None))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b30bff5-9c3c-486e-b471-ec4f47bbbe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=[1, 2, 3])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, '''[1, 2, 3]''')]\n",
    "schema = ArrayType(IntegerType())\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ce42f-4718-47f6-aa48-8a81d301cf17",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.schema_of_json(json: ColumnOrName, options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column\n",
    "Parses a JSON string and infers its schema in DDL format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac5254-7744-4094-9134-f5d3f85b6eaf",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- json: Column or str | \n",
    "a JSON string or a foldable string column containing a JSON string.\n",
    "- optionsdict, optional | \n",
    "options to control parsing. accepts the same options as the JSON datasource. See Data Source Option for the version you use. (Changed in version 3.0.0: It accepts options parameter to control schema inferring.)\n",
    "\n",
    "Returns – Column | \n",
    "a string representation of a StructType parsed from given JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4e60c571-1fbe-4fe6-8995-f360e296904e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='STRUCT<a: BIGINT>')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c184a712-3c6d-44f1-8655-4fe9bd54c605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='STRUCT<a: BIGINT>')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
    "df.select(schema.alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec95b3-f1de-4512-9d13-8e02bd8e83a9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_json(col: ColumnOrName, options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column\n",
    "Converts a column containing a StructType, ArrayType or a MapType into a JSON string. Throws an exception, in the case of an unsupported type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a810ec-28b9-4963-b0a0-da7a8f417afb",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "name of column containing a struct, an array or a map.\n",
    "- options: dict, optional | \n",
    "options to control converting. accepts the same options as the JSON datasource. See Data Source Option for the version you use. Additionally the function supports the pretty option which enables pretty JSON generation.\n",
    "\n",
    "Returns – Column | \n",
    "JSON object as string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6946d4d-f71a-418a-9641-4213adfb33f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='{\"age\":2,\"name\":\"Alice\"}')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, Row(age=2, name='Alice'))]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e5c379a-720c-4ad1-9ef0-671ee395d999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ff078a1-70d2-4400-8f8f-2da2fbeaaa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='{\"name\":\"Alice\"}')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, {\"name\": \"Alice\"})]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f500f8c9-0d84-4f29-b06f-b6f7ce439a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d807a83-8d41-4ce3-b4f0-89679f77e9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json='[\"Alice\",\"Bob\"]')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, [\"Alice\", \"Bob\"])]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_json(df.value).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b050f3-93fc-435e-a9ab-3c53fb106bd2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.json_array_length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of elements in the outermost JSON array. NULL is returned in case of any other valid JSON string, NULL or an invalid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03a3d416-e2c1-454f-ace3-e2746ecbc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None), Row(r=3), Row(r=0)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), ('[1, 2, 3]',), ('[]',)], ['data'])\n",
    "df.select(json_array_length(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353998b-e03e-4558-905a-c5f43db5d14c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.json_object_keys(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns all the keys of the outermost JSON object as an array. If a valid JSON object is given, all the keys of the outermost object will be returned as an array. If it is any other valid JSON string, an invalid JSON string or an empty string, the function returns null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9117d500-d67e-4bca-ab39-f62b1e80d4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None), Row(r=[]), Row(r=['key1', 'key2'])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), ('{}',), ('{\"key1\":1, \"key2\":2}',)], ['data'])\n",
    "df.select(json_object_keys(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead777ef-8ff8-4dd7-96f4-fc402909afe4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.size(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns the length of the array or map stored in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57349fcc-ac3c-48dc-8724-6badede19188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
    "df.select(size(df.data)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6dc354-2936-4561-9a31-6f5b3664c977",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.cardinality(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns the length of the array or map stored in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5ffc664d-0be9-419d-a0fa-8bd3748f69e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|cardinality(data)|\n",
      "+-----------------+\n",
      "|                3|\n",
      "|                1|\n",
      "|                0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [([1, 2, 3],),([1],),([],)], ['data']\n",
    ").select(cardinality(\"data\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b1bec-1ffd-4263-82e8-3848c109624e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.struct(*cols: Union[ColumnOrName, List[ColumnOrName_], Tuple[ColumnOrName_, …]]) → pyspark.sql.column.Column\n",
    "Creates a new struct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "19bb48d5-38b1-4376-b179-9acb22397aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
    "df.select(struct('age', 'name').alias(\"struct\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b05ff5ba-2acd-41a7-962f-5b7f6f0830e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(struct([df.age, df.name]).alias(\"struct\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a095d2-9735-4005-befa-ba0dc06d447c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sort_array(col: ColumnOrName, asc: bool = True) → pyspark.sql.column.Column\n",
    "Collection function: sorts the input array in ascending or descending order according to the natural ordering of the array elements. Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ff474-d3dd-4678-9096-52222a4ba36f",
   "metadata": {},
   "source": [
    "##### Parameters: \n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- asc: bool, optional | \n",
    "whether to sort in ascending or descending order. If asc is True (default) then ascending and if False then descending.\n",
    "\n",
    "Returns – Column | \n",
    "sorted array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1129f1c7-2cbb-4bf5-99ab-2baef1c39947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
    "df.select(sort_array(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3c050a1f-8cc6-424f-9a42-f7f3201e9e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(sort_array(df.data, asc=False).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e752f8-1e1a-49d0-a411-43dab0f742e1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_max(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns the maximum value of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a341519-0646-4f76-ba61-96248974c4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max=3), Row(max=10)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_max(df.data).alias('max')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45492b93-54f0-411c-a72c-2b68f497ad00",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_min(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns the minimum value of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "697e3130-08ad-4ad1-bc39-2043ea54d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min=1), Row(min=-1)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_min(df.data).alias('min')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270a912-8bab-4c5f-bacb-0ea8d55dfcd3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.shuffle(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: Generates a random permutation of the given array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a96c3-bcb0-40e1-b651-f5003d5169a1",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    The function is non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9f0e9a01-c619-425a-8429-723903db6e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=[20, 1, 3, 5]), Row(s=[None, 3, 20, 1])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
    "df.select(shuffle(df.data).alias('s')).collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1de8d5-6489-42e9-bda8-3782409d28a7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.reverse(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: returns a reversed string or an array with reverse order of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9f5bd9e5-ee0c-4d20-a891-76fccf8cb348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='LQS krapS')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
    "df.select(reverse(df.data).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c9b2654-f805-468e-827a-b0dc7644b207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
    "df.select(reverse(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6604733-29f3-4358-8404-72afafbe2703",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.flatten(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bb04d3c1-d425-44e9-9ff6-6e7ef986a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|data                    |\n",
      "+------------------------+\n",
      "|[[1, 2, 3], [4, 5], [6]]|\n",
      "|[NULL, [4, 5]]          |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2e5de988-7242-4488-8ea0-96808eec9129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                 r|\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "|              NULL|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(flatten(df.data).alias('r')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1af09-c16a-4deb-a5b4-27a204cb9682",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sequence(start: ColumnOrName, stop: ColumnOrName, step: Optional[ColumnOrName] = None) → pyspark.sql.column.Column[source]\n",
    "Generate a sequence of integers from start to stop, incrementing by step. If step is not set, incrementing by 1 if start is less than or equal to stop, otherwise -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1d9dc-152e-4e81-a3dc-b827467a11f9",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- start: Column or str | \n",
    "starting value (inclusive)\n",
    "- stop: Column or str | \n",
    "last values (inclusive)\n",
    "- step: Column or str, optional | \n",
    "value to add to current to get next element (default is 1)\n",
    "\n",
    "Returns – Column | \n",
    "an array of sequence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0ef0caf2-c964-4ccf-9ccb-19c98872aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[-2, -1, 0, 1, 2])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
    "df1.select(sequence('C1', 'C2').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "28eaa988-4158-42d1-bb48-484fe4c86e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[4, 2, 0, -2, -4])]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
    "df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8f9e5-3529-4b1f-8ccc-62a3d3030a3c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_repeat(col: ColumnOrName, count: Union[ColumnOrName, int]) → pyspark.sql.column.Column\n",
    "Collection function: creates an array containing a column repeated count times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ca7cd596-1d53-4500-92f8-2c0e35a8a48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=['ab', 'ab', 'ab'])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ab',)], ['data'])\n",
    "df.select(array_repeat(df.data, 3).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7665f7e-1d2c-42bd-87fd-7211e43cd93b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_contains_key(col: ColumnOrName, value: Any) → pyspark.sql.column.Column\n",
    "Returns true if the map contains the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "689f407e-9e3e-4691-8ed6-51c422facc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|array_contains(map_keys(data), 1)|\n",
      "+---------------------------------+\n",
      "|                             true|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.select(map_contains_key(\"data\", 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d8cb7e1f-eff5-4928-af6e-1714645661d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_contains(map_keys(data), -1)|\n",
      "+----------------------------------+\n",
      "|                             false|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(map_contains_key(\"data\", -1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38859d00-9ed1-4ec8-98bd-5c2802ba202d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_keys(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: Returns an unordered array containing the keys of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ca8aaa87-c336-483b-9025-b0fadc70f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  keys|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.select(map_keys(\"data\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd7f1c-ffef-405d-a9ce-102aa1c2ee25",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_values(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Collection function: Returns an unordered array containing the values of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88015462-5ab9-4155-b7b3-1ea60271c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|[a, b]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.select(map_values(\"data\").alias(\"values\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047ad87-3304-4e60-987c-86983757162b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_entries(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: Returns an unordered array of all entries in the given map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7b4cbe14-dfe4-4b35-b14a-e575bd178e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|         entries|\n",
      "+----------------+\n",
      "|[{1, a}, {2, b}]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df = df.select(map_entries(\"data\").alias(\"entries\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b087291d-ec11-4c95-923e-dff20236c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entries: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- key: integer (nullable = false)\n",
      " |    |    |-- value: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6b8bd-f0c2-4f67-af7b-bc97b96b0c20",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_from_entries(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Collection function: Converts an array of entries (key value struct types) to a map of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8eac2538-2f4d-4d4d-9c6b-23fdc67e2c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             map|\n",
      "+----------------+\n",
      "|{1 -> a, 2 -> b}|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
    "df.select(map_from_entries(\"data\").alias(\"map\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd10d2-3bb5-4736-90fd-fd644e3a3dc6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.arrays_zip(*cols: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays. If one of the arrays is shorter than others then resulting struct type value will be a null for missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2735752a-7cf3-4dcc-99d0-038f96c1f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|zipped                              |\n",
      "+------------------------------------+\n",
      "|[{1, 2, 3}, {2, 4, 6}, {3, 6, NULL}]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3], [2, 4, 6], [3, 6])], ['vals1', 'vals2', 'vals3'])\n",
    "df = df.select(arrays_zip(df.vals1, df.vals2, df.vals3).alias('zipped'))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "08168755-f68a-4ff7-9b22-0283b08b41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zipped: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- vals1: long (nullable = true)\n",
      " |    |    |-- vals2: long (nullable = true)\n",
      " |    |    |-- vals3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b8c15-e8f5-4ade-9349-c4b20735c1be",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.map_concat(*cols: Union[ColumnOrName, List[ColumnOrName_], Tuple[ColumnOrName_, …]]) → pyspark.sql.column.Column\n",
    "Returns the union of all the given maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f6bc8c7-144c-4f26-9380-f4298bd0e674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|map3                    |\n",
      "+------------------------+\n",
      "|{1 -> a, 2 -> b, 3 -> c}|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
    "df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31443b91-542f-4d78-bf0c-2b28e525a8d8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.from_csv(col: ColumnOrName, schema: Union[pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column\n",
    "Parses a column containing a CSV string to a row with the specified schema. Returns null, in the case of an unparseable string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014e4a0-8a65-434a-94f2-40429efa9f93",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "a column or column name in CSV format\n",
    "- schema: class:`~pyspark.sql.Column` or str | \n",
    "a column, or Python string literal with schema in DDL format, to use when parsing the CSV column.\n",
    "- options: dict, optional | \n",
    "options to control parsing. accepts the same options as the CSV datasource. See Data Source Option for the version you use.\n",
    "\n",
    "Returns– Column | \n",
    "a column of parsed CSV values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bb7f6901-92c0-496e-ba28-92ad2dbc3a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv=Row(a=1, b=2, c=3))]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"1,2,3\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0fb96b8c-067a-4554-8e12-a4628e231fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv=Row(_c0=1, _c1=2, _c2=3))]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = data[0][0]\n",
    "df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "155931fd-cb6c-482d-9fe7-a356c1681917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv=Row(s='abc'))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"   abc\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "options = {'ignoreLeadingWhiteSpace': True}\n",
    "df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c89dd0-074d-40ba-b4e8-b55d893c6a5e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.schema_of_csv(csv: ColumnOrName, options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column\n",
    "Parses a CSV string and infers its schema in DDL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1058605f-070d-4d32-ade8-0dad08a959f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv='STRUCT<_c0: INT, _c1: STRING>')]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a283ec49-150b-4106-bb4b-c6f6d8d4e2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv='STRUCT<_c0: INT, _c1: STRING>')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b42173-5457-459e-a295-3cb2a07c4a7a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.str_to_map(text: ColumnOrName, pairDelim: Optional[ColumnOrName] = None, keyValueDelim: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Creates a map after splitting the text into key/value pairs using delimiters. Both pairDelim and keyValueDelim are treated as regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788370d2-b834-476f-913e-7592cf626240",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- text: Column or str | \n",
    "Input column or strings.\n",
    "- pairDelim: Column or str, optional | \n",
    "delimiter to use to split pair.\n",
    "- keyValueDelim: Column or str, optional | \n",
    "delimiter to use to split key/value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "79bf2e31-cad7-4f9c-b2c6-9312efcdc1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r={'a': '1', 'b': '2', 'c': '3'})]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
    "df.select(str_to_map(df.e, lit(\",\"), lit(\":\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3a8ae744-8509-4d71-9091-5254e879bf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r={'a': '1', 'b': '2', 'c': '3'})]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
    "df.select(str_to_map(df.e, lit(\",\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a56ca843-1a7d-46fa-aa9b-79af60047d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r={'a': '1', 'b': '2', 'c': '3'})]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
    "df.select(str_to_map(df.e).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118a414-3bf9-421b-8b90-efb8ec28406f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_csv(col: ColumnOrName, options: Optional[Dict[str, str]] = None) → pyspark.sql.column.Column¶\n",
    "Converts a column containing a StructType into a CSV string. Throws an exception, in the case of an unsupported type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dc982-72c3-4512-ae5f-005b8f75e6ba",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- col: Column or str | \n",
    "name of column containing a struct.\n",
    "- options: dict, optional | \n",
    "options to control converting. accepts the same options as the CSV datasource. See Data Source Option for the version you use.\n",
    "\n",
    "Returns – Column | \n",
    "a CSV string converted from given StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bb555ad9-863e-4360-9ce5-1a8167f373b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(csv='2,Alice')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, Row(age=2, name='Alice'))]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(to_csv(df.value).alias(\"csv\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e187a-da07-421f-b752-40eccccc2f78",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.try_element_at(col: ColumnOrName, extraction: ColumnOrName) → pyspark.sql.column.Column\n",
    "(array, index) - Returns element of array at given (1-based) index. If Index is 0, Spark will throw an error. If index < 0, accesses elements from the last to the first. The function always returns NULL if the index exceeds the length of the array.\n",
    "\n",
    "(map, key) - Returns value for given key. The function always returns NULL if the key is not contained in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7e4cfd81-6f69-41a0-aa9e-bf788d112c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='a')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
    "df.select(try_element_at(df.data, lit(1)).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dbf62872-c0f5-4c38-8fd5-2db2e8b8c9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='c')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(try_element_at(df.data, lit(-1)).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2aee94e3-ff81-4a20-ae0b-5c5af1ce7206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=1.0)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
    "df.select(try_element_at(df.data, lit(\"a\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba3f13-eeae-4ddd-bcbf-e1098cce4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce444c00-708e-4d72-a01a-4105f02061aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce686829-b063-4082-8470-1fbf9b067f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17b605-f469-42d3-b0d6-d6044547e7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0843b38-a634-424b-91ef-5bdceb19d9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e810582-e040-40dc-9c41-a2a56b23a6d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Partition Transformation Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#partition-transformation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bb2cf-633c-4ac2-958f-f0aa844be4da",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2.\n",
    "\n",
    "Get AnalysisException: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `catalog`.`db`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389b6af-43f6-4891-b876-441f993c6d51",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.years(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fba128cd-b329-43d1-b3dd-58fc7d34541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    years(\"ts\")\\n).createOrReplace()\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    years(\"ts\")\n",
    ").createOrReplace()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb0cd3-f469-4818-8be6-dd94057f9fc8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.months(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68af08dc-8b2a-4b91-9e7d-91c5a0be2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(\\n    months(\"ts\")\\n).createOrReplace()\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(\n",
    "    months(\"ts\")\n",
    ").createOrReplace()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e926fad-6cb4-4554-b4cb-b99b9779b3d8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.days(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps and dates to partition data into days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70a5212a-b7fb-4cfc-a021-6ffcfaba7ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    days(\"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    days(\"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95a306-1aba-4844-ab34-dc46f53256d7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hours(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for timestamps to partition data into hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24dfaec9-ccca-4f90-92eb-edd6fb7671e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.writeTo(\"catalog.db.table\").partitionedBy(   \\n    hours(\"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.writeTo(\"catalog.db.table\").partitionedBy(   \n",
    "    hours(\"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f06b9-90cb-44d8-8514-fdbb3fd8bde5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bucket(numBuckets: Union[pyspark.sql.column.Column, int], col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Partition transform function: A transform for any type that partitions by a hash of the input column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fcf66-c3fc-4e86-b97b-902c1608b507",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "    This function can be used only in combination with partitionedBy() method of the DataFrameWriterV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6959a902-e792-4a40-ac58-569b65972ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' df.writeTo(\"catalog.db.table\").partitionedBy(  \\n    bucket(42, \"ts\")\\n).createOrReplace()'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" df.writeTo(\"catalog.db.table\").partitionedBy(  \n",
    "    bucket(42, \"ts\")\n",
    ").createOrReplace()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c43e53a2-0a62-42bf-8a1f-d3ad905ff0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Створюємо DataFrame з числовими даними\n",
    "data = [(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,)]\n",
    "df = spark.createDataFrame(data, [\"value\"])\n",
    "num_buckets = 3\n",
    "# Використовуємо bucket() для розділення числових значень\n",
    "# bucketed_df = df.withColumn(\"bucket\", bucket(col=\"value\", numBuckets=3))\n",
    "# bucketed_df = df.withColumn(\"bucket\", expr(\"bucket(value, {})\".format(num_buckets)))\n",
    "# Виводимо результат\n",
    "bucketed_df.show()\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31236ce4-bf64-426f-8701-15c58f00a748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Aggregate Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782904d8-fd58-4d0e-9ce5-fca48f9fd054",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.any_value(col: ColumnOrName, ignoreNulls: Union[bool, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Returns some value of col for a group of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a74c4900-1378-4d5b-8f91-49adb92b327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+\n",
      "|any_value(Player name)|any_value(Value)|\n",
      "+----------------------+----------------+\n",
      "|       Robert Garrigus|              71|\n",
      "+----------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.select(any_value(\"Player name\"), any_value(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5622c514-d6a9-4a79-a5cc-c0336baf4e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any_value(c1)=None, any_value(c2)=1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.select(any_value('c1'), any_value('c2')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "082237b0-2f82-4fa5-ba26-e0c9dc94513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(any_value(c1)='a', any_value(c2)=1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(any_value('c1', True), any_value('c2', True)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2caf8f-d9f5-486c-8a60-738776e79d9b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.approxCountDistinct(col: ColumnOrName, rsd: Optional[float] = None) → pyspark.sql.column.Column\n",
    "##### Use approx_count_distinct() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5f75d-a617-423a-8ee9-4e0355dffdab",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.approx_count_distinct(col: ColumnOrName, rsd: Optional[float] = None) → pyspark.sql.column.Column\n",
    "Aggregate function: returns a new Column for approximate distinct count of column col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2684e784-b10a-4d67-857e-4dddb6acbd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/30 22:03:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 8:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|distinct_values|\n",
      "+---------------+\n",
      "|              3|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df.agg(approx_count_distinct(\"value\").alias('distinct_values')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8c62d-d4e5-4493-b707-297ba164f0f7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.approx_percentile(col: ColumnOrName, percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) → pyspark.sql.column.Column\n",
    "Returns the approximate percentile of the numeric column col which is the smallest value in the ordered col values (sorted from least to greatest) such that no more than percentage of col values is less than the value or equal to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07dbb05-5838-4c1a-b85c-95b4719162d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- approx_percentile(value, array(0.25, 0.5, 0.75), 1000000): array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = (col(\"id\") % 3).alias(\"key\")\n",
    "value = (randn(42) + key * 10).alias(\"value\")\n",
    "df = spark.range(0, 1000, 1, 1).select(key, value)\n",
    "df.select(\n",
    "    approx_percentile(\"value\", [0.25, 0.5, 0.75], 1000000)\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30da06a-d0ee-40a0-8b85-ae9cc7925626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: long (nullable = true)\n",
      " |-- approx_percentile(value, 0.5, 1000000): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"key\").agg(\n",
    "    approx_percentile(\"value\", 0.5, lit(1000000))\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c91fa-0033-4598-abf4-7ca3233b35a2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.array_agg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns a list of objects with duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab9f682-1212-45c0-adc6-d08345118421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[1, 1, 2])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.agg(array_agg('c').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926990e6-5387-4468-9aa0-55ecbabb30fc",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.avg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the average of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011165b3-4c3d-4601-a6bd-92a17d4ca8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    4.5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.select(avg(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0462ccf8-38b7-4d4c-88bd-69e833397538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|gender|          average|\n",
      "+------+-----------------+\n",
      "|female|79.16981132075472|\n",
      "|  male| 87.6842105263158|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.filter(col(\"reading score\") > 80).groupBy(\"gender\").agg(avg(col(\"math score\")).alias(\"average\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a3e5d-b792-408e-8fa5-2c4d83ac5c7a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_and(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the bitwise AND of all non-null input values, or null if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469a57ad-60b2-4b1f-ba78-3659f16fe526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(bit_and(c)=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(bit_and(\"c\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e4315-63f3-4835-ab3e-923667cd768a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_or(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Aggregate function: returns the bitwise OR of all non-null input values, or null if none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5c8f48-36ed-4769-b431-a634161a7dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bit_or(c)=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(bit_or(\"c\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb36f7e-1503-4f49-a11e-32eb88da85d4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_xor(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the bitwise XOR of all non-null input values, or null if none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9507b688-2934-42fd-8c4e-252a0a045ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bit_xor(c)=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(bit_xor(\"c\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd94c73-75f9-4ddf-86ac-2b8494f4213b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bool_and(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns true if all values of col are true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0463f9a4-7c16-47f3-a3a1-085ba6ded645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|bool_and(flag)|\n",
      "+--------------+\n",
      "|          true|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
    "df.select(bool_and(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037b56bd-6446-4cd1-bc16-eb9698f54964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|bool_and(flag)|\n",
      "+--------------+\n",
      "|         false|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
    "df.select(bool_and(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30532d05-f434-462f-b604-bfddc200f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|bool_and(flag)|\n",
      "+--------------+\n",
      "|         false|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
    "df.select(bool_and(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f83cfb-e5be-4a5e-a298-40f869cc1b2b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bool_or(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns true if at least one value of col is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d12cf27-ae35-4c62-8a27-a131fa227453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|bool_or(flag)|\n",
      "+-------------+\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
    "df.select(bool_or(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc08e04-ed38-4ada-81f1-efe2fe689b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|bool_or(flag)|\n",
      "+-------------+\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
    "df.select(bool_or(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5b04d0-1f53-49f5-bed5-dd47deea8b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|bool_or(flag)|\n",
      "+-------------+\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
    "df.select(bool_or(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59ca87-f45c-46f9-a362-630062b1df4c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.collect_list(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns a list of objects with duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d86e8-84d0-44dd-a351-6bf9ab4efc71",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "The function is non-deterministic because the order of collected results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017c7769-03ba-4cf6-adc2-dccc31621db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_list(age)=[2, 5, 5])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(collect_list('age')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b6f53-b347-45c4-90b6-75acca2763a3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.collect_set(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns a set of objects with duplicate elements eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b1d83-748a-4f66-8f5f-49578c6599bf",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "The function is non-deterministic because the order of collected results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb797a7-d433-4f0e-968c-af5df0d4bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=[2, 5])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(array_sort(collect_set('age')).alias('c')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767b147-1d33-4afb-b9ef-aeb6112e8541",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.corr(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new Column for the Pearson Correlation Coefficient for col1 and col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dd6f4eb-6bee-47e2-97db-049d19ce5353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=1.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(20)\n",
    "b = [2 * x for x in range(20)]\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(corr(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b0735-18f4-473a-a773-dfd2f1953b25",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.count(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Aggregate function: returns the number of items in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724298f3-0765-4b32-b80c-fcb09439c0fc",
   "metadata": {},
   "source": [
    "Count by all columns (start), and by a column that does not count None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07860a17-15ec-4c82-b557-bc373e555695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|count(1)|count(alphabets)|\n",
      "+--------+----------------+\n",
      "|       4|               3|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), (\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
    "df.select(count(expr(\"*\")), count(df.alphabets)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9213b45-86ee-4ec1-90fb-687cd35494a3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.count_distinct(col: ColumnOrName, *cols: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns a new Column for distinct count of col or cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a790604-cb9e-4264-a34f-351ec3e006a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|value|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    1|    2|\n",
      "|    1|    1|\n",
      "|    1|    2|\n",
      "|    3|    1|\n",
      "|    3|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([1, 1, 3], IntegerType())\n",
    "df2 = spark.createDataFrame([1, 2], IntegerType())\n",
    "df1.join(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7742a415-ca6b-49e8-a516-7f60e74baf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:================================================>    (132 + 12) / 144]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT value, value)|\n",
      "+----------------------------+\n",
      "|                           4|\n",
      "+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.join(df2).select(count_distinct(df1.value, df2.value)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8c007-8f55-4e7e-9627-b989945c96b7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.countDistinct(col: ColumnOrName, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new Column for distinct count of col or cols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cd21e-37df-4062-8272-7b77aa8d7923",
   "metadata": {},
   "source": [
    "An alias of count_distinct(), and it is encouraged to use count_distinct() directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eabec8-d703-43b7-b83a-4676b347343d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.count_min_sketch(col: ColumnOrName, eps: ColumnOrName, confidence: ColumnOrName, seed: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a count-min sketch of a column with the given esp, confidence and seed. The result is an array of bytes, which can be deserialized to a CountMinSketch before usage. Count-min sketch is a probabilistic data structure used for cardinality estimation using sub-linear space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102032f6-c38d-4795-b80a-cb76bde134e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='0000000100000000000000030000000100000004000000005D8D6AB90000000000000000000000000000000200000000000000010000000000000000')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1], [2], [1]], ['data'])\n",
    "df = df.agg(count_min_sketch(df.data, lit(0.5), lit(0.5), lit(1)).alias('sketch'))\n",
    "df.select(hex(df.sketch).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca7c6c-6a1a-499c-b1bd-c39e16967701",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.count_if(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of TRUE values for the col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2302a4c9-e16c-4871-b317-c9d8b2858088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count_if(((c2 % 2) = 0))|\n",
      "+------------------------+\n",
      "|                       3|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.select(count_if(col('c2') % 2 == 0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b23588-5965-4b4c-8d90-656fdcbb6728",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.covar_pop(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new Column for the population covariance of col1 and col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c26807f-44e8-4487-8bfd-48a0a867ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=0.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548f727-589d-41d1-93cf-10f49f333bdb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.covar_samp(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a new Column for the sample covariance of col1 and col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "515e7e56-c13a-48db-b8b8-37aef2ee45e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=0.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f448e85-5e34-4753-91ba-00f2c305416a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.every(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns true if all values of col are true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1bc0a2b-2e5c-45f6-a50c-b1e6cfa22263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|every(flag)|\n",
      "+-----------+\n",
      "|       true|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[True], [True], [True]], [\"flag\"]\n",
    ").select(every(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d6e4164-f220-48c9-af6b-3d78a196862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|every(flag)|\n",
      "+-----------+\n",
      "|      false|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[True], [False], [True]], [\"flag\"]\n",
    ").select(every(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c7a3df6-043e-43cb-9053-32ae79546274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|every(flag)|\n",
      "+-----------+\n",
      "|      false|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[False], [False], [False]], [\"flag\"]\n",
    ").select(every(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de8ed1-4116-43a2-9e65-124f83bd3299",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.first(col: ColumnOrName, ignorenulls: bool = False) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the first value in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b31ea-20d8-4b98-8814-27bd517cd2b2",
   "metadata": {},
   "source": [
    "The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "##### Notes:\n",
    "The function is non-deterministic because its results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2bedff8-7267-4804-b1a8-251dd40233f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|first(age)|\n",
      "+-----+----------+\n",
      "|Alice|      NULL|\n",
      "|  Bob|         5|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
    "df = df.orderBy(df.age)\n",
    "df.groupby(\"name\").agg(first(\"age\")).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7139a49-f0be-41fa-8986-0618574ab1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|first(age)|\n",
      "+-----+----------+\n",
      "|Alice|         2|\n",
      "|  Bob|         5|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"name\").agg(first(\"age\", ignorenulls=True)).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d392bb9-970e-4f46-9a27-4019c3a85a47",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.first_value(col: ColumnOrName, ignoreNulls: Union[bool, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Returns the first value of col for a group of rows. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e06d92-deae-48b5-ba45-a86131faec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|first_value(a)|first_value(b)|\n",
      "+--------------+--------------+\n",
      "|          NULL|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
    ").select(first_value('a'), first_value('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65793921-7262-44cb-b799-634047112184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|first_value(a)|first_value(b)|\n",
      "+--------------+--------------+\n",
      "|             a|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
    ").select(first_value('a', True), first_value('b', True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276e4b5-52b3-48e6-a57f-bb504f5f8114",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.grouping(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e540e8c-4381-4afa-8ca8-96385f94478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------+\n",
      "| name|grouping(name)|sum(age)|\n",
      "+-----+--------------+--------+\n",
      "| NULL|             1|       7|\n",
      "|Alice|             0|       2|\n",
      "|  Bob|             0|       5|\n",
      "+-----+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
    "df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b127f-e45b-4682-8770-20b1533d9eef",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.grouping_id(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the level of grouping, equals to\n",
    "\n",
    "    (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + … + grouping(cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472577eb-e48c-4111-8875-8ebf4747888e",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The list of columns should match with grouping columns exactly, or empty (means all the grouping columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "102151b7-683f-4987-b557-03e7e4d74c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------------+-------+\n",
      "|  c2|  c3|grouping_id()|sum(c1)|\n",
      "+----+----+-------------+-------+\n",
      "|NULL|NULL|            3|      8|\n",
      "|NULL|   a|            2|      4|\n",
      "|NULL|   c|            2|      4|\n",
      "|   a|NULL|            1|      4|\n",
      "|   a|   a|            0|      4|\n",
      "|   b|NULL|            1|      4|\n",
      "|   b|   c|            0|      4|\n",
      "+----+----+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\", \"a\"),\n",
    "                            (3, \"a\", \"a\"),\n",
    "                            (4, \"b\", \"c\")], [\"c1\", \"c2\", \"c3\"])\n",
    "df.cube(\"c2\", \"c3\").agg(grouping_id(), sum(\"c1\")).orderBy(\"c2\", \"c3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ddd66-6f61-49c2-bd73-43b718e59f1e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.histogram_numeric(col: ColumnOrName, nBins: ColumnOrName) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f9c92-f07d-4a80-aaf9-b31707fabb93",
   "metadata": {},
   "source": [
    "Computes a histogram on numeric ‘col’ using nb bins. The return value is an array of (x,y) pairs representing the centers of the histogram’s bins. As the value of ‘nb’ is increased, the histogram approximation gets finer-grained, but may yield artifacts around outliers. In practice, 20-40 histogram bins appear to work well, with more bins being required for skewed or smaller datasets. Note that this function creates a histogram with non-uniform bin widths. It offers no guarantees in terms of the mean-squared-error of the histogram, but in practice is comparable to the histograms produced by the R/S-Plus statistical computing packages. Note: the output type of the ‘x’ field in the return value is propagated from the input value consumed in the aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d546cad8-941c-4beb-b461-25edd89a0fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|histogram_numeric(c2, 5)|\n",
      "+------------------------+\n",
      "|    [{1, 1.0}, {2, 1....|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.select(histogram_numeric('c2', lit(5))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc033375-17c7-4b4a-a56d-6a5db6dcfdfb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_sketch_agg(col: ColumnOrName, lgConfigK: Union[int, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the updatable binary representation of the Datasketches HllSketch (HyperLogLog) configured with lgConfigK arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba146d01-7e5d-49c8-8f1b-fee177f463a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df1 = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d50e8285-e7bc-459c-9412-abfe5bb3f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.agg(hll_sketch_estimate(\n",
    "    hll_sketch_agg(\"value\", lit(12))\n",
    ").alias(\"distinct_cnt\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d8e0fe5-87fe-4fd7-8a78-38d4958047ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.agg(hll_sketch_estimate(\n",
    "    hll_sketch_agg(col(\"value\"), lit(12))).alias(\"distinct_cnt\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73a6fc-45d7-45f2-a32a-6546948f99ef",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_union_agg(col: ColumnOrName, allowDifferentLgConfigK: Union[bool, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e3fca-7508-4324-ad05-d105e3e6e35a",
   "metadata": {},
   "source": [
    "Aggregate function: returns the updatable binary representation of the Datasketches HllSketch, generated by merging previously created Datasketches HllSketch instances via a Datasketches Union instance. Throws an exception if sketches have different lgConfigK values and allowDifferentLgConfigK is unset or set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24fe8402-4bed-4129-8347-2774da960013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           6|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df1 = df1.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
    "df2 = spark.createDataFrame([4,5,5,6], \"INT\")\n",
    "df2 = df2.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
    "df3 = df1.union(df2).agg(hll_sketch_estimate(\n",
    "    hll_union_agg(\"sketch\")\n",
    ").alias(\"distinct_cnt\"))\n",
    "df3.drop(\"sketch\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e13bbeb1-84e2-49ba-adea-b13222f07710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           6|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df1.union(df2).agg(hll_sketch_estimate(\n",
    "    hll_union_agg(\"sketch\", lit(False))\n",
    ").alias(\"distinct_cnt\"))\n",
    "df4.drop(\"sketch\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3df7c19-1d24-4daf-a8bc-891d8ed42401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           6|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df1.union(df2).agg(hll_sketch_estimate(\n",
    "    hll_union_agg(col(\"sketch\"), lit(False))\n",
    ").alias(\"distinct_cnt\"))\n",
    "df5.drop(\"sketch\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b373638-a31b-49d1-add1-5df1d55f4b29",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.kurtosis(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the kurtosis of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14c1b4d1-8815-4665-b59c-6589a55f8570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|kurtosis(c)|\n",
      "+-----------+\n",
      "|       -1.5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(kurtosis(df.c)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d0eba-f94a-43dc-bc17-e0a48f88a119",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.last(col: ColumnOrName, ignorenulls: bool = False) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the last value in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2d3fb-f889-4685-95b4-d916efad1ef8",
   "metadata": {},
   "source": [
    "The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "##### Notes\n",
    "The function is non-deterministic because its results depends on the order of the rows which may be non-deterministic after a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06aa28be-cb0d-46c9-9105-e2ed1a36ca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|last(age)|\n",
      "+-----+---------+\n",
      "|Alice|     NULL|\n",
      "|  Bob|        5|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
    "df = df.orderBy(df.age.desc())\n",
    "df.groupby(\"name\").agg(last(\"age\")).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dd34215-6e5a-4e1d-9211-fc68d636db22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|last(age)|\n",
      "+-----+---------+\n",
      "|Alice|        2|\n",
      "|  Bob|        5|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"name\").agg(last(\"age\", ignorenulls=True)).orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bfe1e-2e99-484c-a101-8c3d66b1f1e5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.last_value(col: ColumnOrName, ignoreNulls: Union[bool, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Returns the last value of col for a group of rows. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18763a78-7dde-470f-948e-85b82a78827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|last_value(a)|last_value(b)|\n",
      "+-------------+-------------+\n",
      "|         NULL|            2|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
    ").select(last_value('a'), last_value('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03d727ed-3a93-4f26-aba1-4566906694c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|last_value(a)|last_value(b)|\n",
      "+-------------+-------------+\n",
      "|            b|            2|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
    ").select(last_value('a', True), last_value('b', True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cd4ab-8e9d-429b-a449-671677d863a1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.max(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the maximum value of the expression in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdeef9ca-b952-4214-880d-742333b489be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|max(id)|\n",
      "+-------+\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.select(max(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2aa6c-14d1-4e87-8aa2-7b821cd5e11e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.max_by(col: ColumnOrName, ord: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the value associated with the maximum value of ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e88a15dc-00d8-4059-91ba-60a3fa4eb666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|course|max_by(year, earnings)|\n",
      "+------+----------------------+\n",
      "|  Java|                  2013|\n",
      "|dotNET|                  2013|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
    "    (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
    "    schema=(\"course\", \"year\", \"earnings\"))\n",
    "df.groupby(\"course\").agg(max_by(\"year\", \"earnings\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65775f20-40bb-498d-817f-29560fba2635",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.mean(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the average of the values in a group. An alias of avg()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b8c6839-dbb2-4d04-8869-d8a9bc0caa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    4.5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.select(mean(df.id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ad7b1-8ef0-47bb-bf68-998e1ef366f9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.median(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the median of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a01efad0-2973-437e-afe0-e3c941644134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|course|median(earnings)|\n",
      "+------+----------------+\n",
      "|  Java|         22000.0|\n",
      "|dotNET|         10000.0|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
    "    (\"Java\", 2012, 22000), (\"dotNET\", 2012, 10000),\n",
    "    (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
    "    schema=(\"course\", \"year\", \"earnings\"))\n",
    "df.groupby(\"course\").agg(median(\"earnings\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b8eb2-e116-4763-a763-71a3d201f243",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.min(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Aggregate function: returns the minimum value of the expression in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b56f32f-dc5a-42b6-bf12-3f4e24cbc130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|min(id)|\n",
      "+-------+\n",
      "|      0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.select(min(df.id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f656acb-faf7-4d84-bed7-d0ef45e2c484",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.min_by(col: ColumnOrName, ord: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns the value associated with the minimum value of ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a1d3c92-21f0-45f1-81c3-0ae2c99cdef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|course|min_by(year, earnings)|\n",
      "+------+----------------------+\n",
      "|  Java|                  2012|\n",
      "|dotNET|                  2012|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
    "    (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
    "    schema=(\"course\", \"year\", \"earnings\"))\n",
    "df.groupby(\"course\").agg(min_by(\"year\", \"earnings\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a19c4-391b-4435-afe4-ee7fb379f706",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.mode(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the most frequent value in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad784edd-0c40-4fa1-9116-0db820c9030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|course|mode(year)|\n",
      "+------+----------+\n",
      "|  Java|      2012|\n",
      "|dotNET|      2012|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
    "    (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
    "    (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
    "    schema=(\"course\", \"year\", \"earnings\"))\n",
    "df.groupby(\"course\").agg(mode(\"year\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136cc4c-7972-4b19-80b6-3a3a9329c39e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.percentile(col: ColumnOrName, percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], frequency: Union[pyspark.sql.column.Column, int] = 1) → pyspark.sql.column.Column\n",
    "Returns the exact percentile(s) of numeric column expr at the given percentage(s) with value range in [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "378ab1fa-889b-4b82-b616-9aca5b70844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           quantiles|\n",
      "+--------------------+\n",
      "|[0.74419914941216...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = (col(\"id\") % 3).alias(\"key\")\n",
    "value = (randn(42) + key * 10).alias(\"value\")\n",
    "df = spark.range(0, 1000, 1, 1).select(key, value)\n",
    "df.select(\n",
    "    percentile(\"value\", [0.25, 0.5, 0.75], lit(1)).alias(\"quantiles\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ec638cd-81d0-46a4-805c-0b1c147f0aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|              median|\n",
      "+---+--------------------+\n",
      "|  0|-0.03449962216667901|\n",
      "|  1|   9.990389751837329|\n",
      "|  2|  19.967859769284075|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"key\").agg(\n",
    "    percentile(\"value\", 0.5, lit(1)).alias(\"median\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5e1cd-28bc-4216-b405-a0a6aa606ad4",
   "metadata": {},
   "source": [
    "#### yspark.sql.functions.percentile_approx(col: ColumnOrName, percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945ce1c-2e14-444d-837c-41b2dd978d6c",
   "metadata": {},
   "source": [
    "Returns the approximate percentile of the numeric column col which is the smallest value in the ordered col values (sorted from least to greatest) such that no more than percentage of col values is less than the value or equal to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dcc083c9-54f9-448f-ae76-604705e4c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- quantiles: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = (col(\"id\") % 3).alias(\"key\")\n",
    "value = (randn(42) + key * 10).alias(\"value\")\n",
    "df = spark.range(0, 1000, 1, 1).select(key, value)\n",
    "df.select(\n",
    "    percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e73e1f4-b818-4028-bcf4-9d74dfaef9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: long (nullable = true)\n",
      " |-- median: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"key\").agg(\n",
    "    percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc41d4c-7a99-4550-8775-f4a76aecf96d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.product(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the product of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daf90a44-b571-4603-a588-c6e2385c9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|mod3|product|\n",
      "+----+-------+\n",
      "|   0|  162.0|\n",
      "|   1|   28.0|\n",
      "|   2|   80.0|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1, 10).toDF('x').withColumn('mod3', col('x') % 3)\n",
    "prods = df.groupBy('mod3').agg(product('x').alias('product'))\n",
    "prods.orderBy('mod3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b6d8a-8bd1-4725-a0ef-60fbdd2d73e3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.reduce(col: ColumnOrName, initialValue: ColumnOrName, merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) → pyspark.sql.column.Column\n",
    "\n",
    "Applies a binary operator to an initial state and all elements in the array, and reduces this to a single state. The final state is converted into the final result by applying a finish function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216bb14-7860-422b-94b8-0af9eef7f261",
   "metadata": {},
   "source": [
    "Both functions can use methods of Column, functions defined in pyspark.sql.functions and Scala UserDefinedFunctions. Python UserDefinedFunctions are not supported (SPARK-27052)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed893e20-ad04-4e62-9164-88756b209ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| sum|\n",
      "+----+\n",
      "|42.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
    "df.select(reduce(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1907876-82ef-440c-a3c4-36e005f03595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 8.4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge(acc, x):\n",
    "    count = acc.count + 1\n",
    "    sum = acc.sum + x\n",
    "    return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
    "\n",
    "df.select(\n",
    "    reduce(\n",
    "        \"values\",\n",
    "        struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
    "        merge,\n",
    "        lambda acc: acc.sum / acc.count,\n",
    "    ).alias(\"mean\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229a883-7403-4e74-b952-5ef319d58d49",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_avgx(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the average of the independent variable for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8894d871-8efb-4278-85c9-393839adb4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_avgx(y, x)=0.999)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_avgx(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac44c8-7097-4768-9734-283ead54f670",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_avgy(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the average of the dependent variable for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d326204-807e-48df-9f6a-5f5d6f0e3675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_avgy(y, x)=9.980732994136464)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_avgy(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67aae6-95ba-421c-b01d-713e79f1b503",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_count(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the number of non-null number pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c82fc139-3fa4-47c8-b0e9-d7989758db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_count(y, x)=1000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_count(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28818004-f5d0-446d-876a-ce6eeb0b80a0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_intercept(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the intercept of the univariate linear regression line for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2a39a828-0497-449c-8c52-713e367c0d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_intercept(y, x)=-0.04961745990969568)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_intercept(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc71e2-2627-4e24-92e3-d6ddaee77537",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_r2(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the coefficient of determination for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "578a4123-1f6d-4a73-9c9e-d0017eabe7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_r2(y, x)=0.9851908293645436)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_r2(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b7d1e-5e22-4f6b-982f-c83934f02c3d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_slope(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the slope of the linear regression line for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f13142d-f446-4182-9dc2-e0cdfc649c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_slope(y, x)=10.040390844891048)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_slope(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947314d4-c456-4039-a396-ac4f16de5221",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_sxx(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a932f159-cfbb-4bb7-9efb-19ff5d36a349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_sxx(y, x)=666.9989999999996)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_sxx(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc609cc-1726-40a1-aa2b-f82c6779e211",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_sxy(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ca439df-e979-42d7-b592-f4bbc61fc5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_sxy(y, x)=6696.93065315148)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_sxy(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761d68c-1008-483d-b045-60954c4151b5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regr_syy(y: ColumnOrName, x: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group, where y is the dependent variable and x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c3b5a41-a2ce-4099-b1c8-87e05e53334a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regr_syy(y, x)=68250.53503811295)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (col(\"id\") % 3).alias(\"x\")\n",
    "y = (randn(42) + x * 10).alias(\"y\")\n",
    "df = spark.range(0, 1000, 1, 1).select(x, y)\n",
    "df.select(regr_syy(\"y\", \"x\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92dd7a2-cf7b-417c-b56f-71139d7804e0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.skewness(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the skewness of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c51f98-e75d-4855-9bca-0e393be7f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(skewness(c)=0.7071067811865475)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(skewness(df.c)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7998eb8a-afac-47ff-abf4-0fe7f2e9df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-----------------------+\n",
      "|skewness(math score)|skewness(reading score)|skewness(writing score)|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "|-0.27851657191407453|   -0.25871569927829374|   -0.28900962452114143|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(skewness(\"math score\"), skewness(\"reading score\"), skewness(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176efd1f-7002-45dc-a8a8-d561de4fe663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(skewness(gender)=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(skewness(\"gender\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a18ab-ad7a-43a9-8b42-0a909cd7e4cc",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.some(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns true if at least one value of col is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cef95fd-db06-4b91-ae99-bf2c0a5b2f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|some(flag)|\n",
      "+----------+\n",
      "|      true|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[True], [True], [True]], [\"flag\"]\n",
    ").select(some(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8265fc-bc7f-456e-a028-925c888581d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|some(flag)|\n",
      "+----------+\n",
      "|      true|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[True], [False], [True]], [\"flag\"]\n",
    ").select(some(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd59cf48-2510-461f-a6b1-c34077e29cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|some(flag)|\n",
      "+----------+\n",
      "|     false|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [[False], [False], [False]], [\"flag\"]\n",
    ").select(some(\"flag\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6bf24-7a0b-404f-8f0a-34e53f15b455",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.std(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: alias for stddev_samp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b916a5c-2b7d-4afb-acaa-dc2db4c41ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           std(id)|\n",
      "+------------------+\n",
      "|1.8708286933869707|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(6).select(std(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79a8f0-8d40-4a96-b39e-93dda08d9a53",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.stddev(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: alias for stddev_samp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c965838-fa05-4980-a8ea-9fbaa5a6cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        stddev(id)|\n",
      "+------------------+\n",
      "|1.8708286933869707|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(6).select(stddev(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f0a6aa-5f0d-403c-b173-4aa4eee4d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+---------------------+\n",
      "|stddev(math score)|stddev(reading score)|stddev(writing score)|\n",
      "+------------------+---------------------+---------------------+\n",
      "|15.163080096009454|   14.600191937252223|    15.19565701086966|\n",
      "+------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(stddev(\"math score\"), stddev(\"reading score\"), stddev(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72469fb-bf98-4785-9516-8f15091a6670",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.stddev_pop(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns population standard deviation of the expression in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d34e733-a5e0-449b-9d4b-71875ef68b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   stddev_pop(id)|\n",
      "+-----------------+\n",
      "|1.707825127659933|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(6).select(stddev_pop(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c03af96-8363-4561-b037-c2d43cdb9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------------+-------------------------+\n",
      "|stddev_pop(math score)|stddev_pop(reading score)|stddev_pop(writing score)|\n",
      "+----------------------+-------------------------+-------------------------+\n",
      "|     15.15549665962815|       14.592890015346523|       15.188057281956775|\n",
      "+----------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(stddev_pop(\"math score\"), stddev_pop(\"reading score\"), stddev_pop(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f6bbe-8538-4c7b-91fb-a10791fd6409",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.stddev_samp(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the unbiased sample standard deviation of the expression in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c39c3a3-05a8-47a6-b276-1caecc4a9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   stddev_samp(id)|\n",
      "+------------------+\n",
      "|1.8708286933869707|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(6).select(stddev_samp(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60938950-e4dd-4d69-8845-653bad2629a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------+--------------------------+\n",
      "|stddev_samp(math score)|stddev_samp(reading score)|stddev_samp(writing score)|\n",
      "+-----------------------+--------------------------+--------------------------+\n",
      "|     15.163080096009454|        14.600191937252223|         15.19565701086966|\n",
      "+-----------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(stddev_samp(\"math score\"), stddev_samp(\"reading score\"), stddev_samp(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ad587-8cd5-4dc5-9228-0f280967c4d9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sum(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the sum of all values in the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e3b547-7575-40ef-b12e-b2aa43c818c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|sum(id)|\n",
      "+-------+\n",
      "|     45|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.select(sum(df[\"id\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82941205-2b89-452a-9613-112c20b1db65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(math score)=66089)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(sum(\"math score\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372ab5a-fe64-469b-ac7b-2e22bba65e5d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sum_distinct(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the sum of distinct values in the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377156c5-8c9a-4a82-8dfe-ded437e01e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|sum(DISTINCT numbers)|\n",
      "+---------------------+\n",
      "|                    3|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), (1,), (1,), (2,)], schema=[\"numbers\"])\n",
    "df.select(sum_distinct(col(\"numbers\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409391aa-8877-4526-bdaf-53ce27aaac7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(DISTINCT math score)=4808)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(sum_distinct(\"math score\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd6b0b-8f6d-450a-a12b-af99f5509431",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sumDistinct(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the sum of distinct values in the expression.\n",
    "\n",
    "    Deprecated since version 3.2.0: Use sum_distinct() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f9346-c512-459e-8db5-9804ecb875b2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.var_pop(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the population variance of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df789151-9213-4c1b-a1d4-148e840063f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(var_pop(id)=2.9166666666666665)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(6)\n",
    "df.select(var_pop(df.id)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "993b4c1e-82f0-480c-a165-b4cc197634cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+----------------------+\n",
      "|var_pop(math score)|var_pop(reading score)|var_pop(writing score)|\n",
      "+-------------------+----------------------+----------------------+\n",
      "| 229.68907900000005|    212.95243900000023|    230.67708400000024|\n",
      "+-------------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(var_pop(\"math score\"), var_pop(\"reading score\"), var_pop(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ce107-4cf5-4ae4-a9af-84317b101b37",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.var_samp(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Aggregate function: returns the unbiased sample variance of the values in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f2ea16-443a-4718-be10-bc4be786b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|var_samp(id)|\n",
      "+------------+\n",
      "|         3.5|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(6)\n",
    "df.select(var_samp(df.id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abbce469-7024-41b6-a8bc-7cf4c4db8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-----------------------+\n",
      "|var_samp(math score)|var_samp(reading score)|var_samp(writing score)|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "|  229.91899799799805|     213.16560460460482|     230.90799199199222|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(var_samp(\"math score\"), var_samp(\"reading score\"), var_samp(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854303d2-7c81-4ed6-8e0c-d7955f058c83",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.variance(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Aggregate function: alias for var_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b53d9e2c-5c84-4d38-b9d0-d9ed1b8e0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|var_samp(id)|\n",
      "+------------+\n",
      "|         3.5|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(6)\n",
    "df.select(variance(df.id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa41acc4-67d8-406c-b8fe-72f41f84a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-----------------------+\n",
      "|var_samp(math score)|var_samp(reading score)|var_samp(writing score)|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "|  229.91899799799805|     213.16560460460482|     230.90799199199222|\n",
      "+--------------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.select(variance(\"math score\"), variance(\"reading score\"), variance(\"writing score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8be0c8-051c-4dd0-b8c1-4cf4acd9eff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Window Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#window-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b0a83-3f2d-4e9a-9f0a-9a60447ea817",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.cume_dist() → pyspark.sql.column.Column\n",
    "Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "40472995-65ac-4977-985a-608fd41d74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| cd|\n",
      "+-----+---+\n",
      "|    1|0.2|\n",
      "|    2|0.4|\n",
      "|    3|0.8|\n",
      "|    3|0.8|\n",
      "|    4|1.0|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:48:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 2, 3, 3, 4], IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"cd\", cume_dist().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e927603d-e288-4920-8ffe-c967c9fde412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|  cd|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|0.02|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|0.06|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|0.06|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|0.08|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|0.16|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|0.16|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|0.16|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|0.16|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|0.18|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92| 0.2|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|0.24|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|0.24|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|0.26|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|0.34|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|0.34|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|0.34|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|0.34|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|0.46|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|0.46|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|0.46|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|0.46|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|0.46|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|0.46|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|0.48|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93| 0.5|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|0.56|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|0.56|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|0.56|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|0.64|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|0.64|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|0.64|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|0.64|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|0.68|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|0.68|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100| 0.7|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|0.74|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|0.74|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100| 0.8|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96| 0.8|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91| 0.8|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|0.82|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|0.86|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|0.86|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93| 1.0|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100| 1.0|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100| 1.0|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100| 1.0|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99| 1.0|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86| 1.0|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97| 1.0|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:50:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"math score\")\n",
    "students.orderBy(desc(\"reading score\")).limit(50).withColumn(\"cd\", cume_dist().over(w)).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7740c8d-8442-4632-ab14-c78aa966af92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.dense_rank() → pyspark.sql.column.Column\n",
    "Window function: returns the rank of rows within a window partition, without any gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0878be-7169-4e88-acb7-652e4b9119af",
   "metadata": {},
   "source": [
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "This is equivalent to the DENSE_RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "25c2bdbd-caa8-43c7-a35a-dd584f6ccb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:51:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|drank|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    1|    1|\n",
      "|    2|    2|\n",
      "|    3|    3|\n",
      "|    3|    3|\n",
      "|    4|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"drank\", dense_rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a45d403f-ea49-403a-8801-369879b09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|drank|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|    1|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|    2|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|    3|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|    4|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|    4|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|    5|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92|    6|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|    7|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|    8|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|    9|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|    9|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|    9|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|   10|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|   10|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|   10|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|   10|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|   11|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|   12|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|   13|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|   13|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|   13|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|   14|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|   14|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|   14|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|   14|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|   15|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|   15|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100|   16|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|   17|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|   17|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100|   18|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96|   18|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91|   18|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|   19|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|   20|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|   20|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93|   21|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100|   21|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100|   21|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100|   21|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99|   21|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86|   21|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97|   21|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 20:52:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"math score\")\n",
    "students.orderBy(desc(\"reading score\")).limit(50).withColumn(\"drank\", dense_rank().over(w)).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766e255-6385-40a4-88e7-04f2d3e2c29b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.lag(col: ColumnOrName, offset: int = 1, default: Optional[Any] = None) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is offset rows before the current row, and default if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition.\n",
    "\n",
    "This is equivalent to the LAG function in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8eff8-5b01-4a4d-8e0b-997539d34039",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int, optional default 1 | \n",
    "number of row to extend\n",
    "- default: optional | \n",
    "default value\n",
    "\n",
    "Returns – Column | \n",
    "value before current row based on offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a98ed371-1bf7-4dc7-9a28-7bca28d1a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|         NULL|\n",
      "|  a|  2|            1|\n",
      "|  a|  3|            2|\n",
      "|  b|  2|         NULL|\n",
      "|  b|  8|            2|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()\n",
    "\n",
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"previos_value\", lag(\"c2\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "922dfaab-9393-47ba-9a0a-14121c4e3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|            0|\n",
      "|  a|  2|            1|\n",
      "|  a|  3|            2|\n",
      "|  b|  2|            0|\n",
      "|  b|  8|            2|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"previos_value\", lag(\"c2\", 1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a090f3aa-8f9c-452d-8cb3-c5a29ae059e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+\n",
      "| c1| c2|previos_value|\n",
      "+---+---+-------------+\n",
      "|  a|  1|           -1|\n",
      "|  a|  2|           -1|\n",
      "|  a|  3|            1|\n",
      "|  b|  2|           -1|\n",
      "|  b|  8|           -1|\n",
      "+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"previos_value\", lag(\"c2\", 2, -1).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aabec6-9774-467d-94a4-2983732252dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.lead(col: ColumnOrName, offset: int = 1, default: Optional[Any] = None) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is offset rows after the current row, and default if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "\n",
    "This is equivalent to the LEAD function in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876026a-5319-4011-aad4-7e0c4d4ac059",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int, optional default 1 | \n",
    "number of row to extend\n",
    "- default: optional | \n",
    "default value\n",
    "\n",
    "Returns – Column | \n",
    "value after current row based on offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4b0cf86c-3ab7-42db-908f-07fba0ac322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "24e4fa77-7af8-4ee5-be96-a758c7f193e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         2|\n",
      "|  a|  2|         3|\n",
      "|  a|  3|      NULL|\n",
      "|  b|  2|         8|\n",
      "|  b|  8|      NULL|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"next_value\", lead(\"c2\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "999bd3ae-5145-421c-a3fa-d3f7cd884d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         2|\n",
      "|  a|  2|         3|\n",
      "|  a|  3|         0|\n",
      "|  b|  2|         8|\n",
      "|  b|  8|         0|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_value\", lead(\"c2\", 1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b22223c9-bf1e-49c2-a0eb-ee337cf3f8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| c1| c2|next_value|\n",
      "+---+---+----------+\n",
      "|  a|  1|         3|\n",
      "|  a|  2|        -1|\n",
      "|  a|  3|        -1|\n",
      "|  b|  2|        -1|\n",
      "|  b|  8|        -1|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_value\", lead(\"c2\", 2, -1).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fdb31-6d59-4430-9062-c4dba8b43d1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.nth_value(col: ColumnOrName, offset: int, ignoreNulls: Optional[bool] = False) → pyspark.sql.column.Column\n",
    "Window function: returns the value that is the offsetth row of the window frame (counting from 1), and null if the size of window frame is less than offset rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a313e6c-faf8-4d07-a2c4-548520d9e8bf",
   "metadata": {},
   "source": [
    "It will return the offsetth non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "This is equivalent to the nth_value function in SQL.\n",
    "\n",
    "Parameters:\n",
    "- col: Column or str | \n",
    "name of column or expression\n",
    "- offset: int | \n",
    "number of row to use as the value\n",
    "- ignoreNulls: bool, optional | \n",
    "indicates the Nth value should skip null in the determination of which row to use\n",
    "\n",
    "Returns – Column | \n",
    "value of nth row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a4cdda8d-1f64-45a5-bd80-5d625d456c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fd6f4d69-33d3-4339-b7b1-1716c2365fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| c1| c2|nth_value|\n",
      "+---+---+---------+\n",
      "|  a|  1|        1|\n",
      "|  a|  2|        1|\n",
      "|  a|  3|        1|\n",
      "|  b|  2|        2|\n",
      "|  b|  8|        2|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"nth_value\", nth_value(\"c2\", 1).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5449c855-5b08-45fa-9f35-d1c7cd4a2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "| c1| c2|nth_value|\n",
      "+---+---+---------+\n",
      "|  a|  1|     NULL|\n",
      "|  a|  2|        2|\n",
      "|  a|  3|        2|\n",
      "|  b|  2|     NULL|\n",
      "|  b|  8|        8|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"nth_value\", nth_value(\"c2\", 2).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cb38f-df80-4951-86be-f59ce0131964",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.ntile(n: int) → pyspark.sql.column.Column\n",
    "Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4.\n",
    "\n",
    "This is equivalent to the NTILE function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a8635865-3fa3-4302-ba7a-23509bdf10a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  a|  3|\n",
      "|  b|  8|\n",
      "|  b|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1),\n",
    "                            (\"a\", 2),\n",
    "                            (\"a\", 3),\n",
    "                            (\"b\", 8),\n",
    "                            (\"b\", 2)], [\"c1\", \"c2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "59924f16-d1b0-47dc-ab8a-0fd2772dbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| c1| c2|ntile|\n",
      "+---+---+-----+\n",
      "|  a|  1|    1|\n",
      "|  a|  2|    1|\n",
      "|  a|  3|    2|\n",
      "|  b|  2|    1|\n",
      "|  b|  8|    2|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
    "df.withColumn(\"ntile\", ntile(2).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "337a9752-5853-4217-a50c-9a071b48f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|ntile|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        26|           31|           38|    1|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        37|           57|           56|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        39|           64|           57|    1|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        40|           52|           43|    1|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        40|           48|           50|    1|\n",
      "|  male|       group A|         associate's degree|free/reduced|              completed|        40|           55|           53|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        40|           59|           51|    1|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        41|           51|           48|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        42|           61|           58|    1|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        43|           45|           50|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        43|           60|           58|    1|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        44|           41|           38|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        46|           43|           42|    1|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        46|           58|           57|    1|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        46|           43|           41|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        46|           56|           57|    1|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        46|           61|           55|    1|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        46|           43|           44|    1|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        47|           57|           44|    1|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        47|           49|           50|    1|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        47|           53|           58|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        47|           37|           35|    1|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        48|           43|           45|    1|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        49|           51|           43|    2|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        49|           52|           54|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        49|           53|           53|    2|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        49|           51|           51|    2|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        50|           56|           54|    2|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        51|           60|           58|    2|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        51|           51|           54|    2|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        52|           55|           49|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        52|           76|           70|    2|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        52|           66|           73|    2|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        52|           59|           56|    2|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        52|           57|           50|    2|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        52|           59|           62|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        52|           55|           57|    2|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        53|           58|           65|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        53|           71|           67|    2|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        53|           61|           62|    2|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        53|           54|           48|    2|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        53|           45|           40|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        53|           70|           70|    2|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        53|           62|           53|    2|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        54|           58|           61|    2|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        54|           53|           47|    2|\n",
      "|female|       group B|         associate's degree|free/reduced|                   none|        54|           65|           65|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        54|           61|           58|    3|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        55|           61|           54|    3|\n",
      "|female|       group A|         associate's degree|    standard|              completed|        55|           65|           62|    3|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        55|           72|           79|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        55|           76|           76|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|                   none|        56|           65|           63|    3|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        56|           68|           70|    3|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        57|           56|           57|    3|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        57|           74|           76|    3|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        57|           69|           68|    3|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        57|           78|           67|    3|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        57|           54|           56|    3|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        57|           78|           79|    3|\n",
      "|female|       group E|         associate's degree|free/reduced|              completed|        57|           68|           73|    3|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        57|           77|           80|    3|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        58|           54|           52|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        58|           73|           68|    3|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        58|           63|           65|    3|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        58|           55|           53|    3|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        58|           57|           53|    3|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        59|           66|           67|    3|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        59|           70|           66|    4|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        59|           70|           65|    4|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        59|           73|           72|    4|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        59|           62|           69|    4|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        60|           51|           56|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        60|           75|           74|    4|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        61|           58|           56|    4|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        61|           86|           87|    4|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        61|           55|           52|    4|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        61|           48|           46|    4|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        61|           42|           41|    4|\n",
      "|  male|       group D|         associate's degree|free/reduced|              completed|        61|           71|           73|    4|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        62|           61|           55|    4|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        62|           56|           53|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        62|           74|           70|    4|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        62|           61|           58|    4|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        62|           76|           80|    4|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        62|           65|           58|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        63|           67|           70|    4|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        63|           61|           61|    4|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        64|           73|           68|    4|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        64|           66|           59|    4|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        64|           64|           70|    5|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        64|           76|           74|    5|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        64|           56|           52|    5|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        65|           54|           57|    5|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        65|           69|           70|    5|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        65|           67|           65|    5|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        65|           65|           63|    5|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        65|           77|           74|    5|\n",
      "|female|       group A|         associate's degree|free/reduced|                   none|        65|           85|           76|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        65|           76|           76|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        65|           77|           74|    5|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        65|           73|           68|    5|\n",
      "|female|       group A|         associate's degree|    standard|              completed|        65|           70|           74|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        65|           84|           84|    5|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        65|           75|           77|    5|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        66|           63|           64|    5|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        66|           62|           64|    5|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        66|           65|           69|    5|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        66|           77|           73|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        67|           84|           86|    5|\n",
      "|  male|       group B|         associate's degree|free/reduced|                   none|        67|           62|           60|    5|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        67|           84|           81|    5|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        67|           72|           67|    6|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        67|           54|           63|    6|\n",
      "|  male|       group A|         associate's degree|    standard|                   none|        67|           57|           53|    6|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        68|           67|           69|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        68|           86|           84|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        68|           67|           73|    6|\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        68|           77|           80|    6|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        68|           65|           61|    6|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        68|           76|           67|    6|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        69|           80|           71|    6|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        69|           77|           69|    6|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        69|           70|           63|    6|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        70|           84|           81|    6|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        71|           83|           78|    6|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        71|           71|           74|    6|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        71|           77|           77|    6|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        71|           74|           68|    6|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        71|           66|           60|    6|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        72|           64|           63|    6|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        72|           79|           74|    6|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        72|           57|           62|    6|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        73|           68|           66|    6|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        73|           76|           80|    7|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        73|           83|           76|    7|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        73|           75|           80|    7|\n",
      "|female|       group E|         associate's degree|free/reduced|                   none|        73|           76|           78|    7|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        73|           78|           72|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        74|           81|           83|    7|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        74|           75|           83|    7|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        74|           73|           67|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        74|           88|           90|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        75|           90|           88|    7|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        75|           68|           64|    7|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        75|           66|           73|    7|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        75|           82|           90|    7|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        76|           70|           68|    7|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        76|           71|           67|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        76|           74|           73|    7|\n",
      "|female|       group B|         associate's degree|free/reduced|              completed|        76|           94|           87|    7|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        77|           69|           68|    7|\n",
      "|female|       group D|         associate's degree|free/reduced|              completed|        77|           89|           98|    7|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        77|           67|           64|    7|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        77|           78|           73|    7|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        77|           77|           73|    7|\n",
      "|  male|       group C|         associate's degree|free/reduced|              completed|        78|           81|           82|    8|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        78|           77|           77|    8|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        78|           74|           72|    8|\n",
      "|  male|       group A|         associate's degree|free/reduced|              completed|        79|           82|           82|    8|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        79|           88|           94|    8|\n",
      "|  male|       group D|         associate's degree|free/reduced|              completed|        79|           82|           80|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           75|           77|    8|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        80|           76|           64|    8|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        80|           86|           83|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           68|           72|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           63|           63|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        80|           75|           69|    8|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        81|           81|           79|    8|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        81|           82|           82|    8|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        81|           73|           72|    8|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        81|           77|           79|    8|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        81|           72|           77|    8|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        81|           75|           78|    8|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        81|           71|           73|    8|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        82|           85|           86|    8|\n",
      "|  male|       group B|         associate's degree|free/reduced|              completed|        82|           78|           74|    8|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        82|           75|           77|    8|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        82|           80|           77|    9|\n",
      "|female|       group C|         associate's degree|free/reduced|              completed|        82|           93|           93|    9|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        82|           95|           89|    9|\n",
      "|female|       group A|         associate's degree|    standard|                   none|        82|           93|           93|    9|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        82|           84|           78|    9|\n",
      "|female|       group E|         associate's degree|free/reduced|              completed|        83|           86|           88|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        83|           72|           78|    9|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        83|           85|           90|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        84|           80|           80|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        84|           95|           92|    9|\n",
      "|female|       group D|         associate's degree|    standard|                   none|        85|           91|           89|    9|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        85|           76|           71|    9|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        85|           89|           95|    9|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        85|           84|           82|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        85|           92|           85|    9|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        87|           85|           73|    9|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        87|           74|           76|    9|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        87|          100|           95|    9|\n",
      "|  male|       group D|         associate's degree|    standard|              completed|        87|           84|           85|    9|\n",
      "|female|       group E|         associate's degree|    standard|                   none|        87|           94|           95|    9|\n",
      "|  male|       group C|         associate's degree|free/reduced|                   none|        87|           73|           72|    9|\n",
      "|female|       group D|         associate's degree|    standard|              completed|        88|           92|           95|    9|\n",
      "|  male|       group E|         associate's degree|    standard|                   none|        89|           76|           74|   10|\n",
      "|  male|       group D|         associate's degree|free/reduced|                   none|        90|           87|           75|   10|\n",
      "|  male|       group B|         associate's degree|    standard|                   none|        90|           78|           81|   10|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        90|           90|           91|   10|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        90|           87|           85|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|                   none|        90|           90|           82|   10|\n",
      "|  male|       group B|         associate's degree|    standard|              completed|        91|           89|           92|   10|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           86|           84|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|        91|           73|           80|   10|\n",
      "|female|       group C|         associate's degree|    standard|                   none|        91|           95|           94|   10|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        92|           79|           84|   10|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        93|          100|           95|   10|\n",
      "|female|       group B|         associate's degree|    standard|              completed|        94|           87|           92|   10|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        94|           85|           82|   10|\n",
      "|female|       group E|         associate's degree|    standard|              completed|        95|           89|           92|   10|\n",
      "|female|       group C|         associate's degree|    standard|              completed|        96|           96|           99|   10|\n",
      "|  male|       group E|         associate's degree|    standard|              completed|        97|           82|           88|   10|\n",
      "|  male|       group A|         associate's degree|    standard|              completed|        97|           92|           86|   10|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        97|           93|           91|   10|\n",
      "|  male|       group C|         associate's degree|    standard|              completed|        98|           87|           90|   10|\n",
      "|  male|       group E|         associate's degree|free/reduced|              completed|       100|          100|           93|   10|\n",
      "|female|       group E|         associate's degree|    standard|                   none|       100|          100|          100|   10|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        29|           41|           47|    1|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        37|           56|           47|    1|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        37|           45|           38|    1|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        39|           42|           38|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        43|           51|           54|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        43|           62|           61|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        44|           63|           62|    1|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        45|           59|           64|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        47|           62|           66|    1|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        48|           51|           46|    1|\n",
      "|  male|       group A|          bachelor's degree|free/reduced|              completed|        49|           58|           60|    1|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        50|           60|           59|    1|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        50|           42|           48|    2|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        51|           49|           51|    2|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        51|           72|           79|    2|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        52|           65|           69|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        52|           61|           66|    2|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        53|           58|           55|    2|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        54|           49|           47|    2|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        55|           59|           54|    2|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        55|           46|           44|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        56|           79|           72|    2|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        58|           55|           48|    2|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        59|           64|           75|    2|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        59|           70|           73|    3|\n",
      "|female|       group A|          bachelor's degree|    standard|                   none|        59|           72|           70|    3|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        59|           54|           51|    3|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        61|           72|           70|    3|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|                   none|        61|           66|           61|    3|\n",
      "|female|       group E|          bachelor's degree|free/reduced|                   none|        61|           58|           62|    3|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        61|           70|           76|    3|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        62|           63|           56|    3|\n",
      "|  male|       group A|          bachelor's degree|free/reduced|                   none|        62|           72|           65|    3|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        62|           78|           79|    3|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        62|           72|           74|    3|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        63|           66|           67|    3|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        63|           75|           81|    4|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        63|           71|           69|    4|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        63|           73|           78|    4|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        63|           64|           66|    4|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        64|           73|           70|    4|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        64|           60|           58|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        65|           72|           74|    4|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        65|           67|           62|    4|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        65|           73|           75|    4|\n",
      "|female|       group B|          bachelor's degree|    standard|              completed|        65|           81|           81|    4|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        65|           79|           81|    4|\n",
      "|  male|       group B|          bachelor's degree|    standard|                   none|        66|           60|           57|    4|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        66|           64|           62|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        66|           83|           83|    5|\n",
      "|female|       group B|          bachelor's degree|    standard|              completed|        66|           74|           81|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        66|           74|           81|    5|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        67|           69|           75|    5|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        67|           86|           83|    5|\n",
      "|  male|       group D|          bachelor's degree|    standard|              completed|        67|           61|           68|    5|\n",
      "|female|       group C|          bachelor's degree|free/reduced|                   none|        67|           75|           72|    5|\n",
      "|  male|       group D|          bachelor's degree|    standard|              completed|        68|           74|           74|    5|\n",
      "|female|       group D|          bachelor's degree|    standard|              completed|        68|           75|           81|    5|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|                   none|        68|           68|           67|    5|\n",
      "|  male|       group E|          bachelor's degree|    standard|                   none|        68|           68|           64|    5|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        69|           63|           61|    6|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        69|           58|           57|    6|\n",
      "|  male|       group E|          bachelor's degree|free/reduced|              completed|        70|           68|           72|    6|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        70|           64|           70|    6|\n",
      "|  male|       group C|          bachelor's degree|free/reduced|              completed|        70|           75|           74|    6|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        71|           74|           68|    6|\n",
      "|female|       group D|          bachelor's degree|    standard|              completed|        71|           76|           83|    6|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        71|           70|           70|    6|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        72|           72|           74|    6|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        73|           56|           57|    6|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        73|           79|           84|    6|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        74|           71|           80|    6|\n",
      "|  male|       group D|          bachelor's degree|free/reduced|              completed|        74|           79|           75|    7|\n",
      "|female|       group C|          bachelor's degree|free/reduced|              completed|        74|           86|           89|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        75|           85|           82|    7|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        75|           58|           62|    7|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        75|           73|           74|    7|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        75|           84|           80|    7|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        76|           62|           66|    7|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        77|           67|           68|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        77|           88|           87|    7|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        77|           94|           95|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        77|           85|           87|    7|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        78|           82|           79|    7|\n",
      "|female|       group B|          bachelor's degree|free/reduced|                   none|        78|           79|           76|    8|\n",
      "|female|       group D|          bachelor's degree|free/reduced|                   none|        78|           90|           93|    8|\n",
      "|  male|       group E|          bachelor's degree|free/reduced|              completed|        79|           74|           72|    8|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        79|           92|           89|    8|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        79|           89|           89|    8|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        79|           81|           82|    8|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        80|           78|           81|    8|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|        80|           83|           83|    8|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        80|           73|           72|    8|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        81|           88|           90|    8|\n",
      "|  male|       group E|          bachelor's degree|    standard|                   none|        82|           62|           62|    8|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        83|           82|           84|    8|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        83|           78|           73|    9|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        83|           93|           95|    9|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|        85|           66|           71|    9|\n",
      "|  male|       group C|          bachelor's degree|    standard|                   none|        86|           83|           86|    9|\n",
      "|female|       group C|          bachelor's degree|    standard|                   none|        86|           92|           87|    9|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|              completed|        87|           90|           88|    9|\n",
      "|  male|       group A|          bachelor's degree|    standard|              completed|        87|           84|           87|    9|\n",
      "|  male|       group B|          bachelor's degree|free/reduced|                   none|        88|           75|           76|    9|\n",
      "|  male|       group D|          bachelor's degree|    standard|                   none|        88|           78|           83|    9|\n",
      "|female|       group D|          bachelor's degree|    standard|                   none|        89|          100|          100|    9|\n",
      "|  male|       group A|          bachelor's degree|    standard|                   none|        91|           96|           92|    9|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        91|           81|           79|   10|\n",
      "|female|       group E|          bachelor's degree|free/reduced|              completed|        92|          100|          100|   10|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        92|          100|           99|   10|\n",
      "|female|       group D|          bachelor's degree|free/reduced|              completed|        93|          100|          100|   10|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        94|           90|           91|   10|\n",
      "|female|       group C|          bachelor's degree|    standard|              completed|        96|          100|          100|   10|\n",
      "|  male|       group C|          bachelor's degree|    standard|              completed|        96|           90|           92|   10|\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        97|           97|           96|   10|\n",
      "|female|       group E|          bachelor's degree|    standard|              completed|        99|          100|          100|   10|\n",
      "|female|       group E|          bachelor's degree|    standard|                   none|       100|          100|          100|   10|\n",
      "|  male|       group E|          bachelor's degree|    standard|              completed|       100|          100|          100|   10|\n",
      "|female|       group B|                high school|free/reduced|                   none|         8|           24|           23|    1|\n",
      "|female|       group B|                high school|free/reduced|              completed|        23|           44|           36|    1|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        27|           34|           36|    1|\n",
      "|female|       group C|                high school|    standard|                   none|        29|           29|           30|    1|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        30|           24|           15|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        33|           41|           43|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        34|           42|           39|    1|\n",
      "|female|       group A|                high school|free/reduced|              completed|        34|           48|           41|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        35|           61|           54|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        35|           53|           46|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        36|           53|           43|    1|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        36|           29|           27|    1|\n",
      "|female|       group B|                high school|free/reduced|                   none|        38|           60|           50|    1|\n",
      "|female|       group D|                high school|free/reduced|                   none|        39|           52|           46|    1|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        40|           46|           50|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        41|           46|           43|    1|\n",
      "|  male|       group D|                high school|    standard|                   none|        41|           52|           51|    1|\n",
      "|female|       group E|                high school|free/reduced|                   none|        41|           45|           40|    1|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        42|           39|           34|    1|\n",
      "|female|       group C|                high school|free/reduced|                   none|        42|           62|           60|    1|\n",
      "|female|       group B|                high school|    standard|                   none|        42|           52|           51|    2|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        44|           51|           48|    2|\n",
      "|female|       group C|                high school|    standard|                   none|        44|           61|           52|    2|\n",
      "|  male|       group D|                high school|    standard|                   none|        45|           48|           46|    2|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        45|           47|           49|    2|\n",
      "|female|       group D|                high school|    standard|                   none|        45|           63|           59|    2|\n",
      "|female|       group B|                high school|free/reduced|              completed|        46|           54|           58|    2|\n",
      "|  male|       group D|                high school|    standard|                   none|        46|           34|           36|    2|\n",
      "|  male|       group B|                high school|    standard|                   none|        47|           46|           42|    2|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        48|           45|           41|    2|\n",
      "|female|       group B|                high school|    standard|                   none|        48|           62|           60|    2|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        49|           45|           45|    2|\n",
      "|female|       group D|                high school|free/reduced|                   none|        49|           57|           52|    2|\n",
      "|female|       group E|                high school|    standard|                   none|        50|           50|           47|    2|\n",
      "|female|       group B|                high school|free/reduced|                   none|        50|           67|           63|    2|\n",
      "|female|       group C|                high school|free/reduced|              completed|        50|           66|           64|    2|\n",
      "|female|       group B|                high school|    standard|                   none|        50|           53|           55|    2|\n",
      "|  male|       group C|                high school|    standard|                   none|        50|           48|           42|    2|\n",
      "|female|       group D|                high school|    standard|                   none|        51|           66|           62|    2|\n",
      "|  male|       group C|                high school|    standard|                   none|        52|           53|           49|    2|\n",
      "|female|       group D|                high school|free/reduced|              completed|        52|           57|           56|    3|\n",
      "|  male|       group B|                high school|    standard|                   none|        52|           48|           49|    3|\n",
      "|  male|       group B|                high school|    standard|              completed|        52|           49|           46|    3|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        53|           51|           51|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        53|           52|           42|    3|\n",
      "|  male|       group C|                high school|    standard|              completed|        53|           52|           49|    3|\n",
      "|  male|       group A|                high school|free/reduced|                   none|        53|           58|           44|    3|\n",
      "|female|       group C|                high school|free/reduced|                   none|        53|           72|           64|    3|\n",
      "|female|       group A|                high school|free/reduced|              completed|        53|           50|           60|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        54|           52|           52|    3|\n",
      "|female|       group C|                high school|    standard|                   none|        54|           59|           62|    3|\n",
      "|female|       group B|                high school|    standard|                   none|        54|           64|           68|    3|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        54|           72|           59|    3|\n",
      "|  male|       group E|                high school|free/reduced|                   none|        55|           56|           51|    3|\n",
      "|female|       group A|                high school|    standard|                   none|        55|           73|           73|    3|\n",
      "|  male|       group D|                high school|    standard|              completed|        55|           41|           48|    3|\n",
      "|female|       group D|                high school|    standard|              completed|        56|           68|           74|    3|\n",
      "|female|       group D|                high school|    standard|                   none|        56|           52|           55|    3|\n",
      "|  male|       group A|                high school|    standard|                   none|        57|           43|           47|    3|\n",
      "|  male|       group D|                high school|    standard|                   none|        57|           50|           54|    3|\n",
      "|  male|       group E|                high school|free/reduced|              completed|        57|           56|           54|    4|\n",
      "|female|       group D|                high school|    standard|              completed|        57|           58|           64|    4|\n",
      "|  male|       group B|                high school|    standard|                   none|        57|           48|           51|    4|\n",
      "|female|       group E|                high school|free/reduced|                   none|        57|           58|           57|    4|\n",
      "|female|       group E|                high school|free/reduced|              completed|        57|           75|           73|    4|\n",
      "|  male|       group A|                high school|    standard|                   none|        57|           51|           54|    4|\n",
      "|female|       group B|                high school|    standard|              completed|        58|           70|           68|    4|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        58|           61|           52|    4|\n",
      "|female|       group B|                high school|    standard|                   none|        58|           62|           59|    4|\n",
      "|  male|       group C|                high school|free/reduced|              completed|        58|           51|           52|    4|\n",
      "|female|       group C|                high school|    standard|              completed|        58|           75|           77|    4|\n",
      "|female|       group B|                high school|    standard|                   none|        58|           68|           61|    4|\n",
      "|  male|       group C|                high school|    standard|              completed|        58|           52|           54|    4|\n",
      "|female|       group E|                high school|    standard|              completed|        59|           63|           75|    4|\n",
      "|  male|       group A|                high school|    standard|                   none|        59|           52|           46|    4|\n",
      "|female|       group C|                high school|    standard|                   none|        59|           72|           68|    4|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        59|           53|           52|    4|\n",
      "|  male|       group B|                high school|    standard|                   none|        59|           58|           47|    4|\n",
      "|female|       group C|                high school|free/reduced|              completed|        59|           71|           65|    4|\n",
      "|  male|       group B|                high school|    standard|              completed|        60|           44|           47|    4|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        60|           57|           51|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        60|           68|           72|    5|\n",
      "|  male|       group B|                high school|    standard|                   none|        60|           68|           60|    5|\n",
      "|female|       group B|                high school|free/reduced|                   none|        60|           72|           68|    5|\n",
      "|female|       group C|                high school|    standard|              completed|        60|           64|           74|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        61|           73|           63|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        61|           72|           70|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        61|           56|           55|    5|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        61|           60|           55|    5|\n",
      "|female|       group A|                high school|    standard|                   none|        61|           68|           63|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        62|           55|           49|    5|\n",
      "|female|       group D|                high school|    standard|                   none|        62|           64|           64|    5|\n",
      "|female|       group C|                high school|free/reduced|                   none|        62|           67|           64|    5|\n",
      "|  male|       group B|                high school|    standard|                   none|        62|           55|           54|    5|\n",
      "|female|       group B|                high school|    standard|                   none|        62|           62|           63|    5|\n",
      "|  male|       group C|                high school|    standard|                   none|        62|           67|           58|    5|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        62|           55|           55|    5|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        63|           57|           56|    5|\n",
      "|female|       group C|                high school|    standard|                   none|        63|           69|           74|    5|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        63|           48|           47|    5|\n",
      "|  male|       group A|                high school|    standard|                   none|        63|           63|           62|    6|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        64|           64|           67|    6|\n",
      "|female|       group E|                high school|free/reduced|                   none|        64|           62|           68|    6|\n",
      "|  male|       group D|                high school|    standard|                   none|        64|           54|           50|    6|\n",
      "|female|       group B|                high school|free/reduced|                   none|        64|           73|           71|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        65|           81|           73|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        65|           64|           62|    6|\n",
      "|female|       group C|                high school|    standard|                   none|        65|           69|           67|    6|\n",
      "|female|       group D|                high school|free/reduced|              completed|        65|           61|           71|    6|\n",
      "|  male|       group D|                high school|    standard|                   none|        66|           69|           63|    6|\n",
      "|female|       group C|                high school|    standard|                   none|        66|           71|           76|    6|\n",
      "|female|       group E|                high school|free/reduced|              completed|        66|           74|           78|    6|\n",
      "|  male|       group B|                high school|free/reduced|                   none|        66|           77|           70|    6|\n",
      "|female|       group C|                high school|free/reduced|                   none|        66|           76|           68|    6|\n",
      "|  male|       group C|                high school|free/reduced|                   none|        66|           66|           59|    6|\n",
      "|female|       group B|                high school|    standard|                   none|        66|           72|           70|    6|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        66|           74|           69|    6|\n",
      "|female|       group C|                high school|free/reduced|              completed|        67|           79|           84|    6|\n",
      "|female|       group B|                high school|free/reduced|              completed|        67|           78|           79|    6|\n",
      "|female|       group B|                high school|free/reduced|              completed|        67|           80|           81|    6|\n",
      "|female|       group D|                high school|    standard|                   none|        67|           72|           74|    7|\n",
      "|female|       group B|                high school|    standard|              completed|        68|           83|           78|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        68|           60|           53|    7|\n",
      "|  male|       group D|                high school|    standard|              completed|        68|           64|           66|    7|\n",
      "|  male|       group A|                high school|    standard|                   none|        68|           70|           66|    7|\n",
      "|female|       group A|                high school|    standard|              completed|        68|           80|           76|    7|\n",
      "|female|       group D|                high school|    standard|                   none|        69|           72|           77|    7|\n",
      "|female|       group B|                high school|    standard|              completed|        69|           76|           74|    7|\n",
      "|  male|       group C|                high school|    standard|              completed|        69|           58|           53|    7|\n",
      "|female|       group D|                high school|    standard|                   none|        69|           77|           73|    7|\n",
      "|female|       group D|                high school|    standard|              completed|        69|           77|           78|    7|\n",
      "|  male|       group D|                high school|    standard|                   none|        69|           75|           71|    7|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        69|           70|           67|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           70|           65|    7|\n",
      "|  male|       group E|                high school|    standard|                   none|        70|           55|           56|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           56|           51|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        70|           74|           71|    7|\n",
      "|  male|       group B|                high school|    standard|                   none|        70|           65|           60|    7|\n",
      "|  male|       group D|                high school|    standard|                   none|        70|           70|           70|    7|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           79|           71|    8|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           66|           65|    8|\n",
      "|  male|       group C|                high school|    standard|                   none|        71|           60|           61|    8|\n",
      "|  male|       group A|                high school|    standard|                   none|        71|           74|           64|    8|\n",
      "|female|       group B|                high school|free/reduced|                   none|        71|           87|           82|    8|\n",
      "|  male|       group A|                high school|    standard|              completed|        72|           73|           74|    8|\n",
      "|female|       group C|                high school|    standard|                   none|        72|           80|           75|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        72|           65|           68|    8|\n",
      "|  male|       group A|                high school|free/reduced|              completed|        72|           67|           65|    8|\n",
      "|  male|       group C|                high school|    standard|              completed|        72|           67|           64|    8|\n",
      "|female|       group C|                high school|    standard|                   none|        72|           80|           83|    8|\n",
      "|  male|       group D|                high school|    standard|                   none|        72|           66|           66|    8|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        73|           68|           66|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        73|           71|           68|    8|\n",
      "|  male|       group B|                high school|    standard|              completed|        73|           69|           68|    8|\n",
      "|female|       group D|                high school|free/reduced|                   none|        73|           92|           84|    8|\n",
      "|  male|       group E|                high school|    standard|                   none|        73|           64|           57|    8|\n",
      "|female|       group E|                high school|    standard|                   none|        74|           81|           71|    8|\n",
      "|female|       group E|                high school|    standard|                   none|        74|           76|           73|    8|\n",
      "|female|       group E|                high school|    standard|              completed|        74|           79|           80|    9|\n",
      "|female|       group B|                high school|    standard|                   none|        74|           72|           72|    9|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        74|           70|           69|    9|\n",
      "|female|       group E|                high school|    standard|                   none|        75|           86|           79|    9|\n",
      "|  male|       group D|                high school|free/reduced|                   none|        75|           74|           66|    9|\n",
      "|  male|       group C|                high school|    standard|              completed|        75|           69|           68|    9|\n",
      "|female|       group C|                high school|    standard|                   none|        75|           88|           85|    9|\n",
      "|female|       group A|                high school|    standard|              completed|        75|           82|           79|    9|\n",
      "|  male|       group C|                high school|    standard|                   none|        75|           81|           71|    9|\n",
      "|  male|       group D|                high school|    standard|                   none|        76|           73|           68|    9|\n",
      "|female|       group C|                high school|    standard|                   none|        76|           76|           74|    9|\n",
      "|  male|       group B|                high school|    standard|              completed|        76|           62|           60|    9|\n",
      "|female|       group B|                high school|free/reduced|              completed|        76|           85|           82|    9|\n",
      "|female|       group A|                high school|free/reduced|              completed|        77|           88|           85|    9|\n",
      "|female|       group B|                high school|    standard|              completed|        77|           82|           89|    9|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        78|           77|           80|    9|\n",
      "|female|       group D|                high school|    standard|                   none|        78|           81|           80|    9|\n",
      "|  male|       group B|                high school|    standard|                   none|        79|           60|           65|    9|\n",
      "|  male|       group E|                high school|    standard|                   none|        80|           76|           65|    9|\n",
      "|  male|       group E|                high school|    standard|              completed|        81|           80|           76|   10|\n",
      "|female|       group B|                high school|    standard|                   none|        81|           91|           89|   10|\n",
      "|female|       group C|                high school|    standard|                   none|        81|           84|           82|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        81|           66|           64|   10|\n",
      "|  male|       group C|                high school|    standard|              completed|        82|           84|           82|   10|\n",
      "|  male|       group B|                high school|    standard|                   none|        82|           82|           80|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        84|           77|           74|   10|\n",
      "|  male|       group E|                high school|    standard|                   none|        84|           73|           69|   10|\n",
      "|  male|       group C|                high school|    standard|              completed|        86|           81|           80|   10|\n",
      "|  male|       group E|                high school|free/reduced|              completed|        86|           81|           75|   10|\n",
      "|female|       group B|                high school|    standard|                   none|        87|           95|           86|   10|\n",
      "|  male|       group E|                high school|    standard|              completed|        87|           91|           81|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        88|           89|           86|   10|\n",
      "|  male|       group D|                high school|    standard|                   none|        88|           78|           75|   10|\n",
      "|female|       group D|                high school|    standard|              completed|        88|           99|          100|   10|\n",
      "|  male|       group D|                high school|    standard|                   none|        89|           87|           79|   10|\n",
      "|  male|       group C|                high school|    standard|                   none|        90|           75|           69|   10|\n",
      "|  male|       group E|                high school|    standard|                   none|        94|           73|           71|   10|\n",
      "|female|       group E|                high school|    standard|                   none|        99|           93|           90|   10|\n",
      "|female|       group D|            master's degree|free/reduced|                   none|        40|           59|           54|    1|\n",
      "|female|       group C|            master's degree|free/reduced|                   none|        40|           58|           54|    1|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        45|           56|           54|    1|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        46|           42|           46|    1|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        47|           58|           67|    1|\n",
      "|  male|       group B|            master's degree|free/reduced|                   none|        49|           53|           52|    1|\n",
      "|female|       group A|            master's degree|    standard|                   none|        50|           53|           58|    2|\n",
      "|female|       group A|            master's degree|free/reduced|                   none|        50|           67|           73|    2|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        52|           70|           62|    2|\n",
      "|female|       group C|            master's degree|free/reduced|                   none|        52|           65|           61|    2|\n",
      "|female|       group D|            master's degree|    standard|                   none|        53|           61|           68|    2|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        54|           59|           50|    2|\n",
      "|female|       group D|            master's degree|    standard|                   none|        54|           60|           63|    3|\n",
      "|female|       group C|            master's degree|    standard|              completed|        54|           64|           67|    3|\n",
      "|female|       group D|            master's degree|    standard|                   none|        55|           64|           70|    3|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        56|           72|           65|    3|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        58|           76|           78|    3|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        61|           67|           66|    3|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        61|           71|           78|    4|\n",
      "|female|       group D|            master's degree|    standard|                   none|        62|           70|           75|    4|\n",
      "|female|       group E|            master's degree|    standard|                   none|        62|           68|           68|    4|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        62|           68|           75|    4|\n",
      "|female|       group D|            master's degree|    standard|                   none|        64|           63|           66|    4|\n",
      "|female|       group C|            master's degree|free/reduced|              completed|        65|           81|           81|    4|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        67|           57|           59|    5|\n",
      "|female|       group C|            master's degree|    standard|              completed|        69|           84|           85|    5|\n",
      "|female|       group D|            master's degree|    standard|              completed|        70|           71|           74|    5|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        71|           67|           67|    5|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        72|           66|           72|    5|\n",
      "|  male|       group A|            master's degree|free/reduced|                   none|        73|           74|           72|    5|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        73|           70|           75|    6|\n",
      "|female|       group C|            master's degree|    standard|                   none|        73|           78|           74|    6|\n",
      "|female|       group D|            master's degree|    standard|                   none|        74|           79|           82|    6|\n",
      "|female|       group D|            master's degree|    standard|              completed|        77|           82|           91|    6|\n",
      "|female|       group B|            master's degree|free/reduced|              completed|        77|           97|           94|    6|\n",
      "|female|       group B|            master's degree|    standard|                   none|        77|           90|           84|    6|\n",
      "|female|       group D|            master's degree|    standard|                   none|        78|           91|           96|    7|\n",
      "|  male|       group C|            master's degree|free/reduced|                   none|        79|           81|           71|    7|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        79|           78|           77|    7|\n",
      "|  male|       group C|            master's degree|    standard|                   none|        79|           72|           69|    7|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        79|           77|           75|    7|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        80|           80|           72|    7|\n",
      "|female|       group E|            master's degree|    standard|                   none|        81|           92|           91|    8|\n",
      "|female|       group C|            master's degree|    standard|              completed|        81|           91|           87|    8|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        81|           81|           84|    8|\n",
      "|female|       group E|            master's degree|free/reduced|                   none|        81|           86|           87|    8|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        82|           82|           74|    8|\n",
      "|  male|       group D|            master's degree|free/reduced|              completed|        84|           89|           90|    8|\n",
      "|female|       group D|            master's degree|free/reduced|              completed|        85|           95|          100|    9|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        85|           84|           89|    9|\n",
      "|female|       group D|            master's degree|    standard|                   none|        87|          100|          100|    9|\n",
      "|female|       group E|            master's degree|    standard|              completed|        88|           99|           95|    9|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        89|           84|           82|    9|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|    9|\n",
      "|  male|       group E|            master's degree|    standard|                   none|        90|           85|           84|   10|\n",
      "|  male|       group C|            master's degree|    standard|              completed|        91|           85|           85|   10|\n",
      "|female|       group D|            master's degree|    standard|                   none|        92|          100|          100|   10|\n",
      "|female|       group E|            master's degree|    standard|              completed|        94|           99|          100|   10|\n",
      "|  male|       group D|            master's degree|    standard|                   none|        95|           81|           84|   10|\n",
      "|female|       group B|               some college|    standard|                   none|        19|           38|           32|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        22|           39|           33|    1|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        28|           23|           19|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        32|           39|           33|    1|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        35|           28|           27|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        35|           44|           43|    1|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        40|           43|           39|    1|\n",
      "|  male|       group D|               some college|    standard|                   none|        40|           42|           38|    1|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        41|           39|           34|    1|\n",
      "|female|       group E|               some college|free/reduced|              completed|        42|           55|           54|    1|\n",
      "|female|       group C|               some college|free/reduced|              completed|        42|           66|           69|    1|\n",
      "|  male|       group D|               some college|    standard|                   none|        44|           54|           53|    1|\n",
      "|female|       group B|               some college|free/reduced|                   none|        45|           53|           55|    1|\n",
      "|female|       group C|               some college|free/reduced|              completed|        45|           73|           70|    1|\n",
      "|female|       group C|               some college|free/reduced|                   none|        46|           64|           66|    1|\n",
      "|  male|       group B|               some college|    standard|                   none|        47|           43|           41|    1|\n",
      "|female|       group B|               some college|free/reduced|              completed|        48|           56|           58|    1|\n",
      "|female|       group A|               some college|free/reduced|                   none|        49|           65|           55|    1|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        49|           57|           46|    1|\n",
      "|  male|       group E|               some college|free/reduced|              completed|        49|           52|           51|    1|\n",
      "|female|       group D|               some college|free/reduced|                   none|        49|           58|           60|    1|\n",
      "|female|       group D|               some college|free/reduced|                   none|        49|           65|           61|    1|\n",
      "|  male|       group A|               some college|free/reduced|              completed|        50|           47|           54|    1|\n",
      "|  male|       group C|               some college|free/reduced|              completed|        50|           48|           53|    2|\n",
      "|female|       group B|               some college|    standard|              completed|        50|           64|           66|    2|\n",
      "|female|       group D|               some college|    standard|                   none|        51|           58|           54|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        52|           58|           58|    2|\n",
      "|female|       group D|               some college|free/reduced|              completed|        52|           59|           65|    2|\n",
      "|  male|       group E|               some college|    standard|                   none|        53|           55|           48|    2|\n",
      "|  male|       group C|               some college|    standard|                   none|        53|           44|           42|    2|\n",
      "|  male|       group A|               some college|    standard|                   none|        53|           43|           43|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        53|           62|           56|    2|\n",
      "|  male|       group C|               some college|    standard|                   none|        53|           39|           37|    2|\n",
      "|female|       group E|               some college|free/reduced|                   none|        53|           58|           57|    2|\n",
      "|female|       group B|               some college|free/reduced|              completed|        53|           66|           73|    2|\n",
      "|  male|       group B|               some college|    standard|                   none|        54|           52|           51|    2|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        54|           54|           45|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        54|           48|           52|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        54|           64|           65|    2|\n",
      "|female|       group A|               some college|    standard|                   none|        54|           63|           67|    2|\n",
      "|female|       group C|               some college|    standard|                   none|        55|           69|           65|    2|\n",
      "|female|       group D|               some college|free/reduced|                   none|        55|           71|           69|    2|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        55|           55|           47|    2|\n",
      "|  male|       group D|               some college|    standard|                   none|        55|           58|           52|    2|\n",
      "|female|       group A|               some college|    standard|                   none|        56|           58|           64|    2|\n",
      "|  male|       group D|               some college|    standard|              completed|        58|           59|           58|    2|\n",
      "|female|       group D|               some college|free/reduced|              completed|        58|           63|           73|    3|\n",
      "|female|       group A|               some college|    standard|                   none|        58|           70|           67|    3|\n",
      "|female|       group D|               some college|free/reduced|                   none|        58|           67|           62|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        58|           67|           72|    3|\n",
      "|female|       group B|               some college|free/reduced|                   none|        58|           61|           66|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        58|           49|           42|    3|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        58|           57|           54|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        58|           59|           66|    3|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        58|           60|           57|    3|\n",
      "|  male|       group B|               some college|    standard|                   none|        58|           50|           45|    3|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        59|           65|           66|    3|\n",
      "|female|       group C|               some college|free/reduced|                   none|        59|           62|           64|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        59|           41|           42|    3|\n",
      "|  male|       group E|               some college|    standard|                   none|        59|           51|           43|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        59|           71|           70|    3|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        59|           62|           61|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        59|           60|           58|    3|\n",
      "|female|       group D|               some college|free/reduced|              completed|        59|           78|           76|    3|\n",
      "|female|       group C|               some college|    standard|                   none|        60|           72|           74|    3|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        60|           60|           60|    3|\n",
      "|  male|       group D|               some college|    standard|                   none|        60|           63|           59|    3|\n",
      "|female|       group D|               some college|free/reduced|                   none|        60|           66|           70|    3|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        60|           62|           60|    3|\n",
      "|  male|       group C|               some college|    standard|                   none|        61|           61|           62|    4|\n",
      "|female|       group B|               some college|free/reduced|                   none|        61|           68|           66|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        61|           47|           56|    4|\n",
      "|  male|       group A|               some college|    standard|              completed|        61|           51|           52|    4|\n",
      "|female|       group E|               some college|    standard|                   none|        61|           64|           62|    4|\n",
      "|female|       group A|               some college|free/reduced|                   none|        61|           60|           57|    4|\n",
      "|female|       group E|               some college|    standard|                   none|        62|           73|           70|    4|\n",
      "|  male|       group B|               some college|    standard|                   none|        62|           61|           57|    4|\n",
      "|female|       group C|               some college|free/reduced|                   none|        62|           67|           62|    4|\n",
      "|female|       group B|               some college|    standard|                   none|        62|           67|           67|    4|\n",
      "|  male|       group B|               some college|    standard|              completed|        62|           66|           68|    4|\n",
      "|female|       group C|               some college|free/reduced|                   none|        62|           72|           70|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        62|           57|           62|    4|\n",
      "|female|       group C|               some college|    standard|                   none|        62|           69|           69|    4|\n",
      "|female|       group D|               some college|    standard|                   none|        62|           70|           72|    4|\n",
      "|female|       group B|               some college|    standard|                   none|        63|           65|           61|    4|\n",
      "|  male|       group D|               some college|    standard|              completed|        63|           55|           63|    4|\n",
      "|female|       group E|               some college|    standard|              completed|        63|           72|           70|    4|\n",
      "|female|       group C|               some college|    standard|              completed|        63|           78|           80|    4|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        63|           61|           54|    4|\n",
      "|female|       group C|               some college|    standard|                   none|        63|           74|           74|    4|\n",
      "|female|       group D|               some college|free/reduced|              completed|        63|           80|           80|    4|\n",
      "|female|       group C|               some college|free/reduced|              completed|        63|           73|           71|    4|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        63|           61|           60|    5|\n",
      "|female|       group D|               some college|    standard|                   none|        63|           64|           67|    5|\n",
      "|  male|       group C|               some college|    standard|                   none|        63|           63|           60|    5|\n",
      "|female|       group C|               some college|free/reduced|              completed|        64|           85|           85|    5|\n",
      "|female|       group D|               some college|free/reduced|                   none|        64|           74|           75|    5|\n",
      "|female|       group C|               some college|    standard|              completed|        64|           82|           77|    5|\n",
      "|female|       group B|               some college|free/reduced|              completed|        65|           75|           70|    5|\n",
      "|  male|       group D|               some college|    standard|              completed|        65|           77|           74|    5|\n",
      "|female|       group D|               some college|free/reduced|                   none|        65|           81|           77|    5|\n",
      "|female|       group D|               some college|    standard|                   none|        65|           70|           71|    5|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        65|           58|           49|    5|\n",
      "|  male|       group E|               some college|    standard|                   none|        66|           57|           52|    5|\n",
      "|female|       group E|               some college|    standard|              completed|        66|           74|           73|    5|\n",
      "|  male|       group C|               some college|    standard|                   none|        66|           59|           52|    5|\n",
      "|  male|       group B|               some college|    standard|                   none|        66|           65|           60|    5|\n",
      "|female|       group C|               some college|free/reduced|              completed|        67|           75|           70|    5|\n",
      "|female|       group E|               some college|    standard|                   none|        67|           76|           75|    5|\n",
      "|female|       group C|               some college|    standard|              completed|        67|           81|           79|    5|\n",
      "|  male|       group D|               some college|    standard|                   none|        67|           64|           70|    5|\n",
      "|  male|       group C|               some college|free/reduced|              completed|        67|           74|           70|    5|\n",
      "|female|       group D|               some college|free/reduced|              completed|        67|           86|           83|    5|\n",
      "|  male|       group D|               some college|    standard|                   none|        68|           59|           62|    5|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        68|           68|           61|    5|\n",
      "|  male|       group E|               some college|    standard|                   none|        68|           60|           59|    6|\n",
      "|  male|       group E|               some college|    standard|                   none|        68|           72|           65|    6|\n",
      "|female|       group E|               some college|    standard|                   none|        68|           70|           66|    6|\n",
      "|female|       group D|               some college|    standard|              completed|        68|           78|           77|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        69|           90|           88|    6|\n",
      "|  male|       group B|               some college|    standard|                   none|        69|           54|           55|    6|\n",
      "|female|       group D|               some college|    standard|                   none|        69|           74|           74|    6|\n",
      "|  male|       group A|               some college|    standard|                   none|        69|           67|           69|    6|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        69|           66|           60|    6|\n",
      "|female|       group D|               some college|    standard|                   none|        69|           77|           77|    6|\n",
      "|  male|       group D|               some college|free/reduced|              completed|        69|           60|           63|    6|\n",
      "|female|       group D|               some college|    standard|              completed|        69|           79|           81|    6|\n",
      "|female|       group C|               some college|    standard|                   none|        69|           78|           76|    6|\n",
      "|  male|       group C|               some college|    standard|                   none|        69|           64|           68|    6|\n",
      "|  male|       group B|               some college|    standard|              completed|        69|           77|           77|    6|\n",
      "|  male|       group E|               some college|    standard|                   none|        69|           60|           54|    6|\n",
      "|female|       group D|               some college|free/reduced|                   none|        69|           65|           74|    6|\n",
      "|female|       group A|               some college|    standard|                   none|        69|           84|           82|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        70|           89|           88|    6|\n",
      "|female|       group B|               some college|    standard|                   none|        70|           75|           78|    6|\n",
      "|female|       group D|               some college|free/reduced|              completed|        70|           78|           78|    6|\n",
      "|female|       group C|               some college|    standard|              completed|        70|           72|           76|    6|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        70|           63|           58|    6|\n",
      "|  male|       group D|               some college|    standard|              completed|        71|           61|           69|    7|\n",
      "|female|       group D|               some college|free/reduced|                   none|        71|           83|           83|    7|\n",
      "|female|       group E|               some college|free/reduced|                   none|        71|           76|           70|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        71|           81|           80|    7|\n",
      "|female|       group E|               some college|    standard|                   none|        71|           70|           76|    7|\n",
      "|  male|       group D|               some college|    standard|                   none|        71|           49|           52|    7|\n",
      "|  male|       group B|               some college|    standard|              completed|        71|           75|           70|    7|\n",
      "|female|       group C|               some college|    standard|              completed|        71|           71|           80|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        72|           72|           71|    7|\n",
      "|female|       group A|               some college|    standard|              completed|        72|           79|           82|    7|\n",
      "|  male|       group D|               some college|    standard|                   none|        72|           57|           58|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        73|           80|           82|    7|\n",
      "|  male|       group C|               some college|    standard|                   none|        73|           74|           61|    7|\n",
      "|female|       group E|               some college|    standard|              completed|        73|           78|           76|    7|\n",
      "|female|       group C|               some college|    standard|                   none|        73|           76|           78|    7|\n",
      "|female|       group B|               some college|free/reduced|                   none|        74|           81|           76|    7|\n",
      "|female|       group D|               some college|    standard|                   none|        74|           89|           84|    7|\n",
      "|female|       group D|               some college|    standard|              completed|        74|           75|           79|    7|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        74|           77|           73|    7|\n",
      "|  male|       group B|               some college|free/reduced|              completed|        74|           77|           76|    7|\n",
      "|female|       group C|               some college|    standard|              completed|        75|           81|           84|    7|\n",
      "|female|       group E|               some college|free/reduced|              completed|        75|           88|           85|    7|\n",
      "|  male|       group A|               some college|free/reduced|                   none|        75|           81|           74|    8|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        75|           68|           65|    8|\n",
      "|female|       group D|               some college|    standard|              completed|        75|           77|           83|    8|\n",
      "|  male|       group C|               some college|    standard|                   none|        76|           78|           75|    8|\n",
      "|female|       group C|               some college|free/reduced|                   none|        76|           83|           88|    8|\n",
      "|  male|       group D|               some college|    standard|              completed|        76|           83|           79|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        76|           64|           66|    8|\n",
      "|  male|       group E|               some college|    standard|                   none|        76|           67|           67|    8|\n",
      "|  male|       group E|               some college|    standard|                   none|        76|           71|           72|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        76|           71|           73|    8|\n",
      "|female|       group E|               some college|    standard|                   none|        76|           78|           80|    8|\n",
      "|  male|       group D|               some college|    standard|              completed|        77|           62|           62|    8|\n",
      "|female|       group D|               some college|    standard|                   none|        77|           68|           77|    8|\n",
      "|female|       group C|               some college|free/reduced|                   none|        77|           90|           91|    8|\n",
      "|  male|       group D|               some college|free/reduced|                   none|        77|           62|           64|    8|\n",
      "|female|       group D|               some college|free/reduced|                   none|        77|           86|           86|    8|\n",
      "|  male|       group A|               some college|    standard|              completed|        78|           72|           70|    8|\n",
      "|female|       group A|               some college|    standard|              completed|        78|           87|           91|    8|\n",
      "|female|       group B|               some college|    standard|                   none|        79|           86|           92|    8|\n",
      "|  male|       group B|               some college|    standard|                   none|        79|           67|           67|    8|\n",
      "|female|       group D|               some college|    standard|                   none|        79|           86|           81|    8|\n",
      "|  male|       group C|               some college|    standard|              completed|        79|           79|           78|    8|\n",
      "|  male|       group D|               some college|    standard|                   none|        79|           73|           67|    9|\n",
      "|female|       group D|               some college|free/reduced|                   none|        79|           89|           86|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        79|           84|           91|    9|\n",
      "|female|       group D|               some college|    standard|                   none|        80|           90|           89|    9|\n",
      "|  male|       group C|               some college|free/reduced|                   none|        80|           64|           66|    9|\n",
      "|  male|       group A|               some college|free/reduced|              completed|        81|           78|           81|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        81|           74|           71|    9|\n",
      "|  male|       group D|               some college|    standard|                   none|        81|           82|           84|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        82|           90|           94|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        82|           97|           96|    9|\n",
      "|female|       group B|               some college|    standard|                   none|        82|           85|           87|    9|\n",
      "|  male|       group D|               some college|    standard|              completed|        82|           82|           88|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        83|           80|           73|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        83|           83|           90|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        84|           77|           71|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        84|           83|           78|    9|\n",
      "|female|       group C|               some college|    standard|                   none|        84|           87|           91|    9|\n",
      "|  male|       group C|               some college|    standard|                   none|        84|           87|           81|    9|\n",
      "|female|       group D|               some college|    standard|              completed|        85|           86|           98|    9|\n",
      "|  male|       group E|               some college|    standard|              completed|        85|           75|           68|    9|\n",
      "|  male|       group D|               some college|    standard|              completed|        85|           81|           85|    9|\n",
      "|  male|       group E|               some college|    standard|                   none|        86|           76|           74|    9|\n",
      "|female|       group E|               some college|    standard|              completed|        86|           85|           91|   10|\n",
      "|  male|       group E|               some college|free/reduced|              completed|        87|           74|           70|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        87|           89|           94|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        87|           84|           86|   10|\n",
      "|female|       group E|               some college|    standard|                   none|        87|           85|           93|   10|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           93|           93|   10|\n",
      "|  male|       group D|               some college|    standard|                   none|        88|           73|           78|   10|\n",
      "|female|       group C|               some college|    standard|              completed|        88|           95|           94|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        88|           85|           76|   10|\n",
      "|  male|       group D|               some college|    standard|                   none|        88|           77|           77|   10|\n",
      "|  male|       group C|               some college|    standard|                   none|        91|           74|           76|   10|\n",
      "|  male|       group B|               some college|    standard|              completed|        91|           96|           91|   10|\n",
      "|  male|       group E|               some college|free/reduced|                   none|        93|           90|           83|   10|\n",
      "|  male|       group C|               some college|    standard|              completed|        93|           84|           90|   10|\n",
      "|  male|       group E|               some college|    standard|                   none|        97|           87|           82|   10|\n",
      "|  male|       group C|               some college|    standard|              completed|        98|           86|           90|   10|\n",
      "|female|       group D|               some college|    standard|                   none|        98|          100|           99|   10|\n",
      "|  male|       group E|               some college|    standard|              completed|        99|           87|           81|   10|\n",
      "|female|       group E|               some college|    standard|                   none|       100|           92|           97|   10|\n",
      "|  male|       group A|               some college|    standard|              completed|       100|           96|           86|   10|\n",
      "|  male|       group D|               some college|    standard|              completed|       100|           97|           99|   10|\n",
      "|female|       group C|           some high school|free/reduced|                   none|         0|           17|           10|    1|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        18|           32|           28|    1|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        24|           38|           27|    1|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        27|           34|           32|    1|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        29|           40|           44|    1|\n",
      "|  male|       group E|           some high school|    standard|                   none|        30|           26|           22|    1|\n",
      "|female|       group B|           some high school|    standard|              completed|        32|           51|           44|    1|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        32|           34|           38|    1|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        35|           55|           60|    1|\n",
      "|female|       group B|           some high school|    standard|                   none|        37|           46|           46|    1|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        38|           49|           45|    1|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        38|           43|           43|    1|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        39|           39|           34|    1|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        40|           65|           64|    1|\n",
      "|female|       group B|           some high school|    standard|                   none|        41|           55|           51|    1|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        43|           53|           53|    1|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        44|           50|           51|    1|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        44|           64|           58|    1|\n",
      "|female|       group C|           some high school|    standard|              completed|        44|           51|           55|    2|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        44|           45|           45|    2|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        45|           37|           37|    2|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        45|           52|           49|    2|\n",
      "|  male|       group A|           some high school|    standard|              completed|        46|           41|           43|    2|\n",
      "|  male|       group A|           some high school|    standard|              completed|        47|           49|           49|    2|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        47|           59|           50|    2|\n",
      "|female|       group C|           some high school|    standard|                   none|        47|           54|           53|    2|\n",
      "|  male|       group B|           some high school|free/reduced|                   none|        48|           52|           45|    2|\n",
      "|female|       group A|           some high school|    standard|                   none|        48|           66|           65|    2|\n",
      "|female|       group D|           some high school|    standard|                   none|        48|           58|           54|    2|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        48|           58|           52|    2|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        48|           56|           51|    2|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        48|           54|           53|    2|\n",
      "|  male|       group C|           some high school|    standard|                   none|        49|           49|           41|    2|\n",
      "|female|       group C|           some high school|    standard|                   none|        49|           63|           56|    2|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        49|           58|           55|    2|\n",
      "|  male|       group B|           some high school|free/reduced|              completed|        49|           50|           52|    2|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        50|           64|           59|    3|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        50|           60|           60|    3|\n",
      "|female|       group D|           some high school|    standard|                   none|        51|           63|           61|    3|\n",
      "|  male|       group B|           some high school|    standard|              completed|        51|           54|           41|    3|\n",
      "|  male|       group C|           some high school|    standard|                   none|        51|           52|           44|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        51|           56|           53|    3|\n",
      "|  male|       group A|           some high school|    standard|                   none|        51|           31|           36|    3|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        52|           67|           72|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        53|           37|           40|    3|\n",
      "|  male|       group A|           some high school|    standard|                   none|        53|           54|           48|    3|\n",
      "|female|       group B|           some high school|    standard|              completed|        54|           61|           62|    3|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        55|           65|           62|    3|\n",
      "|  male|       group D|           some high school|free/reduced|              completed|        55|           59|           59|    3|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        55|           46|           43|    3|\n",
      "|  male|       group D|           some high school|    standard|                   none|        55|           47|           44|    3|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        56|           54|           52|    3|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        56|           61|           60|    3|\n",
      "|female|       group B|           some high school|    standard|                   none|        57|           67|           72|    3|\n",
      "|  male|       group C|           some high school|    standard|                   none|        57|           61|           54|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           58|           59|    4|\n",
      "|  male|       group C|           some high school|free/reduced|              completed|        59|           69|           65|    4|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        59|           42|           41|    4|\n",
      "|female|       group C|           some high school|    standard|              completed|        59|           54|           67|    4|\n",
      "|female|       group A|           some high school|    standard|              completed|        59|           85|           80|    4|\n",
      "|female|       group A|           some high school|free/reduced|                   none|        59|           73|           69|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           67|           61|    4|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        59|           63|           64|    4|\n",
      "|female|       group D|           some high school|    standard|                   none|        59|           72|           80|    4|\n",
      "|female|       group B|           some high school|    standard|              completed|        60|           70|           70|    4|\n",
      "|  male|       group D|           some high school|    standard|                   none|        60|           59|           54|    4|\n",
      "|female|       group B|           some high school|    standard|              completed|        60|           70|           74|    4|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        61|           57|           56|    4|\n",
      "|female|       group D|           some high school|    standard|              completed|        61|           74|           72|    4|\n",
      "|  male|       group B|           some high school|    standard|              completed|        61|           56|           56|    4|\n",
      "|  male|       group A|           some high school|free/reduced|              completed|        61|           62|           61|    4|\n",
      "|  male|       group A|           some high school|    standard|              completed|        62|           67|           69|    4|\n",
      "|  male|       group D|           some high school|    standard|                   none|        62|           67|           61|    5|\n",
      "|  male|       group D|           some high school|    standard|              completed|        62|           66|           68|    5|\n",
      "|female|       group B|           some high school|    standard|                   none|        62|           64|           66|    5|\n",
      "|  male|       group D|           some high school|free/reduced|                   none|        62|           49|           52|    5|\n",
      "|  male|       group C|           some high school|    standard|                   none|        62|           64|           55|    5|\n",
      "|  male|       group C|           some high school|    standard|              completed|        63|           60|           57|    5|\n",
      "|female|       group C|           some high school|    standard|                   none|        63|           73|           68|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        63|           67|           67|    5|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        63|           78|           79|    5|\n",
      "|female|       group D|           some high school|    standard|              completed|        64|           60|           74|    5|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        64|           79|           77|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        64|           53|           57|    5|\n",
      "|  male|       group A|           some high school|    standard|                   none|        64|           50|           43|    5|\n",
      "|  male|       group C|           some high school|    standard|                   none|        64|           58|           51|    5|\n",
      "|  male|       group B|           some high school|    standard|              completed|        65|           66|           62|    5|\n",
      "|female|       group C|           some high school|free/reduced|                   none|        65|           86|           80|    5|\n",
      "|female|       group C|           some high school|    standard|              completed|        65|           74|           77|    5|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        65|           59|           53|    5|\n",
      "|female|       group D|           some high school|    standard|                   none|        65|           82|           81|    6|\n",
      "|female|       group C|           some high school|    standard|                   none|        65|           69|           76|    6|\n",
      "|female|       group D|           some high school|    standard|              completed|        65|           78|           82|    6|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        65|           76|           75|    6|\n",
      "|female|       group B|           some high school|    standard|              completed|        65|           82|           78|    6|\n",
      "|female|       group B|           some high school|    standard|                   none|        66|           69|           68|    6|\n",
      "|female|       group D|           some high school|    standard|              completed|        66|           78|           78|    6|\n",
      "|  male|       group A|           some high school|    standard|              completed|        66|           68|           64|    6|\n",
      "|  male|       group B|           some high school|    standard|                   none|        67|           64|           61|    6|\n",
      "|female|       group B|           some high school|    standard|                   none|        67|           89|           82|    6|\n",
      "|female|       group C|           some high school|    standard|              completed|        67|           74|           77|    6|\n",
      "|  male|       group C|           some high school|    standard|              completed|        67|           73|           68|    6|\n",
      "|female|       group D|           some high school|free/reduced|                   none|        67|           84|           84|    6|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        68|           72|           64|    6|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        68|           63|           54|    6|\n",
      "|  male|       group E|           some high school|    standard|              completed|        68|           51|           57|    6|\n",
      "|female|       group D|           some high school|    standard|                   none|        68|           71|           75|    6|\n",
      "|  male|       group B|           some high school|    standard|                   none|        68|           54|           53|    6|\n",
      "|female|       group C|           some high school|    standard|                   none|        69|           75|           78|    7|\n",
      "|female|       group C|           some high school|    standard|                   none|        69|           73|           73|    7|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        69|           71|           65|    7|\n",
      "|  male|       group D|           some high school|    standard|                   none|        69|           66|           61|    7|\n",
      "|female|       group D|           some high school|free/reduced|              completed|        69|           86|           81|    7|\n",
      "|female|       group B|           some high school|    standard|                   none|        70|           64|           72|    7|\n",
      "|female|       group C|           some high school|    standard|              completed|        70|           82|           76|    7|\n",
      "|female|       group C|           some high school|free/reduced|              completed|        71|           84|           87|    7|\n",
      "|female|       group A|           some high school|    standard|                   none|        71|           83|           77|    7|\n",
      "|  male|       group A|           some high school|    standard|                   none|        71|           62|           50|    7|\n",
      "|  male|       group D|           some high school|    standard|              completed|        71|           69|           68|    7|\n",
      "|  male|       group B|           some high school|    standard|                   none|        72|           68|           67|    7|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        72|           81|           79|    7|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        72|           79|           77|    7|\n",
      "|female|       group D|           some high school|    standard|                   none|        73|           86|           82|    7|\n",
      "|female|       group D|           some high school|    standard|                   none|        73|           84|           85|    7|\n",
      "|  male|       group E|           some high school|free/reduced|              completed|        73|           67|           59|    7|\n",
      "|  male|       group C|           some high school|    standard|                   none|        73|           66|           66|    7|\n",
      "|  male|       group D|           some high school|    standard|                   none|        73|           66|           62|    8|\n",
      "|female|       group B|           some high school|    standard|                   none|        73|           79|           79|    8|\n",
      "|  male|       group C|           some high school|    standard|                   none|        73|           66|           63|    8|\n",
      "|  male|       group D|           some high school|    standard|              completed|        74|           71|           78|    8|\n",
      "|  male|       group D|           some high school|    standard|                   none|        74|           74|           72|    8|\n",
      "|  male|       group B|           some high school|    standard|                   none|        74|           63|           57|    8|\n",
      "|female|       group B|           some high school|free/reduced|              completed|        74|           90|           88|    8|\n",
      "|  male|       group E|           some high school|    standard|              completed|        74|           64|           60|    8|\n",
      "|female|       group E|           some high school|free/reduced|                   none|        74|           74|           72|    8|\n",
      "|female|       group C|           some high school|    standard|                   none|        74|           75|           82|    8|\n",
      "|  male|       group D|           some high school|    standard|                   none|        75|           74|           69|    8|\n",
      "|  male|       group C|           some high school|    standard|                   none|        75|           72|           62|    8|\n",
      "|female|       group D|           some high school|    standard|                   none|        76|           72|           71|    8|\n",
      "|  male|       group D|           some high school|    standard|              completed|        76|           70|           69|    8|\n",
      "|female|       group C|           some high school|    standard|              completed|        76|           87|           85|    8|\n",
      "|  male|       group C|           some high school|    standard|              completed|        76|           80|           73|    8|\n",
      "|  male|       group E|           some high school|    standard|              completed|        77|           76|           77|    8|\n",
      "|female|       group E|           some high school|    standard|                   none|        77|           79|           80|    8|\n",
      "|female|       group C|           some high school|    standard|              completed|        77|           90|           85|    9|\n",
      "|female|       group C|           some high school|    standard|                   none|        77|           91|           88|    9|\n",
      "|  male|       group D|           some high school|    standard|              completed|        77|           68|           69|    9|\n",
      "|  male|       group E|           some high school|free/reduced|              completed|        78|           83|           80|    9|\n",
      "|  male|       group D|           some high school|    standard|              completed|        78|           81|           86|    9|\n",
      "|  male|       group C|           some high school|    standard|              completed|        78|           72|           69|    9|\n",
      "|  male|       group A|           some high school|free/reduced|                   none|        79|           82|           73|    9|\n",
      "|  male|       group C|           some high school|free/reduced|                   none|        79|           76|           65|    9|\n",
      "|  male|       group B|           some high school|    standard|              completed|        79|           85|           86|    9|\n",
      "|  male|       group D|           some high school|free/reduced|              completed|        80|           79|           79|    9|\n",
      "|female|       group D|           some high school|    standard|                   none|        80|           90|           82|    9|\n",
      "|female|       group D|           some high school|    standard|              completed|        80|           92|           88|    9|\n",
      "|female|       group E|           some high school|    standard|              completed|        80|           85|           85|    9|\n",
      "|female|       group D|           some high school|    standard|                   none|        81|           97|           96|    9|\n",
      "|  male|       group D|           some high school|    standard|                   none|        81|           78|           78|    9|\n",
      "|  male|       group E|           some high school|    standard|              completed|        81|           75|           76|    9|\n",
      "|female|       group B|           some high school|    standard|                   none|        82|           82|           80|    9|\n",
      "|  male|       group E|           some high school|    standard|                   none|        82|           67|           61|    9|\n",
      "|  male|       group B|           some high school|    standard|              completed|        84|           83|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        84|           84|           80|   10|\n",
      "|female|       group C|           some high school|    standard|              completed|        85|           92|           93|   10|\n",
      "|  male|       group B|           some high school|    standard|              completed|        85|           84|           78|   10|\n",
      "|female|       group A|           some high school|    standard|              completed|        85|           90|           92|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        86|           80|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|                   none|        86|           73|           70|   10|\n",
      "|  male|       group E|           some high school|    standard|              completed|        87|           84|           76|   10|\n",
      "|  male|       group B|           some high school|    standard|                   none|        88|           84|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|              completed|        88|           74|           75|   10|\n",
      "|  male|       group D|           some high school|    standard|              completed|        89|           88|           82|   10|\n",
      "|  male|       group E|           some high school|    standard|              completed|        89|           84|           77|   10|\n",
      "|  male|       group E|           some high school|    standard|                   none|        92|           87|           78|   10|\n",
      "|female|       group A|           some high school|    standard|              completed|        92|          100|           97|   10|\n",
      "|  male|       group E|           some high school|    standard|                   none|        94|           88|           78|   10|\n",
      "|  male|       group B|           some high school|    standard|              completed|        94|           86|           87|   10|\n",
      "|female|       group D|           some high school|    standard|              completed|        97|          100|          100|   10|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"ntile\", ntile(10).over(w)).show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb01513-b583-47ba-9016-df6a33b0a181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.percent_rank() → pyspark.sql.column.Column\n",
    "Window function: returns the relative rank (i.e. percentile) of rows within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cbc75042-a995-4d53-9fc5-4299f27b5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| pr|\n",
      "+-----+---+\n",
      "|    1|0.0|\n",
      "|    1|0.0|\n",
      "|    2|0.4|\n",
      "|    3|0.6|\n",
      "|    3|0.6|\n",
      "|    4|1.0|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:15:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"pr\", percent_rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f6ffe69f-5971-4bd6-a8d5-45c3c54faf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|math score|                  pr|\n",
      "+----------+--------------------+\n",
      "|        44| 0.05128205128205128|\n",
      "|        49| 0.08547008547008547|\n",
      "|        58| 0.27586206896551724|\n",
      "|        77|                0.76|\n",
      "|        90|  0.9095022624434389|\n",
      "|        53|  0.1452991452991453|\n",
      "|        79|  0.6379310344827587|\n",
      "|        60| 0.28444444444444444|\n",
      "|        59|  0.3089887640449438|\n",
      "|        35|  0.0449438202247191|\n",
      "|        59| 0.37435897435897436|\n",
      "|        92|  0.9775280898876404|\n",
      "|        61| 0.23076923076923078|\n",
      "|        41| 0.07865168539325842|\n",
      "|        51| 0.12669683257918551|\n",
      "|        57| 0.24434389140271492|\n",
      "|        63| 0.49743589743589745|\n",
      "|        69|  0.6461538461538462|\n",
      "|        53|  0.1724137931034483|\n",
      "|        69|  0.5288888888888889|\n",
      "|        58| 0.28054298642533937|\n",
      "|        29|                 0.0|\n",
      "|        60|  0.3595505617977528|\n",
      "|        61|   0.334841628959276|\n",
      "|        95|  0.9683257918552036|\n",
      "|        74|                 0.8|\n",
      "|        88|  0.9550561797752809|\n",
      "|        75|  0.7022222222222222|\n",
      "|        97|  0.9733333333333334|\n",
      "|        24|0.011235955056179775|\n",
      "|        53| 0.24719101123595505|\n",
      "|        54|  0.2512820512820513|\n",
      "|        90|  0.9897435897435898|\n",
      "|        81|  0.7241379310344828|\n",
      "|        71|  0.6460674157303371|\n",
      "|        63|  0.3891402714932127|\n",
      "|        82|  0.7918552036199095|\n",
      "|        85|  0.8376068376068376|\n",
      "|        88|  0.9692307692307692|\n",
      "|        99|                 1.0|\n",
      "|        44| 0.04888888888888889|\n",
      "|        74|  0.6068376068376068|\n",
      "|        54| 0.19909502262443438|\n",
      "|        64|  0.4222222222222222|\n",
      "|        79|  0.7194570135746606|\n",
      "|        98|  0.9909502262443439|\n",
      "|        78|  0.6206896551724138|\n",
      "|        91|  0.9482758620689655|\n",
      "|        50| 0.16923076923076924|\n",
      "|        83|  0.8280542986425339|\n",
      "|        37|0.008547008547008548|\n",
      "|        55|  0.1623931623931624|\n",
      "|        19|                 0.0|\n",
      "|        39| 0.00904977375565611|\n",
      "|       100|  0.9911111111111112|\n",
      "|        77|  0.8769230769230769|\n",
      "|        40|                 0.0|\n",
      "|        88|  0.8793103448275862|\n",
      "|        88|  0.8803418803418803|\n",
      "|        41| 0.07692307692307693|\n",
      "|        27|0.010256410256410256|\n",
      "|        28|0.008888888888888889|\n",
      "|        71|  0.6133333333333333|\n",
      "|        45| 0.11235955056179775|\n",
      "|        82|  0.9282051282051282|\n",
      "|        67| 0.49321266968325794|\n",
      "|        52|  0.1282051282051282|\n",
      "|        75|  0.8256410256410256|\n",
      "|        79|  0.8974358974358975|\n",
      "|        48| 0.07111111111111111|\n",
      "|        66|  0.5337078651685393|\n",
      "|        65|  0.5384615384615384|\n",
      "|        32|0.013333333333333334|\n",
      "|        61|  0.4358974358974359|\n",
      "|        41|0.035555555555555556|\n",
      "|        62|   0.398876404494382|\n",
      "|        68|  0.5203619909502263|\n",
      "|        64|  0.3793103448275862|\n",
      "|        87|  0.8620689655172413|\n",
      "|        95|                 1.0|\n",
      "|        41| 0.03167420814479638|\n",
      "|        79|  0.8426966292134831|\n",
      "|        43| 0.04072398190045249|\n",
      "|        74|  0.6289592760180995|\n",
      "|        66|   0.558974358974359|\n",
      "|        45| 0.05333333333333334|\n",
      "|        96|  0.9572649572649573|\n",
      "|        36| 0.05128205128205128|\n",
      "|        64|  0.4550561797752809|\n",
      "|        71|  0.7128205128205128|\n",
      "|        55| 0.17777777777777778|\n",
      "|        50| 0.20224719101123595|\n",
      "|        47| 0.14358974358974358|\n",
      "|        44|  0.0898876404494382|\n",
      "|        89|  0.9662921348314607|\n",
      "|        54| 0.15384615384615385|\n",
      "|        81|  0.7948717948717948|\n",
      "|        72|  0.7384615384615385|\n",
      "|        70|  0.6348314606741573|\n",
      "|        38|0.056179775280898875|\n",
      "+----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"pr\", percent_rank().over(w)).select(\"math score\", \"pr\").distinct().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec764c-f6da-4048-89eb-d4a1298ed9d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.rank() → pyspark.sql.column.Column\n",
    "Window function: returns the rank of rows within a window partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca76cb0-c3b9-42a7-8e58-f51c74223a21",
   "metadata": {},
   "source": [
    "The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "This is equivalent to the RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5859a360-aa81-494b-876f-d75ccd15565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:20:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|drank|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    1|    1|\n",
      "|    2|    3|\n",
      "|    3|    4|\n",
      "|    3|    4|\n",
      "|    4|    6|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
    "w = Window.orderBy(\"value\")\n",
    "df.withColumn(\"drank\", rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "78c713f8-defb-4916-9a49-883118d94a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|math score|drank|\n",
      "+----------+-----+\n",
      "|        68|   57|\n",
      "|        73|  152|\n",
      "|        47|   24|\n",
      "|        72|  131|\n",
      "|        84|  187|\n",
      "|        90|  202|\n",
      "|        92|  211|\n",
      "|        72|   69|\n",
      "|        64|   96|\n",
      "|        58|   67|\n",
      "|        68|  116|\n",
      "|        78|  148|\n",
      "|        89|  106|\n",
      "|        42|   10|\n",
      "|        76|  168|\n",
      "|        64|  102|\n",
      "|        81|  169|\n",
      "|        66|   48|\n",
      "|        48|   30|\n",
      "|        50|   37|\n",
      "|        41|    8|\n",
      "|        55|   20|\n",
      "|        98|  221|\n",
      "|        81|   94|\n",
      "|        39|    3|\n",
      "|        88|  200|\n",
      "|        60|   80|\n",
      "|        69|   61|\n",
      "|        76|  148|\n",
      "|        27|    4|\n",
      "|        99|  223|\n",
      "|        52|   27|\n",
      "|        62|   32|\n",
      "|        70|   27|\n",
      "|        68|  114|\n",
      "|        39|   13|\n",
      "|        53|   18|\n",
      "|        64|   41|\n",
      "|        78|   84|\n",
      "|        75|  137|\n",
      "|        51|   14|\n",
      "|        75|   75|\n",
      "|        87|   51|\n",
      "|        66|  106|\n",
      "|        53|   38|\n",
      "|        90|   54|\n",
      "|        84|  163|\n",
      "|        71|   66|\n",
      "|        92|  109|\n",
      "|        69|   26|\n",
      "|        61|   70|\n",
      "|        63|   85|\n",
      "|        86|  168|\n",
      "|        29|    4|\n",
      "|        34|    7|\n",
      "|        65|  106|\n",
      "|        79|  176|\n",
      "|        70|   63|\n",
      "|        46|   13|\n",
      "|        63|   87|\n",
      "|        78|  174|\n",
      "|        58|   46|\n",
      "|        60|   65|\n",
      "|        49|   11|\n",
      "|        50|   12|\n",
      "|        23|    2|\n",
      "|        41|   16|\n",
      "|        77|  152|\n",
      "|        69|  120|\n",
      "|        75|  144|\n",
      "|        87|  102|\n",
      "|        46|    4|\n",
      "|        76|   79|\n",
      "|        32|    4|\n",
      "|        59|   56|\n",
      "|        80|   91|\n",
      "|        71|  140|\n",
      "|        86|  186|\n",
      "|        54|   36|\n",
      "|        84|  184|\n",
      "|        54|   19|\n",
      "|        74|   72|\n",
      "|        46|   27|\n",
      "|        48|   27|\n",
      "|        49|   33|\n",
      "|        81|  158|\n",
      "|        85|  165|\n",
      "|        26|    1|\n",
      "|        52|    9|\n",
      "|        97|  220|\n",
      "|        70|  125|\n",
      "|        54|   50|\n",
      "|       100|  224|\n",
      "|        81|   43|\n",
      "|        51|   39|\n",
      "|        88|  190|\n",
      "|        47|    5|\n",
      "|        68|  122|\n",
      "|        19|    1|\n",
      "|        48|   17|\n",
      "+----------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"drank\", rank().over(w)).select(\"math score\", \"drank\").distinct().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3e18c-67ca-402c-bfee-11e5c38e0cc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.row_number() → pyspark.sql.column.Column\n",
    "Window function: returns a sequential number starting at 1 within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9384c2aa-fe24-493a-9c2e-5492a29ce288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|desc_order|\n",
      "+---+----------+\n",
      "|  2|         1|\n",
      "|  1|         2|\n",
      "|  0|         3|\n",
      "+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/22 21:21:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(3)\n",
    "w = Window.orderBy(df.id.desc())\n",
    "df.withColumn(\"desc_order\", row_number().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bdbfea4a-ecd4-4853-9752-7ae4bc54b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|math score|desc_order|\n",
      "+----------+----------+\n",
      "|        32|         8|\n",
      "|        47|        24|\n",
      "|        53|        49|\n",
      "|        68|        57|\n",
      "|        68|        60|\n",
      "|        69|        62|\n",
      "|        72|        69|\n",
      "|        61|        77|\n",
      "|        62|        78|\n",
      "|        63|        81|\n",
      "|        60|        84|\n",
      "|        80|        93|\n",
      "|        69|       112|\n",
      "|        72|       131|\n",
      "|        73|       152|\n",
      "|        73|       153|\n",
      "|        84|       185|\n",
      "|        84|       187|\n",
      "|        90|       202|\n",
      "|        92|       211|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"parental level of education\").orderBy(\"math score\")\n",
    "students.withColumn(\"desc_order\", row_number().over(w)).select(\"math score\", \"desc_order\").distinct().limit(20).orderBy(asc(\"desc_order\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02006d2-6938-453c-aaa1-6e1cc5b7f8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Sort Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#sort-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd37ae-43f2-43d3-a5e6-0c1f9309cb8c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "36119302-a8fc-4cd4-90b4-ffb423e4b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(5)\n",
    "df = df.sort(desc(\"id\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7a981d3f-becb-4610-9142-82c8a39fe34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(asc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "44c15f16-f19b-4ae9-94c5-0cb8fde17158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>male</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender race/ethnicity parental level of education         lunch  \\\n",
       "0    female        group C            some high school  free/reduced   \n",
       "1    female        group B                 high school  free/reduced   \n",
       "2    female        group B            some high school  free/reduced   \n",
       "3    female        group B                some college      standard   \n",
       "4    female        group C                some college  free/reduced   \n",
       "..      ...            ...                         ...           ...   \n",
       "995  female        group E           bachelor's degree      standard   \n",
       "996    male        group A                some college      standard   \n",
       "997    male        group D                some college      standard   \n",
       "998    male        group E           bachelor's degree      standard   \n",
       "999  female        group E          associate's degree      standard   \n",
       "\n",
       "    test preparation course  math score  reading score  writing score  \n",
       "0                      none           0             17             10  \n",
       "1                      none           8             24             23  \n",
       "2                      none          18             32             28  \n",
       "3                      none          19             38             32  \n",
       "4                      none          22             39             33  \n",
       "..                      ...         ...            ...            ...  \n",
       "995                    none         100            100            100  \n",
       "996               completed         100             96             86  \n",
       "997               completed         100             97             99  \n",
       "998               completed         100            100            100  \n",
       "999                    none         100            100            100  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.orderBy(asc(\"math score\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fe147-2b19-43a3-af86-c8e85fb40f48",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc_nulls_first(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8bc2aecf-6e02-43a9-ab8f-3c908ecb82bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  0| NULL|\n",
      "|  2|Alice|\n",
      "|  1|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Bob\"),\n",
    "                             (0, None),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(asc_nulls_first(df1.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078833e-bcfb-4237-b206-771019b6b438",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.asc_nulls_last(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2f0eeb43-f6ec-4a45-9a2c-c18d8df3e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "|  1|  Bob|\n",
      "|  0| NULL|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(asc_nulls_last(df1.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08264c03-7481-476e-9e38-733497570554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------------+--------------------+-----+\n",
      "|     Player Name|Season|           Statistic|            Variable|Value|\n",
      "+----------------+------+--------------------+--------------------+-----+\n",
      "|   Jordan Spieth|  2018| Par 4 Eagle Leaders|Par 4 Eagle Leade...| NULL|\n",
      "|   Gary Woodland|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|Brendon de Jonge|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|      Jon Curran|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "| Charley Hoffman|  2017|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "| David Lingmerth|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Charlie Wi|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|    Patrick Reed|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|    Bubba Watson|  2013|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|    Zach Johnson|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Paul Casey|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|  Billy Horschel|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|       Luke List|  2017|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|      Paul Casey|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|      Ryan Moore|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|   Brooks Koepka|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|    Lee Westwood|  2014|Driving Pct. 240-...|Driving Pct. 240-...| NULL|\n",
      "|    Webb Simpson|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "|     Sean O'Hair|  2010|FedExCup Season P...|FedExCup Season P...| NULL|\n",
      "|        Kevin Na|  2016|% of Potential Pt...|% of Potential Pt...| NULL|\n",
      "+----------------+------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(asc(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "072ddcff-f89c-40e4-bce8-0db556ec3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------------------+--------------------+----------+\n",
      "|   Player Name|Season|           Statistic|            Variable|     Value|\n",
      "+--------------+------+--------------------+--------------------+----------+\n",
      "|   Chris Riley|  2010|Percentage of Ava...|Percentage of Ava...|$1,001,580|\n",
      "|   Chris Riley|  2010|Percentage of pot...|Percentage of pot...|$1,001,581|\n",
      "|   Chris Riley|  2010|Money per Event L...|Money per Event L...|$1,001,581|\n",
      "|Steve Stricker|  2017|      Official Money|Official Money - ...|$1,002,036|\n",
      "|Steve Stricker|  2017|Percentage of Ava...|Percentage of Ava...|$1,002,036|\n",
      "|Steve Stricker|  2017|Money per Event L...|Money per Event L...|$1,002,036|\n",
      "|Steve Stricker|  2017|Percentage of pot...|Percentage of pot...|$1,002,036|\n",
      "|  Robert Streb|  2016|Percentage of Ava...|Percentage of Ava...|$1,003,359|\n",
      "|  Robert Streb|  2016|Percentage of pot...|Percentage of pot...|$1,003,362|\n",
      "|  Robert Streb|  2016|Money per Event L...|Money per Event L...|$1,003,362|\n",
      "|  Robert Streb|  2016|Total Money (Offi...|Total Money (Offi...|$1,003,363|\n",
      "|  Robert Streb|  2016|      Official Money|Official Money - ...|$1,003,363|\n",
      "|      Jon Rahm|  2016|Percentage of Ava...|Percentage of Ava...|$1,004,033|\n",
      "|      Jon Rahm|  2016|Percentage of pot...|Percentage of pot...|$1,004,034|\n",
      "|      Jon Rahm|  2016|Money per Event L...|Money per Event L...|$1,004,034|\n",
      "|      Jon Rahm|  2016|Total Money (Offi...|Total Money (Offi...|$1,004,035|\n",
      "|      Jon Rahm|  2016|      Official Money|Official Money - ...|$1,004,035|\n",
      "|Brendan Steele|  2013|Percentage of Ava...|Percentage of Ava...|$1,004,159|\n",
      "|Brendan Steele|  2013|Percentage of pot...|Percentage of pot...|$1,004,160|\n",
      "|Brendan Steele|  2013|Money per Event L...|Money per Event L...|$1,004,160|\n",
      "+--------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(asc_nulls_last(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0115a-be99-4225-9735-016e2a8c5340",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b1a6930e-6106-45a0-8cab-8249173a009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).orderBy(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b22e20b3-ecf6-4509-a810-e76278c23ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|       Player Name|Season|           Statistic|            Variable|               Value|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|    Soren Kjeldsen|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Jason Dufner|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Vijay Singh|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|     Scott Gregory|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Ryan Ruffels|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Andrew Dorn|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|         John Hahn|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Kenny Perry|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Harry Ellis|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Andrew Svoboda|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|        Chris Kirk|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|     Justin Thomas|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|      Will Claxton|  2012|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|     Gary Woodland|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|Charles Howell III|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|      Jason Dufner|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|   Robert Karlsson|  2011|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Justin Leonard|  2014|       Longest Putts|Longest Putts - (...|the Memorial Tour...|\n",
      "|Charles Howell III|  2013|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(desc(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959cc72-7c75-4333-a9c5-d01837d06608",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc_nulls_first(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0b57cc6a-4ad3-4101-87ea-602f4b4013f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  0| NULL|\n",
      "|  1|  Bob|\n",
      "|  2|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(desc_nulls_first(df1.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4521b41-ce7b-46d6-9e13-1bc41ed451be",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.desc_nulls_last(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3e2c40a7-1b60-4c47-8399-1d394f0edc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|  Bob|\n",
      "|  2|Alice|\n",
      "|  0| NULL|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(0, None),\n",
    "                             (1, \"Bob\"),\n",
    "                             (2, \"Alice\")], [\"age\", \"name\"])\n",
    "df1.sort(desc_nulls_last(df1.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "35ebf309-69f0-403c-8c4c-67d939738b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|       Player Name|Season|           Statistic|            Variable|               Value|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "|      Jason Dufner|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Vijay Singh|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Soren Kjeldsen|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Andrew Dorn|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|     Scott Gregory|  2017|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Kenny Perry|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|      Ryan Ruffels|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|       Harry Ellis|  2018|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|         John Hahn|  2016|        Lowest Round|Lowest Round - (T...|     the Memorial/Mu|\n",
      "|    Andrew Svoboda|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Troy Matteson|  2011|      Longest Drives|Longest Drives - ...|the Memorial Tour...|\n",
      "|     Justin Thomas|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|      Chad Collins|  2010|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Gary Woodland|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|     Kevin Stadler|  2011|      Longest Drives|Longest Drives - ...|the Memorial Tour...|\n",
      "|      Jason Dufner|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|Charles Howell III|  2013|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2014|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|    Steve Stricker|  2011|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "|        Adam Scott|  2010|Longest Hole Outs...|Longest Hole Outs...|the Memorial Tour...|\n",
      "+------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tour.orderBy(desc_nulls_last(\"Value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916483f-91a2-43b5-b624-6fa9eb4103ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [String Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#string-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a405d4-40c7-472d-9c0b-7553c0ce6460",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ascii(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the numeric value of the first character of the string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "73586068-bf91-49a4-85ce-94922b67a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|ascii(value)|\n",
      "+------------+\n",
      "|          83|\n",
      "|          80|\n",
      "|          80|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(ascii(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080cdbd-2dbd-491a-92bf-f0290cbe4838",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.base64(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the BASE64 encoding of a binary column and returns it as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8fe35e49-cfbf-457b-a0b9-9ed984f619e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   base64(value)|\n",
      "+----------------+\n",
      "|        U3Bhcms=|\n",
      "|    UHlTcGFyaw==|\n",
      "|UGFuZGFzIEFQSQ==|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(base64(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d40bb5-5986-4a99-b6dd-a6681fde784a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the bit length for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "01b90731-32f1-4a4e-acec-ee620bc1e496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bit_length(cat)=24), Row(bit_length(cat)=32), Row(bit_length(cat)=48)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('cat',), ( '🐈',), ( 'кіт',)], ['cat'])\n",
    "df.select(bit_length('cat')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d43294-5110-478c-afce-293c89b0ed2b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.btrim(str: ColumnOrName, trim: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Remove the leading and trailing trim characters from str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "33a4e53e-785f-4771-a96c-b07198c874ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='parkSQ')]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"SSparkSQLS\", \"SL\", )], ['a', 'b'])\n",
    "df.select(btrim(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "48c4d90a-b841-459d-9748-c2d1d66ec62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='SparkSQL')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"    SparkSQL   \",)], ['a'])\n",
    "df.select(btrim(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829f7c1-82b2-4dc6-ae96-58d29460aeb0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.char(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the ASCII character having the binary equivalent to col. If col is larger than 256 the result is equivalent to char(col % 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "044c35c3-5ce2-4345-be9f-01290390d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|char(65)|\n",
      "+--------+\n",
      "|       A|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(char(lit(65))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cedd1c-90dd-4cf2-9a7f-6be9e6eb1035",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.character_length(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d8753ce7-eb44-4895-abf6-1a2efacce3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|character_length(SparkSQL)|\n",
      "+--------------------------+\n",
      "|                         8|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sf.character_length(sf.lit(\"SparkSQL\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d4380-cae7-4e82-ac1d-1c768e82ac8d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.char_length(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fa04afee-6f0c-41c5-97ac-599fddde280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|char_length(SparkSQL)|\n",
      "+---------------------+\n",
      "|                    8|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(char_length(lit(\"SparkSQL\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30741cf4-ab9c-4be3-b048-ab7f4486d3ce",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.concat_ws(sep: str, *cols: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "45d34ecb-ff0e-4e95-ae5b-de9533e9cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd-123')]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat_ws('-', df.s, df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b0ab0-1f90-4de6-9f46-964b291b0bd7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.contains(left: ColumnOrName, right: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if right is found inside left. Returns NULL if either input expression is NULL. Otherwise, returns False. Both left or right must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c55aba-132a-45eb-8fa6-e798192d070f",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- left: Column or str | \n",
    "The input column or strings to check, may be NULL.\n",
    "- right: Column or str | \n",
    "The input column or strings to find, may be NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1564d5fa-a8ea-4df8-b426-cbf9fad8ac66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n",
    "df.select(contains(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4a85e6da-3c7d-4ac2-b410-13cf8bed2f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c: binary (nullable = true)\n",
      " |-- d: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"c\", \"d\"])\n",
    "df = df.select(to_binary(\"c\").alias(\"c\"), to_binary(\"d\").alias(\"d\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "38ebaf26-377d-494a-98f2-104995c18d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|contains(c, d)|contains(d, c)|\n",
      "+--------------+--------------+\n",
      "|          true|         false|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(contains(\"c\", \"d\"), contains(\"d\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4637f-74d9-45ab-a514-42db41e8547f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.decode(col: ColumnOrName, charset: str) → pyspark.sql.column.Column\n",
    "Computes the first argument into a string from a binary using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "28b1f67a-f0de-4be0-8af9-9aa6c26113ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|decode(a, UTF-8)|\n",
      "+----------------+\n",
      "|            abcd|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['a'])\n",
    "df.select(decode(\"a\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "136d04b4-e011-48b9-930a-962b114d5225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|decode(a, UTF-8)|\n",
      "+----------------+\n",
      "| кіт кит дельфін|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('кіт кит дельфін',)], ['a'])\n",
    "df.select(decode(\"a\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece79dd-cba9-4485-8429-34e477135a9e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.elt(*inputs: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the n-th input, e.g., returns input2 when n is 2. The function returns NULL if the index exceeds the length of the array and spark.sql.ansi.enabled is set to false. If spark.sql.ansi.enabled is set to true, it throws ArrayIndexOutOfBoundsException for invalid indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "13c64c3e-0dbd-4bc4-ae90-63fdaf6b82e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='scala')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1ff60cb8-63c2-46e6-9228-1cd9dbbf5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='scala')]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "53df6d43-0903-4dff-8e01-cd11d62b139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='java')]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ggg\", \"scala\", \"java\")], ['a', 'b', 'c'])\n",
    "df.select(elt(lit(2), df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0400b2e2-df93-42b5-bedb-f0b19ca765c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None, r=None)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(elt(lit(0), df.b, df.c).alias('r'), elt(lit(3), df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ca776-3273-4460-8036-944df318a445",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.encode(col: ColumnOrName, charset: str) → pyspark.sql.column.Column\n",
    "Computes the first argument into a binary from a string using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e490f32d-91fb-4754-9fac-c9d9da2bf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|encode(c, UTF-8)|\n",
      "+----------------+\n",
      "|   [61 62 63 64]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['c'])\n",
    "df.select(encode(\"c\", \"UTF-8\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3039ed6e-a197-44d0-93c5-9553e5f2ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------+\n",
      "|encode                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|[FE FF 04 3A 04 56 04 42 00 20 04 3A 04 38 04 42 00 20 04 34 04 35 04 3B 04 4C 04 44 04 56 04 3D]|\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+----------------------+\n",
      "|decode(encode, UTF-16)|\n",
      "+----------------------+\n",
      "|       кіт кит дельфін|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('кіт кит дельфін',)], ['c'])\n",
    "df = df.select(encode(\"c\", \"UTF-16\").alias(\"encode\"))\n",
    "df.show(truncate=False)\n",
    "df.select(decode(\"encode\", \"UTF-16\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0c692-c3fa-4d58-a870-7de76b1351b5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.endswith(str: ColumnOrName, suffix: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if str ends with suffix. Returns NULL if either input expression is NULL. Otherwise, returns False. Both str or suffix must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "75af7500-b568-4731-898f-4673f54a205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=False)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
    "df.select(endswith(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "841a72ea-55fc-4483-8521-91611dd622cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- e: binary (nullable = true)\n",
      " |-- f: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"e\", \"f\"])\n",
    "df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "867dd8a0-8fde-4fcb-8850-c70d341ec9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|endswith(e, f)|endswith(f, e)|\n",
      "+--------------+--------------+\n",
      "|          true|         false|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(endswith(\"e\", \"f\"), endswith(\"f\", \"e\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc654ba-6b28-4190-a8bc-658ce4748b0a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.find_in_set(str: ColumnOrName, str_array: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the index (1-based) of the given string (str) in the comma-delimited list (strArray). Returns 0, if the string was not found or if the given string (str) contains a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ad8dcbe7-c0b4-437e-8c80-3a7591ab3459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ab\", \"abc,b,ab,c,def\")], ['a', 'b'])\n",
    "df.select(find_in_set(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80de42-ec13-4e69-9b2a-2e802fa3bc85",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.format_number(col: ColumnOrName, d: int) → pyspark.sql.column.Column\n",
    "Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "40e5bea4-8a70-44fc-ac4c-e89fd2174a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5.0000')]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c05005-8dea-4a04-bce7-134f7df519f6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.format_string(format: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Formats the arguments in printf-style and returns the result as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b192f663-9989-4d6d-bc3f-4090c9b61aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5 hello')]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
    "df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879684b5-dcc9-4556-a60f-d721c949534c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ilike(str: ColumnOrName, pattern: ColumnOrName, escapeChar: Optional[Column] = None) → pyspark.sql.column.Column\n",
    "Returns true if str matches pattern with escape case-insensitively, null if any arguments are null, false otherwise. The default escape character is the ‘’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862249fe-4179-4d8b-b60d-7160829d8597",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- str: Column or str | \n",
    "A string.\n",
    "- pattern: Column or str |\n",
    "A string. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order to match “\u0007bc”, the pattern should be “abc”. When SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back to Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the pattern to match “\u0007bc” should be “\u0007bc”.\n",
    "- escape: Column |\n",
    "An character added since Spark 3.0. The default escape character is the ‘’. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "dae24b4f-cfa4-4eb0-92e5-5d1a83d78477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
    "df.select(ilike(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ce69036d-6c93-40a6-9bb8-3c8641490ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
    "    ['a', 'b']\n",
    ")\n",
    "df.select(ilike(df.a, df.b, lit('/')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdefa7-2306-4d5d-8bd8-6805bf9eb43e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.initcap(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Translate the first letter of each word to upper case in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c96ee3e8-fc22-4a3f-aa12-68b7fc79936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='Ab Cd')]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e97d88-94ad-40c7-b63e-901a7eca6e8a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.instr(str: ColumnOrName, substr: str) → pyspark.sql.column.Column\n",
    "Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a594be7-6755-4620-ab82-fafabfd3c82e",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9700e872-b8fb-4de3-aa60-cbe527dedf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=2)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(instr(df.s, 'b').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cce506-89e8-4b8e-8d6c-e95b822617e8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lcase(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns str with all characters changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8c59d7d6-287b-4330-8957-48bfcb247e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|lcase(Spark)|\n",
      "+------------+\n",
      "|       spark|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sf.lcase(sf.lit(\"Spark\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949877f-77e8-4ab9-9582-1e84d2b9c56d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "cf6a1d99-6636-4f2d-80c4-8a7ea28045aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(length=4)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feecca4-5abb-48ba-97b0-3751fd6333ea",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.like(str: ColumnOrName, pattern: ColumnOrName, escapeChar: Optional[Column] = None) → pyspark.sql.column.Column\n",
    "Returns true if str matches pattern with escape, null if any arguments are null, false otherwise. The default escape character is the ‘’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a80e2b-0b3b-4ff5-88bb-d267287d48ec",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str |\n",
    "A string.\n",
    "- pattern: Column or str | \n",
    "A string. The pattern is a string which is matched literally, with exception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regular expressions) Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order to match “\u0007bc”, the pattern should be “abc”. When SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it falls back to Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the pattern to match “\u0007bc” should be “\u0007bc”.\n",
    "- escape: Column |\n",
    "An character added since Spark 3.0. The default escape character is the ‘’. If an escape character precedes a special symbol or another escape character, the following character is matched literally. It is invalid to escape any other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "59498fc2-a2de-4296-8281-5f0998fafa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
    "df.select(like(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1410c45c-0c5b-48f7-b8a1-71066e0f39fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
    "    ['a', 'b']\n",
    ")\n",
    "df.select(like(df.a, df.b, lit('/')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993e112-61e8-4a70-80f5-9ad3a1c4aaa8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lower(col: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Converts a string expression to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9fe4a1fa-d3ad-42e6-a2aa-196bedf0c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|lower(value)|\n",
      "+------------+\n",
      "|       spark|\n",
      "|     pyspark|\n",
      "|  pandas api|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(lower(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b82f3-c660-41fc-96b0-dc76f602be9f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.left(str: ColumnOrName, len: ColumnOrName) → pyspark.sql.column.Column\n",
    "    Returns the leftmost len`(`len can be string type) characters from the string str, if len is less or equal than 0 the result is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c0414682-1823-4f44-8390-e586be41db21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spa')]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
    "df.select(left(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a15683-c93f-48a6-94b5-44b6bb4a50dd",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.levenshtein(left: ColumnOrName, right: ColumnOrName, threshold: Optional[int] = None) → pyspark.sql.column.Column\n",
    "Computes the Levenshtein distance of the two given strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdb7f8-b658-45b8-b0b5-0f2f58af2490",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- leftColumn or str\n",
    "first column value.\n",
    "- right: Column or str | \n",
    "second column value.\n",
    "- threshold: int, optional | \n",
    "if set when the levenshtein distance of the two given strings less than or equal to a given threshold then return result distance, or -1\n",
    "\n",
    "Returns – Column | Levenshtein distance as integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "83e8e60c-2c32-460a-a9d6-18793f63917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
    "df0.select(levenshtein('l', 'r').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "323e1cc6-bcae-4ca4-994d-d5eafd9ae2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=-1)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.select(levenshtein('l', 'r', 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f0cd-520c-4615-ae10-dab27bd60fc7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.locate(substr: str, str: ColumnOrName, pos: int = 1) → pyspark.sql.column.Column\n",
    "Locate the position of the first occurrence of substr in a string column, after position pos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996d059-2e65-4226-bb39-aee1b7661186",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a342a11e-071e-45b0-aff8-7574bd2a4ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=2)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(locate('b', df.s, 1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad19392-f4cc-481e-bd99-5756381f7f11",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.lpad(col: ColumnOrName, len: int, pad: str) → pyspark.sql.column.Column\n",
    "Left-pad the string column to width len with pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "53307981-37ac-4288-99a5-dcead0a24ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='##abcd')]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(lpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4af9e7a0-76ff-4ed5-99ad-115536fe20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   s|\n",
      "+--------------------+\n",
      "|~~~~~Robert Garrigus|\n",
      "|~~~~~~~~Bubba Watson|\n",
      "|~~~~~~Dustin Johnson|\n",
      "|~~~~~Brett Wetterich|\n",
      "|~~~~~~~~~J.B. Holmes|\n",
      "|~~~~~~~~~~~John Daly|\n",
      "|~~~~~~~Graham DeLaet|\n",
      "|~~~~~~~Angel Cabrera|\n",
      "|~~~~~~Charles Warren|\n",
      "|~~~~~~~~~D.J. Trahan|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tour.limit(10).select(lpad(tour[\"Player name\"], 20, '~').alias('s')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97ad6f-a2eb-4515-bafc-16086375f3e8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ltrim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from left end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d3807d84-622b-4be6-8432-6e3181554a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|      r|length|\n",
      "+-------+------+\n",
      "|  Spark|     5|\n",
      "|Spark  |     7|\n",
      "|  Spark|     5|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15580a-701b-400c-935d-5b5db5fc399c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.mask(col: ColumnOrName, upperChar: Optional[ColumnOrName] = None, lowerChar: Optional[ColumnOrName] = None, digitChar: Optional[ColumnOrName] = None, otherChar: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Masks the given string value. This can be useful for creating copies of tables with sensitive information removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728a271-feed-4ae0-b9ef-869712aed9b4",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "    col: :class:`~pyspark.sql.Column` or str\n",
    "        target column to compute on.\n",
    "\n",
    "    upperChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace upper-case characters with. Specify NULL to retain original character.\n",
    "\n",
    "    lowerChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace lower-case characters with. Specify NULL to retain original character.\n",
    "\n",
    "    digitChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace digit characters with. Specify NULL to retain original character.\n",
    "\n",
    "    otherChar: :class:`~pyspark.sql.Column` or str\n",
    "        character to replace all other characters with. Specify NULL to retain original character.\n",
    "\n",
    "Returns | Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8cbc4fc2-29a8-431c-ab17-350f70958128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='XxXXnnn-@$#'), Row(r='xxxx-XXXX-nnnn-nnnn')]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"AbCD123-@$#\",), (\"abcd-EFGH-8765-4321\",)], ['data'])\n",
    "df.select(mask(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "47f7dd92-985c-476b-b2e9-13970d45b1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YxYYnnn-@$#'), Row(r='xxxx-YYYY-nnnn-nnnn')]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e4baca6d-305c-4f26-b660-b3a59087b0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYnnn-@$#'), Row(r='yyyy-YYYY-nnnn-nnnn')]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "610d2584-dc8f-47f2-b9b1-26e7965b0bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYddd-@$#'), Row(r='yyyy-YYYY-dddd-dddd')]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y'), lit('d')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "7dfbe8d3-e6e1-4dc7-8f92-9c9c47d72cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='YyYYddd****'), Row(r='yyyy*YYYY*dddd*dddd')]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(mask(df.data, lit('Y'), lit('y'), lit('d'), lit('*')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "edef701c-5bdb-4fec-bacd-ee14e2d9d066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='XxXXnnn-@$#'),\n",
       " Row(r='xxxx-XXXX-nnnn-nnnn'),\n",
       " Row(r='nnnn-nnnn-nnnn-nnnn'),\n",
       " Row(r='nnnnnnnn'),\n",
       " Row(r='Xxxxx Xxxxxxxx'),\n",
       " Row(r=\"Xxxx_nnnn_!'№;_xxxx\"),\n",
       " Row(r='x[XXn&[&xxxx')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"AbCD123-@$#\",),\n",
    "                            (\"abcd-EFGH-8765-4321\",),\n",
    "                            (\"1234-5678-8765-4321\",),\n",
    "                            (\"12345678\",),\n",
    "                            (\"Абаба Галамага\",),\n",
    "                            (\"Тест_1234_!'№;_test\",),\n",
    "                            (\"k[MY0&[&jwts\",),\n",
    "                           ], ['data'])\n",
    "df.select(mask(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fff05c-b57e-4e23-8059-304f0d079c98",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.octet_length(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the byte length for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "41fc574c-6706-4af5-a468-2590f58b72ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(octet_length(cat)=3), Row(octet_length(cat)=4)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
    "     .select(octet_length('cat')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894a794-bab5-4022-b667-9eeb5c095cf0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.parse_url(url: ColumnOrName, partToExtract: ColumnOrName, key: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Extracts a part from a URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361734c-1c8e-410c-ad78-e0573b19356f",
   "metadata": {},
   "source": [
    "    'HOST': Домен (наприклад, www.example.com).\n",
    "    'PATH': Шлях (наприклад, /page1).\n",
    "    'QUERY': Рядок запиту (наприклад, ?param1=value1&param2=value2).\n",
    "    'SCHEME': Протокол (наприклад, http).\n",
    "    'FILE': Шлях до файлу.\n",
    "    'REF': Фрагмент (якщо він вказаний у URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1d3725eb-0074-40cf-bc38-1e85c21c2948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='1')]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"http://spark.apache.org/path?query=1\", \"QUERY\", \"query\",)],\n",
    "    [\"a\", \"b\", \"c\"]\n",
    ")\n",
    "df.select(parse_url(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "0834cf30-875a-449d-b2eb-bf87b91c814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='query=1')]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "0de29889-dff4-4680-a0c2-e57ef3d99022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='/path')]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, lit(\"PATH\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "6f736315-4716-4c5a-b5fc-0888803966a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='spark.apache.org')]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(parse_url(df.a, lit(\"HOST\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f153ee-8f2f-4ee1-9701-f709ee490f42",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.position(substr: ColumnOrName, str: ColumnOrName, start: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Returns the position of the first occurrence of substr in str after position start. The given start and return value are 1-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b495d2bd-fbc1-4969-ac96-0be848cf21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|position(a, b, c)|\n",
      "+-----------------+\n",
      "|                7|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
    ").select(position(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "4e7f65b6-e687-45dd-aec4-64c3af8897cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|position(a, b, 1)|\n",
      "+-----------------+\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
    ").select(position(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137f345-f98c-46f9-bf55-c79ffcf501f7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.printf(format: ColumnOrName, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Formats the arguments in printf-style and returns the result as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6581d0f1-d978-4dbe-bb9d-055bfd0018d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|printf(a, b, c)|\n",
      "+---------------+\n",
      "|     aa123ccqwe|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"aa%d%sqwe\", 123, \"cc\",)], [\"a\", \"b\", \"c\"]\n",
    ").select(printf(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90846aa1-a290-4429-8ddd-0aa6a9820ce2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rlike(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f517d203-9f4e-4496-9617-0572d8228cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=True)]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"])\n",
    "df.select(rlike('str', lit(r'(\\d+)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "f03c3ddd-5dbe-45f0-b6ea-edec9eb406b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=False)]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(rlike('str', lit(r'\\d{2}b')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ca8b8061-ac33-4802-832f-fc8b65dba6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=True)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(rlike(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810f694-381a-497b-b0ff-9e8ba9d80e8f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606c633-30db-4ee3-8298-f90e3d82fca9",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str | \n",
    "target column to work on.\n",
    "- regexp: Column or str | \n",
    "regex pattern to apply.\n",
    "\n",
    "Returns – Column | true if str matches a Java regex, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "9d13a52e-7692-4f95-81eb-6b57025729cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|REGEXP(str, (\\d+))|\n",
      "+------------------+\n",
      "|              true|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', lit(r'(\\d+)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6fc92011-17fe-4adf-b473-a636f4393ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|REGEXP(str, \\d{2}b)|\n",
      "+-------------------+\n",
      "|              false|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', lit(r'\\d{2}b'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4def6e69-1013-405a-aa18-f085e09ba5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|REGEXP(str, regexp)|\n",
      "+-------------------+\n",
      "|               true|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp('str', col(\"regexp\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa87da-100c-4408-9738-033e1917586a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_like(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if str matches the Java regex regexp, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e35e7b-50e1-4ff3-8aed-cf31c4d28cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|REGEXP_LIKE(str, (\\d+))|\n",
      "+-----------------------+\n",
      "|                   true|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', lit(r'(\\d+)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8452d170-08f8-4ec7-971c-9fc6965e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|REGEXP_LIKE(str, \\d{2}b)|\n",
      "+------------------------+\n",
      "|                   false|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', lit(r'\\d{2}b'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b945518-74d4-4e11-8b83-56f471c73ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|REGEXP_LIKE(str, regexp)|\n",
      "+------------------------+\n",
      "|                    true|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
    ").select(regexp_like('str', col(\"regexp\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f9139-cb66-4bc0-8677-31b91e465a61",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_count(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a count of the number of times that the Java regex pattern regexp is matched in the string str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6243a3-8aa2-4951-9a87-50229013a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_count('str', lit(r'\\d+')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dbe0135-6ee0-4762-be03-2f65bc94e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_count('str', lit(r'mmm')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470ae498-5132-4ee7-9796-56ab41fb4b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_count(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec13ad-a251-4e33-ae93-a1dff8c91f17",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_extract(str: ColumnOrName, pattern: str, idx: int) → pyspark.sql.column.Column\n",
    "Extract a specific group matched by the Java regex regexp, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e7b9e2-755f-4994-afe8-badc906c942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='100')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e50677-fee7-40fc-804a-2c4c25ea0ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('foo',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574c49fd-32ea-46ea-8991-da5180555079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('aaaac',)], ['str'])\n",
    "df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4502b-92f1-4e76-a7e4-537406085683",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_extract_all(str: ColumnOrName, regexp: ColumnOrName, idx: Union[int, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Extract all strings in the str that match the Java regex regexp and corresponding to the regex group index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3239181-99eb-43aa-92dc-bcd2ed0dfb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"100-200, 300-400\", r\"(\\d+)-(\\d+)\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f39e61-1677-4842-ab0f-c2a7c66225b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 1).alias('d')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1980467f-c500-4859-93b0-86fd70918a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['200', '400'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 2).alias('d')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06af0978-da79-4a36-a06c-096bb216f8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=['100', '300'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract_all('str', col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b130c-7b7c-48ca-a68f-68f9e61c5293",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_replace(string: ColumnOrName, pattern: Union[str, pyspark.sql.column.Column], replacement: Union[str, pyspark.sql.column.Column]) → pyspark.sql.column.Column\n",
    "Replace all substrings of the specified string value that match regexp with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a57c72-13d8-4eb1-bc60-5e700fb8b91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='-----')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"100-200\", r\"(\\d+)\", \"--\")], [\"str\", \"pattern\", \"replacement\"])\n",
    "df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7401b823-c26f-4235-8334-e7c70964ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='-----')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_replace(\"str\", col(\"pattern\"), col(\"replacement\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06719ad5-0f72-411b-b492-fb158136cca7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_substr(str: ColumnOrName, regexp: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the substring that matches the Java regex regexp within the string str. If the regular expression is not found, the result is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c78866-f468-4847-a06e-b7f652946501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='1')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_substr('str', lit(r'\\d+')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a659e121-222d-42ae-b66b-af0b258a58f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_substr('str', lit(r'mmm')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877f7520-2cfa-40cf-b211-c9f73d03e6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='1')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_substr(\"str\", col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4b3d9-f611-40f6-83de-d82e1580c997",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.regexp_instr(str: ColumnOrName, regexp: ColumnOrName, idx: Union[int, pyspark.sql.column.Column, None] = None) → pyspark.sql.column.Column\n",
    "Extract all strings in the str that match the Java regex regexp and corresponding to the regex group index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645aaa2a-abe7-459c-a23c-ace91f5a8157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+(a|b|m)\")], [\"str\", \"regexp\"])\n",
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)')).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335a05a4-3391-4d70-8555-e049ff69bcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 1).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55752538-79e8-4678-ba32-b41a0be96bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 2).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8333d08f-b9eb-4908-968f-29da15cd39d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_instr('str', col(\"regexp\")).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e7adc-6798-4b72-a2da-449d67cfd878",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.replace(src: ColumnOrName, search: ColumnOrName, replace: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Replaces all occurrences of search with replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4875b5fd-9579-4b2d-af35-52414e19272f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='ABCDEF')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"ABCabc\", \"abc\", \"DEF\",)], [\"a\", \"b\", \"c\"])\n",
    "df.select(replace(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d35333d-647c-41b9-82be-7a73307d3d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='ABC')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(replace(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007d0a5-cef8-4f14-a794-9f86d4ad97c3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.right(str: ColumnOrName, len: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "    Returns the rightmost len`(`len can be string type) characters from the string str, if len is less or equal than 0 the result is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fe076ed-4707-4c27-869c-0a676d5fec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='SQL')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
    "df.select(right(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07983af-5996-4566-b638-ce9a329ed12a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ucase(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns str with all characters changed to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea206ba2-d64f-401f-8af2-f4ebf51a7236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|ucase(Spark)|\n",
      "+------------+\n",
      "|       SPARK|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(ucase(lit(\"Spark\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c164206-b7a7-4e81-8e0f-ad52613735e4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.unbase64(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Decodes a BASE64 encoded string column and returns it as a binary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4da8fef0-7843-4e5f-b301-4c5aea759e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     unbase64(value)|\n",
      "+--------------------+\n",
      "|    [53 70 61 72 6B]|\n",
      "|[50 79 53 70 61 7...|\n",
      "|[50 61 6E 64 61 7...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"U3Bhcms=\",\n",
    "                            \"UHlTcGFyaw==\",\n",
    "                            \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n",
    "df.select(unbase64(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71262f5-94c8-4ee0-bcc4-74b924f9d054",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rpad(col: ColumnOrName, len: int, pad: str) → pyspark.sql.column.Column\n",
    "Right-pad the string column to width len with pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77d9b861-625d-47fc-8d6f-55c858e31e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd##')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(rpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37306412-bf4a-4f4d-ab85-36549aca8912",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.repeat(col: ColumnOrName, n: int) → pyspark.sql.column.Column\n",
    "Repeats a string column n times, and returns it as a new string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e46ff79f-3d02-47e8-b701-a843e833cdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='ababab')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ab',)], ['s',])\n",
    "df.select(repeat(df.s, 3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c3cc-41d3-4adb-b886-072bfd32a41e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.rtrim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from right end for the specified string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b82fbf3b-c244-4db2-bbdf-485ac6151035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|       r|length|\n",
      "+--------+------+\n",
      "|   Spark|     8|\n",
      "|   Spark|     5|\n",
      "|   Spark|     6|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afb40e-9407-4975-b6c4-0af5c233ca04",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.soundex(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the SoundEx encoding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c034022e-842d-47b7-a67f-bcfedb6bdc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(soundex='P362'), Row(soundex='U612')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
    "df.select(soundex(df.name).alias(\"soundex\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa79fa-31a2-4356-8498-a2a75cba85ed",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.split(str: ColumnOrName, pattern: str, limit: int = - 1) → pyspark.sql.column.Column\n",
    "Splits str around matches of the given pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d9afe-47c8-462a-a034-1c51c3f9f423",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- str: Column or str | \n",
    "a string expression to split\n",
    "- pattern: str | \n",
    "a string representing a regular expression. The regex string should be a Java regular expression.\n",
    "- limit: int, optional | \n",
    "an integer which controls the number of times pattern is applied.\n",
    "    * limit > 0: The resulting array’s length will not be more than limit, and the\n",
    "resulting array’s last entry will contain all input beyond the last matched pattern.\n",
    "    * limit <= 0: pattern will be applied as many times as possible, and the resulting\n",
    "array can be of any size.\n",
    "\n",
    "Returns – Column | \n",
    "array of separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f804cd54-e963-4b77-aaf1-1fde2b97aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=['one', 'twoBthreeC'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
    "df.select(split(df.s, '[ABC]', 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d041b1dd-90d3-4e54-8652-ddf2496468c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=['one', 'two', 'three', ''])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(split(df.s, '[ABC]', -1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d44796-6125-4a38-82d4-7a8014c066fc",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.split_part(src: ColumnOrName, delimiter: ColumnOrName, partNum: ColumnOrName) → pyspark.sql.column.Column\n",
    "Splits str by delimiter and return requested part of the split (1-based). If any input is null, returns null. if partNum is out of range of split parts, returns empty string. If partNum is 0, throws an error. If partNum is negative, the parts are counted backward from the end of the string. If the delimiter is an empty string, the str is not split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5473c9df-58f1-4a21-a2fa-5be85ae11dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='13')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"11.12.13\", \".\", 3,)], [\"a\", \"b\", \"c\"])\n",
    "df.select(split_part(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648894-8944-409e-989c-eb153d06d6f1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.startswith(str: ColumnOrName, prefix: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a boolean. The value is True if str starts with prefix. Returns NULL if either input expression is NULL. Otherwise, returns False. Both str or prefix must be of STRING or BINARY type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43e7b20a-0c20-4501-b28b-dcacd17bbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
    "df.select(startswith(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64d0b253-cfc9-4b22-a402-4db19335351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- e: binary (nullable = true)\n",
      " |-- f: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\", \"4142\",)], [\"e\", \"f\"])\n",
    "df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfa1de02-e176-42cf-8146-59655f2e51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|startswith(e, f)|startswith(f, e)|\n",
      "+----------------+----------------+\n",
      "|            true|           false|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(startswith(\"e\", \"f\"), startswith(\"f\", \"e\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41689e-2bf5-43d4-859c-2c3032697fc4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substr(str: ColumnOrName, pos: ColumnOrName, len: Optional[ColumnOrName] = None) → pyspark.sql.column.Column¶\n",
    "Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceed9698-2d17-49b2-9f54-a70916ace338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|substr(a, b, c)|\n",
      "+---------------+\n",
      "|              k|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
    ").select(substr(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea66e77d-e119-4429-b6f7-6a93e426a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|substr(a, b, 2147483647)|\n",
      "+------------------------+\n",
      "|                   k SQL|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
    ").select(substr(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec889031-9f24-4224-bd7a-0936dd48039a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substring(str: ColumnOrName, pos: int, len: int) → pyspark.sql.column.Column\n",
    "Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71f041-333f-4431-a308-cb7ab79dfa31",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "The position is not zero based, but 1 based index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "104469ad-d73d-47a5-a292-d0e774e24b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='ab')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(substring(df.s, 1, 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d554ab-5d03-4c87-bb0b-e17c9036c7a0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.substring_index(str: ColumnOrName, delim: str, count: int) → pyspark.sql.column.Column\n",
    "Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25bbfadf-8ae6-4112-8ade-daf7ea281c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='a.b')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
    "df.select(substring_index(df.s, '.', 2).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60a08534-c4ed-4283-9c24-1559063f70f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='b.c.d')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(substring_index(df.s, '.', -3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ec10b-0c37-450f-a087-8a1fa56187fe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.overlay(src: ColumnOrName, replace: ColumnOrName, pos: Union[ColumnOrName, int], len: Union[ColumnOrName, int] = - 1) → pyspark.sql.column.Column\n",
    "Overlay the specified portion of src with replace, starting from byte position pos of src and proceeding for len bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92deee66-5920-48b4-8454-e5206215ae79",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- src: Column or str | \n",
    "column name or column containing the string that will be replaced\n",
    "- replace: Column or str | \n",
    "column name or column containing the substitution string\n",
    "- pos: Column or str or int | \n",
    "column name, column, or int containing the starting position in src\n",
    "- len: Column or str or int, optional | \n",
    "column name, column, or int containing the number of bytes to replace in src string by ‘replace’ defaults to -1, which represents the length of the ‘replace’ string\n",
    "\n",
    "Returns – Column | \n",
    "string with replaced values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "405ccb20-e097-43ef-80c9-f77f5f0458d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_CORE')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
    "df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79ee9b01-8673-4d1b-b91a-2578549da599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_CORESQL')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(overlay(\"x\", \"y\", 7, 0).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3916e7ae-020f-4716-a0d2-bf82e8d4d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlayed='SPARK_COREL')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(overlay(\"x\", \"y\", 7, 2).alias(\"overlayed\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7973acb-becf-4a8d-b764-b35321bee353",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sentences(string: ColumnOrName, language: Optional[ColumnOrName] = None, country: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Splits a string into arrays of sentences, where each sentence is an array of words. The ‘language’ and ‘country’ arguments are optional, and if omitted, the default locale is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c65828df-a257-479c-82fb-b49866152874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|sentences(string, en, US)          |\n",
      "+-----------------------------------+\n",
      "|[[This, is, an, example, sentence]]|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\n",
    "df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd7c12ba-44f0-415e-b66e-03a2eb994ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|sentences(s, , )                 |\n",
      "+---------------------------------+\n",
      "|[[Hello, world], [How, are, you]]|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"Hello world. How are you?\"]], [\"s\"])\n",
    "df.select(sentences(\"s\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffd68a-d53d-4448-a08c-fe4c442cc8d9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_binary(col: ColumnOrName, format: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "Converts the input col to a binary value based on the supplied format. The format can be a case-insensitive string literal of “hex”, “utf-8”, “utf8”, or “base64”. By default, the binary format for conversion is “hex” if format is omitted. The function returns NULL if at least one of the input parameters is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a772c500-632f-4b43-89ee-1aadd70ea24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'abc'))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
    "df.select(to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ffd1901-6d77-4ea8-b056-5574dec752ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'ABC'))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
    "df.select(to_binary(df.e).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf579e9d-e707-4177-8fe6-ea183747ff95",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_char(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Convert col to a string based on the format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d281f0ef-794d-475b-803d-939908f36888",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input value, generating a result string of the same length as the corresponding sequence in the format string. The result string is left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of the decimal value, starts with 0, and is before the decimal point. Otherwise, it is padded with spaces. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive values but ‘MI’ prints a space. ‘PR’: Only allowed at the end of the format string; specifies that the result string will be wrapped by angle brackets if the input value is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5d72722-6217-4037-9578-e414f857dbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='$78.12')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
    "df.select(to_char(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe377-474e-4609-9d8a-9200330caead",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_number(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Convert string ‘col’ to a number based on the string format ‘format’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdddf0-537a-4a95-943d-350a55286434",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input string. If the 0/9 sequence starts with 0 and is before the decimal point, it can only match a digit sequence of the same size. Otherwise, if the sequence starts with 9 or is after the decimal point, it can match a digit sequence that has the same or smaller size. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘col’ must match the grouping separator relevant for the size of the number. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ allows ‘-‘ but ‘MI’ does not. ‘PR’: Only allowed at the end of the format string; specifies that ‘col’ indicates a negative number with wrapping angled brackets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ff7f611-8757-4757-b415-d448bde0ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=Decimal('78.12'))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
    "df.select(to_number(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5a23c-5e0e-4ff7-ac42-8e24517d9b46",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.to_varchar(col: ColumnOrName, format: ColumnOrName) → pyspark.sql.column.Column\n",
    "Convert col to a string based on the format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96757825-b2bc-44d6-9eeb-42be11f8bc2b",
   "metadata": {},
   "source": [
    "Throws an exception if the conversion fails. The format can consist of the following characters, case insensitive: ‘0’ or ‘9’: Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format string matches a sequence of digits in the input value, generating a result string of the same length as the corresponding sequence in the format string. The result string is left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of the decimal value, starts with 0, and is before the decimal point. Otherwise, it is padded with spaces. ‘.’ or ‘D’: Specifies the position of the decimal point (optional, only allowed once). ‘,’ or ‘G’: Specifies the position of the grouping (thousands) separator (,). There must be a 0 or 9 to the left and right of each grouping separator. ‘′:𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑠𝑡ℎ𝑒𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛𝑜𝑓𝑡ℎ𝑒\n",
    " currency sign. This character may only be specified once. ‘S’ or ‘MI’: Specifies the position of a ‘-‘ or ‘+’ sign (optional, only allowed once at the beginning or end of the format string). Note that ‘S’ prints ‘+’ for positive values but ‘MI’ prints a space. ‘PR’: Only allowed at the end of the format string; specifies that the result string will be wrapped by angle brackets if the input value is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "054bb93d-97ff-41f9-a8b3-d05e7a82347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='$78.12')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
    "df.select(to_varchar(df.e, lit(\"$99.99\")).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca88d41-d0dd-4ac7-a92e-6243d25f9d5d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.translate(srcCol: ColumnOrName, matching: str, replace: str) → pyspark.sql.column.Column\n",
    "A function translate any character in the srcCol by a character in matching. The characters in replace is corresponding to the characters in matching. Translation will happen whenever any character in the string is matching with the character in the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9a58a99-be4a-492f-b0db-f79550c6676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='1a2s3ae')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('translate',)], ['a']) \\\n",
    "     .select(translate('a', \"rnlt\", \"123\") \\\n",
    "     .alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfb0ff-32d0-4836-98b3-c15c10df39bf",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.trim(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Trim the spaces from both ends for the specified string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c03403e-d0ae-48f7-b3ea-6b0e873a6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|    r|length|\n",
      "+-----+------+\n",
      "|Spark|     5|\n",
      "|Spark|     5|\n",
      "|Spark|     5|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
    "df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f73ae-250d-41a5-8add-ff3173337bb4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.upper(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Converts a string expression to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5924cae-fd19-46c2-82df-3b05ba89d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|upper(value)|\n",
      "+------------+\n",
      "|       SPARK|\n",
      "|     PYSPARK|\n",
      "|  PANDAS API|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
    "df.select(upper(\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c5b66ce-9a63-4586-b95c-952e2c3ea817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    Player name|upper(Player name)|\n",
      "+---------------+------------------+\n",
      "|Robert Garrigus|   ROBERT GARRIGUS|\n",
      "|   Bubba Watson|      BUBBA WATSON|\n",
      "| Dustin Johnson|    DUSTIN JOHNSON|\n",
      "|Brett Wetterich|   BRETT WETTERICH|\n",
      "|    J.B. Holmes|       J.B. HOLMES|\n",
      "|      John Daly|         JOHN DALY|\n",
      "|  Graham DeLaet|     GRAHAM DELAET|\n",
      "|  Angel Cabrera|     ANGEL CABRERA|\n",
      "| Charles Warren|    CHARLES WARREN|\n",
      "|    D.J. Trahan|       D.J. TRAHAN|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tour.limit(10).select(\"Player name\", upper(\"Player name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b68a9-1a74-4fce-8471-74af4ceaf1e3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.url_decode(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Decodes a str in ‘application/x-www-form-urlencoded’ format using a specific encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20eb61a0-0282-494f-9a66-722b613a20cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='https://spark.apache.org')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"a\"])\n",
    "df.select(url_decode(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90464bab-c420-4f64-be1f-063daf047065",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.url_encode(str: ColumnOrName) → pyspark.sql.column.Column\n",
    "Translates a string into ‘application/x-www-form-urlencoded’ format using a specific encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9717cf5-372d-47f0-89da-e38778ec8938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='https%3A%2F%2Fspark.apache.org')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"https://spark.apache.org\",)], [\"a\"])\n",
    "df.select(url_encode(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ec5e6-d7d7-44e3-8c79-6d2d9ba87156",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Bitwise Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#bitwise-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752813bb-5b08-49eb-8db7-dde98ada0870",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_count(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bc9f6bb0-504d-40d1-8d7d-187db4293adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|bit_count(c)|\n",
      "+------------+\n",
      "|           1|\n",
      "|           1|\n",
      "|           1|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
    "df.select(bit_count(\"c\")).show()\n",
    "# Returns Column | the number of bits that are set in the argument expr as\n",
    "# an unsigned 64-bit integer, or NULL if the argument is NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77ae61-4b0f-4781-9131-2850c57455fb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.bit_get(col: ColumnOrName, pos: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the value of the bit (0 or 1) at the specified position. The positions are numbered from right to left, starting at zero. The position argument cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "426c3261-e5e0-4351-a88b-090e61be4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  2|  1|  0|\n",
      "+---+---+---+\n",
      "|  0|  0|  1|\n",
      "|  0|  0|  1|\n",
      "|  0|  1|  0|\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  1|  0|  1|\n",
      "|  1|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[1],[1],[2],[3],[4],[5],[6],], [\"c\"])\n",
    "df.select(bit_get(\"c\", lit(2)).alias(\"2\"), bit_get(\"c\", lit(1)).alias(\"1\"), bit_get(\"c\", lit(0)).alias(\"0\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc915872-e8b9-46c1-a8a9-83704a71414a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.getbit(col: ColumnOrName, pos: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the value of the bit (0 or 1) at the specified position. The positions are numbered from right to left, starting at zero. The position argument cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "101dbab4-bdeb-4f84-a56c-39a89bd79c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|getbit(c, 1)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "spark.createDataFrame(\n",
    "    [[1], [1], [2]], [\"c\"]\n",
    ").select(sf.getbit(\"c\", sf.lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6b785-7ad8-433f-848e-3e62ac39c717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Call Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#call-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f3a09-27d0-48f9-b541-d525653b4ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.call_function(funcName: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Call a SQL function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a276f-b9da-4a84-8dfe-4def5c417595",
   "metadata": {},
   "source": [
    "`print(call_function.__doc__)`\n",
    "\n",
    "    Call a SQL function.\n",
    "\n",
    "    .. versionadded:: 3.5.0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    funcName : str\n",
    "        function name that follows the SQL identifier syntax (can be quoted, can be qualified)\n",
    "    cols : :class:`~pyspark.sql.Column` or str\n",
    "        column names or :class:`~pyspark.sql.Column`\\s to be used in the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`~pyspark.sql.Column`\n",
    "        result of executed function.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from pyspark.sql.functions import call_udf, col\n",
    "    >>> from pyspark.sql.types import IntegerType, StringType\n",
    "    >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "    >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "    >>> df.select(call_function(\"intX2\", \"id\")).show()\n",
    "    +---------+\n",
    "    |intX2(id)|\n",
    "    +---------+\n",
    "    |        2|\n",
    "    |        4|\n",
    "    |        6|\n",
    "    +---------+\n",
    "    >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "    >>> df.select(call_function(\"strX2\", col(\"name\"))).show()\n",
    "    +-----------+\n",
    "    |strX2(name)|\n",
    "    +-----------+\n",
    "    |         aa|\n",
    "    |         bb|\n",
    "    |         cc|\n",
    "    +-----------+\n",
    "    >>> df.select(call_function(\"avg\", col(\"id\"))).show()\n",
    "    +-------+\n",
    "    |avg(id)|\n",
    "    +-------+\n",
    "    |    2.0|\n",
    "    +-------+\n",
    "    >>> _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
    "    ... # doctest: +SKIP\n",
    "    >>> df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
    "    ... # doctest: +SKIP\n",
    "    +------------------------------------+\n",
    "    |spark_catalog.default.custom_avg(id)|\n",
    "    +------------------------------------+\n",
    "    |                               102.0|\n",
    "    +------------------------------------+\n",
    "    >>> df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()\n",
    "    ... # doctest: +SKIP\n",
    "    +------------------------------------+\n",
    "    |spark_catalog.default.custom_avg(id)|\n",
    "    +------------------------------------+\n",
    "    |                               102.0|\n",
    "    +------------------------------------+\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88647d6e-2fd0-4e1f-bc72-badce98a45f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|intX2(id)|\n",
      "+---------+\n",
      "|        2|\n",
      "|        4|\n",
      "|        6|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "_ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "df.select(call_function(\"intX2\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d5aa02-e975-469b-9234-cfe647c048ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|strX2(name)|\n",
      "+-----------+\n",
      "|         aa|\n",
      "|         bb|\n",
      "|         cc|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "df.select(call_function(\"strX2\", col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58db1daa-4852-4c82-945e-aae629c3e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|avg(id)|\n",
      "+-------+\n",
      "|    2.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(call_function(\"avg\", col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7528abf4-96b4-47b6-bdea-1ac66ab9e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
    "# df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
    "# df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b86237-19e5-4f92-8f04-99e5dcab40af",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.call_udf(udfName: str, *cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Call an user-defined function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8f898-feec-4991-8c3c-6545d2c57894",
   "metadata": {},
   "source": [
    "`print(call_udf.__doc__)`\n",
    "\n",
    "    Call an user-defined function.\n",
    "\n",
    "    .. versionadded:: 3.4.0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    udfName : str\n",
    "        name of the user defined function (UDF)\n",
    "    cols : :class:`~pyspark.sql.Column` or str\n",
    "        column names or :class:`~pyspark.sql.Column`\\s to be used in the UDF\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`~pyspark.sql.Column`\n",
    "        result of executed udf.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from pyspark.sql.functions import call_udf, col\n",
    "    >>> from pyspark.sql.types import IntegerType, StringType\n",
    "    >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
    "    >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
    "    >>> df.select(call_udf(\"intX2\", \"id\")).show()\n",
    "    +---------+\n",
    "    |intX2(id)|\n",
    "    +---------+\n",
    "    |        2|\n",
    "    |        4|\n",
    "    |        6|\n",
    "    +---------+\n",
    "    >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "    >>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n",
    "    +-----------+\n",
    "    |strX2(name)|\n",
    "    +-----------+\n",
    "    |         aa|\n",
    "    |         bb|\n",
    "    |         cc|\n",
    "    +-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e29366c-fa71-4bf9-8091-04160381b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|intX2(math score)|\n",
      "+-----------------+\n",
      "|              144|\n",
      "|              138|\n",
      "|              180|\n",
      "|               94|\n",
      "|              152|\n",
      "|              142|\n",
      "|              176|\n",
      "|               80|\n",
      "|              128|\n",
      "|               76|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"intX1.5\", lambda i: i * 1.5, IntegerType())\n",
    "students.limit(10).select(call_udf(\"intX2\", \"math score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6623311b-2b1a-420e-9d44-837623a9fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 16:03:15 WARN SimpleFunctionRegistry: The function strx2 replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|strX2(Player name)            |\n",
      "+------------------------------+\n",
      "|Robert GarrigusRobert Garrigus|\n",
      "|Bubba WatsonBubba Watson      |\n",
      "|Dustin JohnsonDustin Johnson  |\n",
      "|Brett WetterichBrett Wetterich|\n",
      "|J.B. HolmesJ.B. Holmes        |\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
    "tour.limit(5).select(call_udf(\"strX2\", col(\"Player name\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee1340-23c1-46f1-8913-a34df8877ddb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)\n",
    "Creates a pandas user defined function (a.k.a. vectorized user defined function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa0e67-c938-4e22-b727-876298cd7763",
   "metadata": {},
   "source": [
    "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "Parameters:\n",
    "- f: function, optional |\n",
    "user-defined function. A python function if used as a standalone function\n",
    "- return: Typepyspark.sql.types.DataType or str, optional | \n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "- function: Typeint, optional | \n",
    "an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR. This parameter exists for compatibility. Using Python type hints is encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fac94-278b-4a7c-a49d-808e8213c7cd",
   "metadata": {},
   "source": [
    "[why] Get same errors with all code snippets. Probably unmatching versions of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06563e5-4c24-4323-a1ed-ab5d4127a4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/25 12:59:32 ERROR ArrowPythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/zsavchenko/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/zsavchenko/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/10/25 12:59:32 ERROR ArrowPythonRunner: This may have been caused by a prior exception:\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/10/25 12:59:32 ERROR Executor: Exception in task 0.0 in stage 17.0 (TID 37)\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/10/25 12:59:32 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 37) (192.168.0.179 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "23/10/25 12:59:32 ERROR TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o267.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 37) (192.168.0.179 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m students\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName_Length\u001b[39m\u001b[38;5;124m\"\u001b[39m, slen(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \u001b[38;5;66;03m# .show()\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o267.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 37) (192.168.0.179 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(returnType=IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()\n",
    "\n",
    "# _ = spark.udf.register(\"slen\", slen, IntegerType())\n",
    "\n",
    "df = students.select(\"gender\")\n",
    "df = df.withColumn(\"Name_Length\", slen(df[\"gender\"])) # .show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b33e61a4-7b73-44ee-ae7b-586edf3c0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "60c1988a-27f8-40b3-9677-0b54cdeab610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cd4dfb6f-98dd-4615-a9bc-70a49e834b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f8985236-40b5-4d47-a588-ac011bc08e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"string\")\\ndef to_upper(s: pd.Series) -> pd.Series:\\n    return s.str.upper()\\n\\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\\ndf.select(to_upper(\"name\")).show()\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(to_upper(\"name\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c95c1c9-c1dd-4670-935f-e1b1f35bc33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"first string, last string\")\\ndef split_expand(s: pd.Series) -> pd.DataFrame:\\n    return s.str.split(expand=True)\\n\\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\\ndf.select(split_expand(\"name\")).show()\\n'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"first string, last string\")\n",
    "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
    "    return s.str.split(expand=True)\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(split_expand(\"name\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a594f1d3-9425-4829-9cd5-53a2bddf7a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator\\n\\n@pandas_udf(\"long\")\\ndef calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\\n    # Do some expensive initialization with a state\\n    state = very_expensive_initialization()\\n    for x in iterator:\\n        # Use that state for whole iterator.\\n        yield calculate_with_state(x, state)\\n\\ndf.select(calculate(\"value\")).show()\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Do some expensive initialization with a state\n",
    "    state = very_expensive_initialization()\n",
    "    for x in iterator:\n",
    "        # Use that state for whole iterator.\n",
    "        yield calculate_with_state(x, state)\n",
    "\n",
    "df.select(calculate(\"value\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "16c65ced-99e9-455c-8d32-65066c727a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator\\n@pandas_udf(\"long\")\\ndef plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\\n    for s in iterator:\\n        yield s + 1\\n\\ndf = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\\ndf.select(plus_one(df.v)).show()\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for s in iterator:\n",
    "        yield s + 1\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.select(plus_one(df.v)).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "95ddeba3-ea31-4402-994e-2f84b2886818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Iterator, Tuple\\nfrom pyspark.sql.functions import struct, col\\n@pandas_udf(\"long\")\\ndef multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\\n    for s1, df in iterator:\\n        yield s1 * df.v\\n\\ndf = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\\ndf.withColumn(\\'output\\', multiply(col(\"v\"), struct(col(\"v\")))).show()\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql.functions import struct, col\n",
    "@pandas_udf(\"long\")\n",
    "def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n",
    "    for s1, df in iterator:\n",
    "        yield s1 * df.v\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "daba8baa-4fbd-452e-ab77-de0a314373b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@pandas_udf(\"double\")\\ndef mean_udf(v: pd.Series) -> float:\\n    return v.mean()\\n\\ndf = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\\ndf.groupby(\"id\").agg(mean_udf(df[\\'v\\'])).show()\\n'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "550f0721-977f-45ea-b08c-506a5ca642b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import Window\\n@pandas_udf(\"double\")\\ndef mean_udf(v: pd.Series) -> float:\\n    return v.mean()\\n\\ndf = spark.createDataFrame(\\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\\nw = Window.partitionBy(\\'id\\').orderBy(\\'v\\').rowsBetween(-1, 0)\\ndf.withColumn(\\'mean_v\\', mean_udf(\"v\").over(w)).show()\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql import Window\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n",
    "df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7b2a3e76-1c6f-4fe5-a4c3-64d7f99c82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Creates a pandas user defined function (a.k.a. vectorized user defined function).\n",
      "\n",
      "    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer\n",
      "    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF\n",
      "    is defined using the `pandas_udf` as a decorator or to wrap the function, and no\n",
      "    additional configuration is required. A Pandas UDF behaves as a regular PySpark function\n",
      "    API in general.\n",
      "\n",
      "    .. versionadded:: 2.3.0\n",
      "\n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    f : function, optional\n",
      "        user-defined function. A python function if used as a standalone function\n",
      "    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n",
      "        the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    functionType : int, optional\n",
      "        an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n",
      "        Default: SCALAR. This parameter exists for compatibility.\n",
      "        Using Python type hints is encouraged.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    In order to use this API, customarily the below are imported:\n",
      "\n",
      "    >>> import pandas as pd\n",
      "    >>> from pyspark.sql.functions import pandas_udf\n",
      "\n",
      "    From Spark 3.0 with Python 3.6+, `Python type hints <https://www.python.org/dev/peps/pep-0484>`_\n",
      "    detect the function types as below:\n",
      "\n",
      "    >>> @pandas_udf(IntegerType())\n",
      "    ... def slen(s: pd.Series) -> pd.Series:\n",
      "    ...     return s.str.len()\n",
      "\n",
      "    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:\n",
      "\n",
      "    >>> from pyspark.sql.functions import PandasUDFType\n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> @pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
      "    ... def slen(s):\n",
      "    ...     return s.str.len()\n",
      "\n",
      "    It is preferred to specify type hints for the pandas UDF instead of specifying pandas UDF\n",
      "    type via `functionType` which will be deprecated in the future releases.\n",
      "\n",
      "    Note that the type hint should use `pandas.Series` in all cases but there is one variant\n",
      "    that `pandas.DataFrame` should be used for its input or output type hint instead when the input\n",
      "    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows\n",
      "    a Pandas UDF which takes long column, string column and struct column, and outputs a struct\n",
      "    column. It requires the function to specify the type hints of `pandas.Series` and\n",
      "    `pandas.DataFrame` as below:\n",
      "\n",
      "    >>> @pandas_udf(\"col1 string, col2 long\")\n",
      "    >>> def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
      "    ...     s3['col2'] = s1 + s2.str.len()\n",
      "    ...     return s3\n",
      "    ...\n",
      "    >>> # Create a Spark DataFrame that has three columns including a struct column.\n",
      "    ... df = spark.createDataFrame(\n",
      "    ...     [[1, \"a string\", (\"a nested string\",)]],\n",
      "    ...     \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
      "    >>> df.printSchema()\n",
      "    root\n",
      "    |-- long_column: long (nullable = true)\n",
      "    |-- string_column: string (nullable = true)\n",
      "    |-- struct_column: struct (nullable = true)\n",
      "    |    |-- col1: string (nullable = true)\n",
      "    >>> df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()\n",
      "    |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      "    |    |-- col1: string (nullable = true)\n",
      "    |    |-- col2: long (nullable = true)\n",
      "\n",
      "    In the following sections, it describes the combinations of the supported type hints. For\n",
      "    simplicity, `pandas.DataFrame` variant is omitted.\n",
      "\n",
      "    * Series to Series\n",
      "        `pandas.Series`, ... -> `pandas.Series`\n",
      "\n",
      "        The function takes one or more `pandas.Series` and outputs one `pandas.Series`.\n",
      "        The output of the function should always be of the same length as the input.\n",
      "\n",
      "        >>> @pandas_udf(\"string\")\n",
      "        ... def to_upper(s: pd.Series) -> pd.Series:\n",
      "        ...     return s.str.upper()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
      "        >>> df.select(to_upper(\"name\")).show()\n",
      "        +--------------+\n",
      "        |to_upper(name)|\n",
      "        +--------------+\n",
      "        |      JOHN DOE|\n",
      "        +--------------+\n",
      "\n",
      "        >>> @pandas_udf(\"first string, last string\")\n",
      "        ... def split_expand(s: pd.Series) -> pd.DataFrame:\n",
      "        ...     return s.str.split(expand=True)\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
      "        >>> df.select(split_expand(\"name\")).show()\n",
      "        +------------------+\n",
      "        |split_expand(name)|\n",
      "        +------------------+\n",
      "        |       [John, Doe]|\n",
      "        +------------------+\n",
      "\n",
      "        .. note:: The length of the input is not that of the whole input column, but is the\n",
      "            length of an internal batch used for each call to the function.\n",
      "\n",
      "    * Iterator of Series to Iterator of Series\n",
      "        `Iterator[pandas.Series]` -> `Iterator[pandas.Series]`\n",
      "\n",
      "        The function takes an iterator of `pandas.Series` and outputs an iterator of\n",
      "        `pandas.Series`. In this case, the created pandas UDF instance requires one input\n",
      "        column when this is called as a PySpark column. The length of the entire output from\n",
      "        the function should be the same length of the entire input; therefore, it can\n",
      "        prefetch the data from the input iterator as long as the lengths are the same.\n",
      "\n",
      "        It is also useful when the UDF execution\n",
      "        requires initializing some states although internally it works identically as\n",
      "        Series to Series case. The pseudocode below illustrates the example.\n",
      "\n",
      "        .. highlight:: python\n",
      "        .. code-block:: python\n",
      "\n",
      "            @pandas_udf(\"long\")\n",
      "            def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
      "                # Do some expensive initialization with a state\n",
      "                state = very_expensive_initialization()\n",
      "                for x in iterator:\n",
      "                    # Use that state for whole iterator.\n",
      "                    yield calculate_with_state(x, state)\n",
      "\n",
      "            df.select(calculate(\"value\")).show()\n",
      "\n",
      "        >>> from typing import Iterator\n",
      "        >>> @pandas_udf(\"long\")\n",
      "        ... def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
      "        ...     for s in iterator:\n",
      "        ...         yield s + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
      "        >>> df.select(plus_one(df.v)).show()\n",
      "        +-----------+\n",
      "        |plus_one(v)|\n",
      "        +-----------+\n",
      "        |          2|\n",
      "        |          3|\n",
      "        |          4|\n",
      "        +-----------+\n",
      "\n",
      "        .. note:: The length of each series is the length of a batch internally used.\n",
      "\n",
      "    * Iterator of Multiple Series to Iterator of Series\n",
      "        `Iterator[Tuple[pandas.Series, ...]]` -> `Iterator[pandas.Series]`\n",
      "\n",
      "        The function takes an iterator of a tuple of multiple `pandas.Series` and outputs an\n",
      "        iterator of `pandas.Series`. In this case, the created pandas UDF instance requires\n",
      "        input columns as many as the series when this is called as a PySpark column.\n",
      "        Otherwise, it has the same characteristics and restrictions as Iterator of Series\n",
      "        to Iterator of Series case.\n",
      "\n",
      "        >>> from typing import Iterator, Tuple\n",
      "        >>> from pyspark.sql.functions import struct, col\n",
      "        >>> @pandas_udf(\"long\")\n",
      "        ... def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n",
      "        ...     for s1, df in iterator:\n",
      "        ...         yield s1 * df.v\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
      "        >>> df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()\n",
      "        +---+------+\n",
      "        |  v|output|\n",
      "        +---+------+\n",
      "        |  1|     1|\n",
      "        |  2|     4|\n",
      "        |  3|     9|\n",
      "        +---+------+\n",
      "\n",
      "        .. note:: The length of each series is the length of a batch internally used.\n",
      "\n",
      "    * Series to Scalar\n",
      "        `pandas.Series`, ... -> `Any`\n",
      "\n",
      "        The function takes `pandas.Series` and returns a scalar value. The `returnType`\n",
      "        should be a primitive data type, and the returned scalar can be either a python primitive\n",
      "        type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64.\n",
      "        `Any` should ideally be a specific scalar type accordingly.\n",
      "\n",
      "        >>> @pandas_udf(\"double\")\n",
      "        ... def mean_udf(v: pd.Series) -> float:\n",
      "        ...     return v.mean()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
      "        >>> df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
      "        +---+-----------+\n",
      "        | id|mean_udf(v)|\n",
      "        +---+-----------+\n",
      "        |  1|        1.5|\n",
      "        |  2|        6.0|\n",
      "        +---+-----------+\n",
      "\n",
      "        This UDF can also be used as window functions as below:\n",
      "\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> @pandas_udf(\"double\")\n",
      "        ... def mean_udf(v: pd.Series) -> float:\n",
      "        ...     return v.mean()\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
      "        >>> w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n",
      "        >>> df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()\n",
      "        +---+----+------+\n",
      "        | id|   v|mean_v|\n",
      "        +---+----+------+\n",
      "        |  1| 1.0|   1.0|\n",
      "        |  1| 2.0|   1.5|\n",
      "        |  2| 3.0|   3.0|\n",
      "        |  2| 5.0|   4.0|\n",
      "        |  2|10.0|   7.5|\n",
      "        +---+----+------+\n",
      "\n",
      "        .. note:: For performance reasons, the input series to window functions are not copied.\n",
      "            Therefore, mutating the input series is not allowed and will cause incorrect results.\n",
      "            For the same reason, users should also not rely on the index of the input series.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The user-defined functions do not support conditional expressions or short circuiting\n",
      "    in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "    can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "\n",
      "    The user-defined functions do not take keyword arguments on the calling side.\n",
      "\n",
      "    The data type of returned `pandas.Series` from the user-defined functions should be\n",
      "    matched with defined `returnType` (see :meth:`types.to_arrow_type` and\n",
      "    :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do\n",
      "    conversion on returned data. The conversion is not guaranteed to be correct and results\n",
      "    should be checked for accuracy by users.\n",
      "\n",
      "    Currently,\n",
      "    :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and\n",
      "    nested :class:`pyspark.sql.types.StructType`\n",
      "    are currently not supported as output types.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    pyspark.sql.GroupedData.agg\n",
      "    pyspark.sql.DataFrame.mapInPandas\n",
      "    pyspark.sql.GroupedData.applyInPandas\n",
      "    pyspark.sql.PandasCogroupedOps.applyInPandas\n",
      "    pyspark.sql.UDFRegistration.register\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pandas_udf.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ceb27-7e72-41eb-96a6-78ae62d03478",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.udf(f: Union[Callable[[…], Any], DataTypeOrString, None] = None, returnType: DataTypeOrString = StringType(), *, useArrow: Optional[bool] = None) → Union[UserDefinedFunctionLike, Callable[[Callable[[…], Any]], UserDefinedFunctionLike]]\n",
    "Creates a user defined function (UDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d38e8d-2d77-4c71-9fa4-eb96568a49b4",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- f: function |\n",
    "python function if used as a standalone function\n",
    "- return: Typepyspark.sql.types.DataType or str | \n",
    "the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "- use: Arrowbool or None | \n",
    "whether to use Arrow to optimize the (de)serialization. When it is None, the Spark config “spark.sql.execution.pythonUDF.arrow.enabled” takes effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3bb37122-2bb1-44ee-882b-8d2bb4e436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4d721c43-118b-43aa-b656-c2ffb218319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|Random int|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        72|           72|           74|         2|\n",
      "|female|       group C|               some college|    standard|              completed|        69|           90|           88|        69|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|        38|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        47|           57|           44|         3|\n",
      "|  male|       group C|               some college|    standard|                   none|        76|           78|           75|        21|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.limit(5).withColumn(\"Random int\", random_udf()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8bc7cf27-73c6-453d-9c92-9d6381a3591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------+\n",
      "|slen(name)|to_upper(name)|add_one(age)|\n",
      "+----------+--------------+------------+\n",
      "|         8|      JOHN DOE|          22|\n",
      "+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "@udf\n",
    "def to_upper(s):\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def add_one(x):\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b2099-4ad6-46bd-a9b4-5b7dedc25aae",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.udtf(cls: Optional[Type] = None, *, returnType: Union[pyspark.sql.types.StructType, str], useArrow: Optional[bool] = None) → Union[pyspark.sql.udtf.UserDefinedTableFunction, Callable[[Type], pyspark.sql.udtf.UserDefinedTableFunction]]\n",
    "Creates a user defined table function (UDTF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4330c0-b9bc-4461-8d12-0f4df5975b86",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- cls: class | \n",
    "the Python user-defined table function handler class.\n",
    "- return: Typepyspark.sql.types.StructType or str | \n",
    "the return type of the user-defined table function. The value can be either a pyspark.sql.types.StructType object or a DDL-formatted struct type string.\n",
    "- use: Arrowbool or None, optional | \n",
    "whether to use Arrow to optimize the (de)serializations. When it’s set to None, the Spark config “spark.sql.execution.pythonUDTF.arrow.enabled” is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c84969ec-d27b-4a36-8a46-435de8054211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlusOne:\n",
    "    def eval(self, a: int):\n",
    "        yield a + 1,\n",
    "plus_one = udtf(PlusOne, returnType=\"r: int\").asDeterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "23cf4934-04c4-4883-9195-eb33184096ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   c1|   c2|\n",
      "+-----+-----+\n",
      "|hello|world|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TestUDTF:\n",
    "    def eval(self, *args: Any):\n",
    "        yield \"hello\", \"world\"\n",
    "\n",
    "test_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")\n",
    "test_udtf().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b0668c5-9015-49ca-bd94-21c5635ca7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udtf(returnType=\"c1: int, c2: int\")\n",
    "class PlusOne:\n",
    "    def eval(self, x: int):\n",
    "        yield x, x + 1\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "PlusOne(lit(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "506c7837-9869-493f-92d7-8c5cbabef662",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udtf(returnType=\"c1: int, c2: int\", useArrow=True)\n",
    "class ArrowPlusOne:\n",
    "    def eval(self, x: int):\n",
    "        yield x, x + 1\n",
    "\n",
    "# eval_type = read_int(infile) Error\n",
    "# ArrowPlusOne(lit(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2c821-b4da-46cc-827d-fa32db4c8c1b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.unwrap_udt(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Unwrap UDT data type column into its underlying type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6707c52b-4c96-4db3-b82c-ddf1d41c6266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class MyUDT:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "data = [(MyUDT(42),)]\n",
    "schema = StructType([StructField(\"my_udt\", MyUDTType(), True)])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef56e7-b773-488c-89c3-9fd13c900d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "result_df = df.select(unwrap_udt(col(\"my_udt\")).alias(\"my_udt_value\"))\n",
    "result_df.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d00dc8f0-80f2-4ede-9805-176bb4762295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import UserDefinedType\n",
    "\n",
    "# Визначаємо користувацький тип даних MyUDT\n",
    "class MyUDT(UserDefinedType):\n",
    "\n",
    "    def __init__(self, value = None):\n",
    "        self.value = value\n",
    "        \n",
    "    def simpleString(self):\n",
    "        return \"MyUDT\"\n",
    "\n",
    "    def serialize(self, obj):\n",
    "        return str(obj).encode('utf-8')\n",
    "\n",
    "    def deserialize(self, datum):\n",
    "        return MyUDT(int(datum.decode('utf-8')))\n",
    "\n",
    "# Оголошуємо функцію unwrap_udt з використанням користувацького типу MyUDT\n",
    "@pandas_udf(MyUDT())\n",
    "def unwrap_udt(s: pd.Series) -> pd.Series:\n",
    "    return s\n",
    "\n",
    "# Створюємо DataFrame\n",
    "data = [(MyUDT(42),), (MyUDT(56),)]\n",
    "df = spark.createDataFrame(data, [\"my_udt\"])\n",
    "\n",
    "# Використовуємо unwrap_udt для розгортання користувацького типу MyUDT\n",
    "result_df = df.select(unwrap_udt(col(\"my_udt\")).alias(\"my_udt_value\"))\n",
    "\n",
    "# Виводимо результат\n",
    "result_df.show()\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617aa81-b5a3-4753-9eac-434351cd8b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Misc Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#misc-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb1036-6b3b-4e51-91ef-a62de46f12de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.aes_decrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9b7b-7225-46de-9397-b3795a0ff7b4",
   "metadata": {},
   "source": [
    "##### The Advanced Encryption Standard (AES) \n",
    "Returns a decrypted value of input using AES in mode with padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM.\n",
    "\n",
    "Parameters:\n",
    "- input: Column or str | The binary value to decrypt.\n",
    "- key: Column or str | The passphrase to use to decrypt the data.\n",
    "- mode: Column or str, optional | Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- aad: Column or str, optional | Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77a9d16-75fa-4f60-997d-4f6b136382e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3926a9-5cd2-47f9-a1c4-1155f04ddbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b226d44-c853-4de4-876b-5a7669a4a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfaa1d44-2239-41dd-b4de-85415090fe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    \"0000111122223333\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdaef3-1a32-479f-8ad4-2af582a99279",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.aes_encrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, iv: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf223f-ecc4-44af-be4d-d4bed5166f03",
   "metadata": {},
   "source": [
    "##### The Advanced Encryption Standard (AES) \n",
    "Returns an encrypted value of input using AES in given mode with the specified padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional initialization vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12 bytes for GCM. If not provided, a random vector will be generated and prepended to the output. Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM.\n",
    "\n",
    "Parameters: \n",
    "- input: Column or str | The binary value to encrypt.\n",
    "- key: Column or str | The passphrase to use to encrypt the data.\n",
    "- mode: Column or str, optional | Specifies which block cipher mode should be used to encrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- iv: Column or str, optional | Optional initialization vector. Only supported for CBC and GCM modes. Valid values: None or “”. 16-byte array for CBC mode. 12-byte array for GCM mode.\n",
    "- aad: Column or str, optional | Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c19dcbd-34d5-49c8-a5e7-5dbfd9e6fe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n",
    ")\n",
    "df.select(base64(aes_encrypt(\n",
    "    df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")), df.aad)\n",
    ").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d315bcca-5685-4524-ac60-3a0f7a357ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='AAAAAAAAAAAAAAAAQiYi+sRNYDAOTjdSEcYBFsAWPL1f')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(base64(aes_encrypt(\n",
    "    df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")))\n",
    ").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f29e7c10-72c2-4c82-bba6-d51a98f22163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark SQL'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"1234567890abcdef\", \"ECB\", \"PKCS\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode, df.padding),\n",
    "    df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea2a481-18c2-461b-80a1-f5886abb6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark SQL'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"0000111122223333\", \"ECB\",)],\n",
    "    [\"input\", \"key\", \"mode\"]\n",
    ")\n",
    "df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode),\n",
    "    df.key, df.mode).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36dd94c0-8591-46e8-8219-9ade838cf4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spark SQL')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", \"abcdefghijklmnop\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(\n",
    "    unbase64(base64(aes_encrypt(df.input, df.key))), df.key\n",
    ").cast(\"STRING\").alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bfe752-457b-4de8-80ab-5f48c0ed641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_1 = \"0000111122223333\"\n",
    "key_2 = \"abcdefghijklmnop\"\n",
    "\n",
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    key_1,)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f36debe1-e01f-44e4-83f6-62ca8a7e2f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='!\\x196�:���\\x7f�4�]��hT�5\\x7fshm<\\x0b4wO\\x06�\\r��\\x10�1')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"Spark SQL\", key_2,)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "\n",
    "df_key_2_en = df.select(aes_encrypt(df.input, df.key).cast(\"STRING\").alias('r'))\n",
    "df_key_2_en.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b601bc8c-bf96-4bfd-b30c-c2d818da792c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='Spark SQL')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_key_2_de = df_key_2_en.select(aes_decrypt(\"r\", lit(key_2)).cast(\"STRING\").alias('r'))\n",
    "df_key_2_de.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fddb37-ec70-4cb9-ba05-19ca95c87188",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_bit_position(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the bit position for the given input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822427e2-76d7-4504-99a8-c34f57d02915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=122)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(123,)], [\"a\"])\n",
    "df.select(bitmap_bit_position(df.a).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9ede2b-25f9-42b0-a026-54cd2532e3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Value='71', bitmap_bit_position(Value)=70),\n",
       " Row(Value='77', bitmap_bit_position(Value)=76),\n",
       " Row(Value='83', bitmap_bit_position(Value)=82),\n",
       " Row(Value='54', bitmap_bit_position(Value)=53),\n",
       " Row(Value='100', bitmap_bit_position(Value)=99),\n",
       " Row(Value='63', bitmap_bit_position(Value)=62),\n",
       " Row(Value='88', bitmap_bit_position(Value)=87),\n",
       " Row(Value='64', bitmap_bit_position(Value)=63),\n",
       " Row(Value='64', bitmap_bit_position(Value)=63),\n",
       " Row(Value='92', bitmap_bit_position(Value)=91),\n",
       " Row(Value='75', bitmap_bit_position(Value)=74),\n",
       " Row(Value='54', bitmap_bit_position(Value)=53),\n",
       " Row(Value='76', bitmap_bit_position(Value)=75),\n",
       " Row(Value='94', bitmap_bit_position(Value)=93),\n",
       " Row(Value='82', bitmap_bit_position(Value)=81),\n",
       " Row(Value='85', bitmap_bit_position(Value)=84),\n",
       " Row(Value='79', bitmap_bit_position(Value)=78),\n",
       " Row(Value='89', bitmap_bit_position(Value)=88),\n",
       " Row(Value='88', bitmap_bit_position(Value)=87),\n",
       " Row(Value='91', bitmap_bit_position(Value)=90)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(\"Value\", bitmap_bit_position(\"Value\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "323045af-6af9-49c9-a27c-ca16ed024644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bitmap_bit_position(Value)=70),\n",
       " Row(bitmap_bit_position(Value)=76),\n",
       " Row(bitmap_bit_position(Value)=82),\n",
       " Row(bitmap_bit_position(Value)=53),\n",
       " Row(bitmap_bit_position(Value)=99),\n",
       " Row(bitmap_bit_position(Value)=62),\n",
       " Row(bitmap_bit_position(Value)=87),\n",
       " Row(bitmap_bit_position(Value)=63),\n",
       " Row(bitmap_bit_position(Value)=63),\n",
       " Row(bitmap_bit_position(Value)=91),\n",
       " Row(bitmap_bit_position(Value)=74),\n",
       " Row(bitmap_bit_position(Value)=53),\n",
       " Row(bitmap_bit_position(Value)=75),\n",
       " Row(bitmap_bit_position(Value)=93),\n",
       " Row(bitmap_bit_position(Value)=81),\n",
       " Row(bitmap_bit_position(Value)=84),\n",
       " Row(bitmap_bit_position(Value)=78),\n",
       " Row(bitmap_bit_position(Value)=88),\n",
       " Row(bitmap_bit_position(Value)=87),\n",
       " Row(bitmap_bit_position(Value)=90)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(bitmap_bit_position(tour.Value)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063fa2-fe3c-4f81-829d-f8a0a9d0fe47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_bucket_number(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the bucket number for the given input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f67e02f-8a98-4f67-8e65-c95916d0149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(123,)], [\"a\"])\n",
    "df.select(bitmap_bucket_number(df.a).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac5e5e48-3453-4986-8676-72dd4b82176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Value='71', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='77', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='83', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='54', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='100', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='63', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='88', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='64', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='64', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='92', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='75', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='54', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='76', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='94', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='82', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='85', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='79', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='89', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='88', bitmap_bucket_number(Value)=1),\n",
       " Row(Value='91', bitmap_bucket_number(Value)=1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tour.limit(20).select(\"Value\", bitmap_bucket_number(\"Value\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19960d-cfcb-46ae-be80-a2a10c9f6491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_construct_agg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a bitmap with the positions of the bits set from all the values from the input column. The input column will most likely be bitmap_bit_position()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4e93cd-e6be-450f-b5d6-f73a18f3652e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='070000')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,),(2,),(3,)], [\"a\"])\n",
    "df.select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(df.a))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e077dbf4-8d7d-40fe-9211-7a71a0bc7115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(math score=72, bitmap_bit_position(math score)=71),\n",
       " Row(math score=69, bitmap_bit_position(math score)=68),\n",
       " Row(math score=90, bitmap_bit_position(math score)=89),\n",
       " Row(math score=47, bitmap_bit_position(math score)=46),\n",
       " Row(math score=76, bitmap_bit_position(math score)=75),\n",
       " Row(math score=71, bitmap_bit_position(math score)=70),\n",
       " Row(math score=88, bitmap_bit_position(math score)=87),\n",
       " Row(math score=40, bitmap_bit_position(math score)=39),\n",
       " Row(math score=64, bitmap_bit_position(math score)=63),\n",
       " Row(math score=38, bitmap_bit_position(math score)=37)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(\"math score\", bitmap_bit_position(\"math score\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df7ab23-8541-46b8-a7ad-26068fb62807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='8100E6')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b0f6813-2667-46ad-bacb-7122fd162d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='8100E6BEFFFFFFFFFFFFFFFF0F000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    ").alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "589441f9-00e0-4eb1-a787-3e96db54964d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'\\x81\\x00\\xe6\\xbe\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.select(bitmap_construct_agg(bitmap_bit_position(\"math score\")).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4463abf8-c1dd-4086-bd02-9b30a928dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='000002')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(20).select(substring(hex(\n",
    "    bitmap_construct_agg(bitmap_bit_position(\"math score\"))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71f02cb0-c84f-4c43-a36b-3bf9d4df6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='00000200A0602282D128800200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(20).select(hex(bitmap_construct_agg(bitmap_bit_position(\"math score\"))).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa84701-bbb6-499b-841a-ade1bbd8ac2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_count(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the number of set bits in the input bitmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a8ae900-4b0b-4eb1-84c2-6ae03a3cecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=16)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"FFFF\",)], [\"a\"])\n",
    "df.select(bitmap_count(to_binary(df.a, lit(\"hex\"))).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "301f8af2-f324-48fc-818b-2b8eeb00c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=4),\n",
       " Row(r=4),\n",
       " Row(r=2),\n",
       " Row(r=4),\n",
       " Row(r=5),\n",
       " Row(r=4),\n",
       " Row(r=2),\n",
       " Row(r=1),\n",
       " Row(r=3),\n",
       " Row(r=3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(bitmap_count(to_binary(\"math score\", lit(\"hex\"))).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0ac3-7e0d-4976-a801-7f73041b4991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0ac126-8798-499b-a5e1-0134d4a0c19a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.bitmap_or_agg(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a bitmap that is the bitwise OR of all of the bitmaps from the input column. The input column should be bitmaps created from bitmap_construct_agg()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2cd91d2-cadf-4336-9def-7e539fce52f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='700000')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"10\",),(\"20\",),(\"40\",)], [\"a\"])\n",
    "df.select(substring(hex(\n",
    "    bitmap_or_agg(to_binary(df.a, lit(\"hex\")))\n",
    "), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48348164-25f6-4c50-afb0-d66170ea6a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='FF0000')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(10).select(substring(hex(\n",
    "    bitmap_or_agg(to_binary(\"math score\", lit(\"hex\")))), 0, 6).alias(\"r\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d7cb2-e512-4ab1-ae65-6b1c24f5e9a8",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_catalog() → pyspark.sql.column.Column\n",
    "Returns the current catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d6ceb37-258b-4e2c-aa90-45971c8fbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|current_catalog()|\n",
      "+-----------------+\n",
      "|    spark_catalog|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_catalog()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc1199-b03d-4225-b43e-3de36bf35475",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_database() → pyspark.sql.column.Column[source]\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecae5548-5809-4eba-977a-c3ba5c13ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_database()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b481d65-dfb6-43b2-baeb-398f6add5c16",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_schema() → pyspark.sql.column.Column\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3641707d-b644-4be5-b52e-a10ca06936ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_schema()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646a071-cf2d-4ba2-8d9a-c0f4b513abba",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.current_user() → pyspark.sql.column.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d771bbd2-63fc-4ae1-af52-30fee4f24749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_user()|\n",
      "+--------------+\n",
      "|    zsavchenko|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(current_user()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a8814-72fd-446c-a233-64761813a90f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.input_file_block_length() → pyspark.sql.column.Column\n",
    "Returns the length of the block being read, or -1 if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c38f3e97-7cee-46fe-92f1-53df6819be4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=72036)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"students.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_length().alias('r')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bd12d6e-aa27-4df8-989b-407f0d3b9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=21305899)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"pga_tour_historical.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_length().alias('r')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02505ea-06b5-452a-b1a0-203568b1cbf6",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.input_file_block_start() → pyspark.sql.column.Column\n",
    "Returns the start offset of the block being read, or -1 if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bd38b62-e59f-4705-95e6-e08da12a7081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"students.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_start().alias('r')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bff80f5-564b-4d54-980c-f19a5d4b6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(r=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(path+\"pga_tour_historical.csv\", lineSep=\",\")\n",
    "df.select(input_file_block_start().alias('r')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ba148-cb39-41dd-b31c-787d219fed4c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.md5(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the MD5 digest and returns the value as a 32 character hex string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f16dc0ac-44e4-4ecf-a9ed-4447a1601c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe9baa-7c1e-470a-8bdb-4c44b3dbb468",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a sha1 hash value as a hex string of the col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdac43de-f220-466c-9dd5-160457fe19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|sha(Spark)                              |\n",
      "+----------------------------------------+\n",
      "|85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(sha(lit(\"Spark\"))).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8c840-46d6-4515-896b-f023a34a0a9e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha1(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the hex string result of SHA-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dfcedb7-ca9a-4c21-8a8b-0d38cecbd209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b5425-b887-473b-bc2b-313a307b94fe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.sha2(col: ColumnOrName, numBits: int) → pyspark.sql.column.Column\n",
    "Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The numBits indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997fd50-64de-4a37-9bc4-7cb1f19f3831",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- col: Column or str | target column to compute on.\n",
    "- num: Bitsint | the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
    "\n",
    "Returns – Column | the column for computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd43cd5d-0897-44e9-8c54-0b4caecb092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------+\n",
      "|name |sha2                                                            |\n",
      "+-----+----------------------------------------------------------------+\n",
      "|Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "|Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n",
      "+-----+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
    "df.withColumn(\"sha2\", sha2(df.name, 256)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079161c8-3978-4058-9e4b-ca8130fbdd27",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.crc32(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c530254b-b0bb-4218-b35b-5fb70a1d1e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(crc32=2743272264)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c38a2c-cbe1-4b86-af49-f742455504d9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hash(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the hash code of given columns, and returns the result as an int column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b22d5c4-3ee6-4a45-ab60-ea6a87a83052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      hash|\n",
      "+----------+\n",
      "|-757602832|\n",
      "+----------+\n",
      "\n",
      "+---------+\n",
      "|     hash|\n",
      "+---------+\n",
      "|599895104|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
    "\n",
    "df.select(hash('c1').alias('hash')).show()\n",
    "\n",
    "df.select(hash('c1', 'c2').alias('hash')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972efaf-ca1b-4617-a99e-f5f7adbd90e9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xxhash64(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm, and returns the result as a long column. The hash computation uses an initial seed of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49bc5bf7-c55f-415f-967d-4901a89f2efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               hash|\n",
      "+-------------------+\n",
      "|4105715581806190027|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|               hash|\n",
      "+-------------------+\n",
      "|3233247871021311208|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
    "\n",
    "df.select(xxhash64('c1').alias('hash')).show()\n",
    "\n",
    "df.select(xxhash64('c1', 'c2').alias('hash')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351565-e265-4924-b2d3-efde2e3dd939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.assert_true(col: ColumnOrName, errMsg: Union[pyspark.sql.column.Column, str, None] = None) → pyspark.sql.column.Column\n",
    "Returns null if the input column is true; throws an exception with the provided error message otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fae58-b408-4eb3-9c48-357e15147c7d",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- col: Column or str | column name or column that represents the input column to test.\n",
    "- errMsg: Column or str, optional | A Python string literal or column containing the error message.\n",
    "\n",
    "Returns – Column | null if the input column is true otherwise throws an error with specified message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97d50672-006c-4011-8b5b-3f6a8fa7b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
    "df.select(assert_true(df.a < df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c65d7b2-47c9-49ac-858a-b1a192923e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d92fd079-da85-4485-91ec-b77649798f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=None)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68c680f6-663a-4fe4-84a9-a8df5e9f4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:38 ERROR Executor: Exception in task 11.0 in stage 91.0 (TID 496)\n",
      "java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "23/10/22 15:48:38 WARN TaskSetManager: Lost task 11.0 in stage 91.0 (TID 496) (192.168.0.179 executor driver): java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "23/10/22 15:48:38 ERROR TaskSetManager: Task 11 in stage 91.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    "    df.select(assert_true(df.a > df.b, 'My error msg').alias('r')).collect()\n",
    "except Py4JJavaError as e:\n",
    "    print(\"My error msg\" in e.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd7a50-acf7-44b1-a9e3-1b497912fdf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.raise_error(errMsg: Union[pyspark.sql.column.Column, str]) → pyspark.sql.column.Column\n",
    "Throws an exception with the provided error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba93d4d6-8f15-48dd-8205-d008e069f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 15:48:38 ERROR Executor: Exception in task 11.0 in stage 92.0 (TID 508)\n",
      "java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "23/10/22 15:48:38 WARN TaskSetManager: Lost task 11.0 in stage 92.0 (TID 508) (192.168.0.179 executor driver): java.lang.RuntimeException: My error msg\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache."
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "df = spark.range(1)\n",
    "\n",
    "try:\n",
    "    df.select(raise_error(\"My error msg\")).show()\n",
    "except Py4JJavaError as e:\n",
    "    print(\"My error msg\" in e.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccbf72-dbc5-4a18-a90f-15b7da9be924",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.reflect(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calls a method with reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f8957-3846-4ecb-9813-018ba36aa626",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal string for the class name, and the second element should be a literal string for the method name, and the remaining are input arguments to the Java method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e347dba-f974-4fea-b06a-35b3c14cb5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "23/10/22 15:48:38 ERROR TaskSetManager: Task 11 in stage 92.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r='a5cf6c42-0c85-418f-af6c-3e4e5b1328f2')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\",)], [\"a\"])\n",
    "df.select(\n",
    "    reflect(lit(\"java.util.UUID\"), lit(\"fromString\"), df.a).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f1dd2-a71d-4c54-96a7-22fb509b1ef9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_sketch_estimate(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the estimated number of unique values given the binary representation of a Datasketches HllSketch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5efee-0d02-4235-9e5a-415b02858b1f",
   "metadata": {},
   "source": [
    "[why] why not use hll_sketch_estimate all time?\n",
    "\n",
    "Datasketches HyperLogLog (HLL) Sketch - це структура даних, призначена для наближеного підрахунку унікальних елементів у великих наборах даних. Вона використовує алгоритм HyperLogLog для оцінки кількості різних елементів у масиві, не зберігаючи всі ці елементи, а лише важливі характеристики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58d374b9-0bc2-4ac4-bd16-a69c14b0887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
    "df = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b759d3d-d30b-4674-bba1-195dc52a9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|      179970|\n",
      "+------------+\n",
      "\n",
      "execution_time - 1.1872360706329346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "tour.agg(hll_sketch_estimate(hll_sketch_agg(\"Value\")).alias(\"distinct_cnt\")).show()\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56daead7-e800-4eb2-9822-f5b3db88cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:====>                                                  (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180981\n",
      "execution_time - 1.5197079181671143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(tour.select(\"Value\").distinct().count())\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c53d04d2-4ccb-4b29-814c-865f0213bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|          81|\n",
      "+------------+\n",
      "\n",
      "execution_time - 0.07657694816589355\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "students.agg(hll_sketch_estimate(hll_sketch_agg(\"math score\")).alias(\"distinct_cnt\")).show()\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "305e2e78-a03d-49af-a39b-b6ebc870d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "execution_time - 0.23991703987121582\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(students.select(\"math score\").distinct().count())\n",
    "print(f\"execution_time - {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b61d03-1d9d-4d8b-a2f7-a2565c756506",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.hll_union(col1: ColumnOrName, col2: ColumnOrName, allowDifferentLgConfigK: Optional[bool] = None) → pyspark.sql.column.Column\n",
    "Merges two binary representations of Datasketches HllSketch objects, using a Datasketches Union object. Throws an exception if sketches have different lgConfigK values and allowDifferentLgConfigK is unset or set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0934c466-1436-4dd8-9f6a-34b0adf08110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|             sketch1|             sketch2|distinct_cnt|\n",
      "+--------------------+--------------------+------------+\n",
      "|[02 01 07 0C 03 0...|[02 01 07 0C 03 0...|           6|\n",
      "+--------------------+--------------------+------------+\n",
      "\n",
      "+------------+\n",
      "|distinct_cnt|\n",
      "+------------+\n",
      "|           6|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,4),(2,5),(2,5),(3,6)], \"struct<v1:int,v2:int>\")\n",
    "df = df.agg(hll_sketch_agg(\"v1\").alias(\"sketch1\"), hll_sketch_agg(\"v2\").alias(\"sketch2\"))\n",
    "df = df.withColumn(\"distinct_cnt\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
    "df.show()\n",
    "df.drop(\"sketch1\", \"sketch2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33b2f1a6-16e3-4acd-ad64-89a48ca42089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|             sketch1|             sketch2|             sketch3|distinct_cnt1|distinct_cnt2|distinct_cnt3|distinct_cnt4|\n",
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|[03 01 07 0C 07 0...|[03 01 07 0C 07 0...|[03 01 07 0C 07 0...|           83|           83|           82|           82|\n",
      "+--------------------+--------------------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|distinct_cnt1|distinct_cnt2|distinct_cnt3|distinct_cnt4|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|           83|           83|           82|           82|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = students.agg(hll_sketch_agg(\"math score\").alias(\"sketch1\"),\n",
    "                  hll_sketch_agg(\"reading score\").alias(\"sketch2\"),\n",
    "                  hll_sketch_agg(\"writing score\").alias(\"sketch3\"),)\n",
    "df = (df.withColumn(\"distinct_cnt1\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
    "        .withColumn(\"distinct_cnt2\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch3\")))\n",
    "        .withColumn(\"distinct_cnt3\", hll_sketch_estimate(hll_union(\"sketch2\", \"sketch3\")))\n",
    "        .withColumn(\"distinct_cnt4\", hll_sketch_estimate(hll_union(\"sketch3\", \"sketch2\")))\n",
    ")\n",
    "df.show()\n",
    "df.drop(\"sketch1\", \"sketch2\", \"sketch3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d19ba9-bd01-4661-b55a-90ddbeb8a98b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.java_method(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Calls a method with reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63158e-79a2-4425-b624-4084bbce4a6a",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal string for the class name, and the second element should be a literal string for the method name, and the remaining are input arguments to the Java method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "504a9681-8976-4008-99a7-6a144a35ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|\n",
      "+-----------------------------------------------------------------------------+\n",
      "|a5cf6c42-0c85-418f-af6c-3e4e5b1328f2                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(\n",
    "    java_method(\n",
    "        lit(\"java.util.UUID\"),\n",
    "        lit(\"fromString\"),\n",
    "        lit(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\")\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604ea34-8910-407a-bdaf-f8d6f75adf16",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.stack(*cols: ColumnOrName) → pyspark.sql.column.Column\n",
    "Separates col1, …, colk into n rows. Uses column names col0, col1, etc. by default unless specified otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a1d98-7500-4d70-b0b5-71578535fc32",
   "metadata": {},
   "source": [
    "Parameters – cols: Column or str | the first element should be a literal int for the number of rows to be separated, and the remaining are input elements to be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f2b7dde-08b1-43fa-a9dc-0b9e979c36fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3)], [\"a\", \"b\", \"c\"])\n",
    "df.select(stack(lit(2), df.a, df.b, df.c)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebc7128a-f4f7-4ee6-b6ca-f6e81bdb7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3, 4), (1, 2, 3, 4)], [\"a\", \"b\", \"c\", \"d\"])\n",
    "df.select(stack(lit(2), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7772cdaf-2572-4f2a-ba0b-f271d7231dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|NULL|NULL|\n",
      "|1   |2   |\n",
      "|3   |4   |\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(3), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69751707-9a89-43f7-bae6-03672e8d638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|1   |2   |3   |4   |\n",
      "|1   |2   |3   |4   |\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(1), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce06de0f-d2fd-4597-a230-1804c2c0bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col0|\n",
      "+----+\n",
      "|1   |\n",
      "|2   |\n",
      "|3   |\n",
      "|4   |\n",
      "|1   |\n",
      "|2   |\n",
      "|3   |\n",
      "|4   |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stack(lit(4), df.a, df.b, df.c, df.d)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397ae1b-3bb7-4ffb-8e84-2e7636f66e93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pyspark.sql.functions.try_aes_decrypt(input: ColumnOrName, key: ColumnOrName, mode: Optional[ColumnOrName] = None, padding: Optional[ColumnOrName] = None, aad: Optional[ColumnOrName] = None) → pyspark.sql.column.Column\n",
    "This is a special version of aes_decrypt that performs the same operation, but returns a NULL value instead of raising an error if the decryption cannot be performed. Returns a decrypted value of input using AES in mode with padding. Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are (‘ECB’, ‘PKCS’), (‘GCM’, ‘NONE’) and (‘CBC’, ‘PKCS’). Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption. The default mode is GCM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70324b9-c4fc-489c-be28-6c14a347820a",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- input: Column or str | \n",
    "The binary value to decrypt.\n",
    "- key: Column or str | \n",
    "The passphrase to use to decrypt the data.\n",
    "- mode: Column or str, optional |\n",
    "Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB, GCM, CBC.\n",
    "- padding: Column or str, optional | \n",
    "Specifies how to pad messages whose length is not a multiple of the block size. Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.\n",
    "- aad: Column or str, optional | \n",
    "Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and must be provided for both encryption and decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6e0fc44-b5ba-4367-964f-c60b718e84d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
    "    \"This is an AAD mixed into the input\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcfb0a80-42dc-4b4d-9289-c988e71afbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
    "    \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
    "    [\"input\", \"key\", \"mode\", \"padding\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(\n",
    "    unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6201239a-1135-44ba-af6e-bc26ca4c605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(try_aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14e54b59-6b29-4969-8e05-1e80fed28fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=bytearray(b'Spark'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\n",
    "    \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
    "    \"0000111122223333\",)],\n",
    "    [\"input\", \"key\"]\n",
    ")\n",
    "df.select(try_aes_decrypt(unhex(df.input), df.key).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d975d6-8971-4bcd-8439-290ff340eccd",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.typeof(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Return DDL-formatted type string for the data type of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "069b7f67-6c96-4969-97d7-501967481d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='bigint')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,)], [\"a\"])\n",
    "df.select(typeof(df.a).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8bb48-8bd1-4694-b8e3-a3e13a1a3da7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.user() → pyspark.sql.column.Column\n",
    "Returns the current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "479c39e6-573f-44a5-ace7-a48f2fc11eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_user()|\n",
      "+--------------+\n",
      "|    zsavchenko|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(user()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042fdb3-ba69-49b3-b2ae-84203e47bde9",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.version() → pyspark.sql.column.Column\n",
    "Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26ccb10a-7c74-456a-b3f0-788cb2b2e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|version()                                     |\n",
      "+----------------------------------------------+\n",
      "|3.5.0 ce5ddad990373636e94071e7cef2f31021add07b|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.select(version()).show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4507ea8-f50f-41dd-88eb-051a2114ec55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Predicate Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#predicate-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c361b-47cd-4051-b8ac-9e4a9c5aa2e9",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* col1: Column or str\n",
    "* col2: Column or str\n",
    "* col3: Column or str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a923363-f56f-45b8-ad65-ebd5a8df13eb",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.equal_null(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns same result as the EQUAL(=) operator for non-null operands, but returns true if both are null, false if one of the them is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "798b4cd6-59fe-4420-8a98-d2f742dbbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "|   1|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=True), Row(r=False), Row(r=True)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, None,), (1, 9,), (1, 1,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(equal_null(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e79d7-9bee-4163-806f-766c799854f1",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.ifnull(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is null, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79988c06-9cf8-4fb6-a20b-0569862aa42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   e|  e2|\n",
      "+----+----+\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "+----+----+\n",
      "\n",
      "+-------------+\n",
      "|ifnull(e, e2)|\n",
      "+-------------+\n",
      "|           12|\n",
      "|            1|\n",
      "|         NULL|\n",
      "|            1|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 12,), (1, None,), (None, None,), (1, 9,),], [\"e\", \"e2\"])\n",
    "df.show()\n",
    "df.select(ifnull(df.e, df.e2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10964946-795e-418d-913c-dac47d801a83",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.isnotnull(col: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns true if col is not null, or false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d1bdcc0-c87c-40cf-9be9-d9bc02af6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   e|\n",
      "+----+\n",
      "|NULL|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=False), Row(r=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
    "df.show()\n",
    "df.select(isnotnull(df.e).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080dd58-ae28-4198-9c16-777537be2a00",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nullif(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns null if col1 equals to col2, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed67a71d-cd09-4bdd-98c1-e68dfc71b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "|   1|   1|\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=None), Row(r=1), Row(r=None), Row(r=None), Row(r=1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, None,), (1, 9,), (1, 1,), (None, 12,), (1, None,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(nullif(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58b879-542e-43e7-a090-73e03059774b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nvl(col1: ColumnOrName, col2: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is null, or col1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4c1308e-d9d9-4584-9370-518dbef38218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|  12|\n",
      "|   1|NULL|\n",
      "|NULL|NULL|\n",
      "|   1|   9|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(r=12), Row(r=1), Row(r=None), Row(r=1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 12,), (1, None,), (None, None,), (1, 9,),], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.select(nvl(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb176098-667f-4240-964e-cd9bd3418fbe",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.nvl2(col1: ColumnOrName, col2: ColumnOrName, col3: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns col2 if col1 is not null, or col3 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b80f0a4-aab5-48f3-a979-e7f27165040d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=6), Row(r=8)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, 8, 6,), (1, 8, 9,)], [\"a\", \"b\", \"c\"])\n",
    "df.select(nvl2(df.a, df.b, df.c).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bac89-15c5-4079-8210-67b136eb28df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Xml Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#xml-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18352f41-a717-4190-a59f-fe1ff5679e3f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a string array of values within the nodes of xml that match the XPath expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "451c17a2-2461-4d22-a41e-d139d85cb9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=['b1', 'b2', 'b3'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>',)], ['x'])\n",
    "df.select(xpath(df.x, lit('a/b/text()')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdbd1d-b27b-434c-98d1-6c7a72a5464f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_boolean(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column¶\n",
    "Returns true if the XPath expression evaluates to true, or if a matching node is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ff0ad24-20b5-4b76-9707-6c2e53db3ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b></a>',)], ['x'])\n",
    "df.select(xpath_boolean(df.x, lit('a/b')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc0eaf-0b45-4db4-ab61-d25d2c83f79f",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_double(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05760e61-c813-4698-ac9d-cb81a8b51889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_double(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a000a4-f517-4e2c-96d9-b2e0c9742dc0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_int(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ed11832-bdfa-40a4-9b35-89812fda2c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_int(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2dae5-2286-42dc-b000-89c0042614ca",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_long(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ccb0452-110f-4607-9baa-9387fa6ac07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_long(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35877bbb-436b-490c-b6f0-83abfe0400b3",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_number(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "041c23c1-63f3-48f4-be41-13e99490554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|xpath_number(x, sum(a/b))|\n",
      "+-------------------------+\n",
      "|                      3.0|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x']).select(xpath_number('x', lit('sum(a/b)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddedb0b5-d2d9-4fb3-a182-f7a6a789cce5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_short(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24a2203c-95dd-4aa2-9bd6-5db6e8476a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=3)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
    "df.select(xpath_short(df.x, lit('sum(a/b)')).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f378ac0-b115-4656-8502-5b39dd7078ed",
   "metadata": {},
   "source": [
    "#### pyspark.sql.functions.xpath_string(xml: ColumnOrName, path: ColumnOrName) → pyspark.sql.column.Column\n",
    "Returns the text contents of the first xml node that matches the XPath expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4e9aa4e-af79-4c51-8275-a33ae2ef2e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='cc')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('<a><b>b</b><c>cc</c></a>',)], ['x'])\n",
    "df.select(xpath_string(df.x, lit('a/c')).alias('r')).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
