{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efca738b-a5c1-434a-9d86-688f99589f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------------+--------------------+-----+\n",
      "|    Player Name|Season|       Statistic|            Variable|Value|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "|Robert Garrigus|  2010|Driving Distance|Driving Distance ...|   71|\n",
      "|   Bubba Watson|  2010|Driving Distance|Driving Distance ...|   77|\n",
      "| Dustin Johnson|  2010|Driving Distance|Driving Distance ...|   83|\n",
      "|Brett Wetterich|  2010|Driving Distance|Driving Distance ...|   54|\n",
      "|    J.B. Holmes|  2010|Driving Distance|Driving Distance ...|  100|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 10:47:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "path = \"datasets/\"\n",
    "students = spark.read.csv(path+\"students.csv\", inferSchema=True, header=True)\n",
    "tour = spark.read.csv(path+\"pga_tour_historical.csv\", inferSchema=True, header=True)\n",
    "tour.limit(100).dropna().dropDuplicates().limit(5).show()\n",
    "students.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42ace4-15c8-4b5a-addf-71e829922f05",
   "metadata": {},
   "source": [
    "#### PandasUDFType\n",
    "#### pandas_udf(f=None, returnType=None, functionType=None)\n",
    "\n",
    "Creates a pandas user defined function (a.k.a. vectorized user defined function).\n",
    "\n",
    "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "Parameters\n",
    "* f – user-defined function. A python function if used as a standalone function\n",
    "* returnType – the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "* functionType – an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR.\n",
    "\n",
    "> Note This parameter exists for compatibility. Using Python type hints is encouraged.\n",
    "In order to use this API, customarily the below are imported:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "```\n",
    "From Spark 3.0 with Python 3.6+, Python type hints detect the function types as below:\n",
    "```\n",
    "@pandas_udf(IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()\n",
    "```\n",
    "Prior to Spark 3.0, the pandas UDF used functionType to decide the execution type as below:\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3676f360-e0f7-4646-8c84-82b31ddc8917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>abs((math score * -1))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   abs((math score * -1))  \n",
       "0                      72  \n",
       "1                      69  \n",
       "2                      90  \n",
       "3                      47  \n",
       "4                      76  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abs(col)\n",
    "students.limit(5).select(\"*\", abs(students[\"math score\"]  * (0 - 1))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51586fa-1767-4d83-b0ea-5aa4fb1514ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>ACOS((math score * 0.01))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>0.766994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>0.809307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>0.451027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>1.081506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>0.707483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   ACOS((math score * 0.01))  \n",
       "0                   0.766994  \n",
       "1                   0.809307  \n",
       "2                   0.451027  \n",
       "3                   1.081506  \n",
       "4                   0.707483  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acos(col)\n",
    "# Returns inverse cosine of col, as if computed by java.lang.Math.acos()\n",
    "students.limit(5).select(\"*\", acos(0.01 * students[\"math score\"])).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938c8d98-d8e3-4739-a6cd-37cddeb29f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(next_month=datetime.date(2015, 5, 8))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add_month(start, months)\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(add_months(df.dt, 1).alias('next_month')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ea3745-1328-4d5b-a7a0-fb6f125a480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/17 23:24:44 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|distinct_math_score|\n",
      "+-------------------+\n",
      "|                 82|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# approxCountDistinct(col, rsd=None)\n",
    "# Deprecated in 2.1, use approx_count_distinct() instead.\n",
    "# approx_count_distinct(col, rsd=None)\n",
    "students.agg(approx_count_distinct(students[\"math score\"]).alias('distinct_math_score')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb568e6-9a0c-4ae7-9c72-455e057dffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     arr|\n",
      "+--------+\n",
      "|[72, 72]|\n",
      "|[69, 90]|\n",
      "|[90, 95]|\n",
      "|[47, 57]|\n",
      "|[76, 78]|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array(*cols)\n",
    "# Creates a new array column.\n",
    "students.select(array('math score', 'reading score').alias(\"arr\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c00b4-8f5e-495c-9f87-543172a463c1",
   "metadata": {},
   "source": [
    "#### array_contains(col, value)\n",
    "##### Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    "Parameters: \n",
    "- col – name of column containing array\n",
    "- value – value or column to check for in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c787648e-f10f-40c7-8a9e-93437db18a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
    "df.select(array_contains(df.data, \"a\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00843989-2970-4184-9574-1f4ee6af700c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(array_contains(df.data, lit(\"a\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05cdd83-bc50-4137-a18f-2c9f5a27bfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_distinct(col)\n",
    "# Collection function: removes duplicate values from the array.\n",
    "df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
    "df.select(array_distinct(df.data)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae58a241-f8ef-4a8c-ad8d-c5d546974c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_except(c1, c2)=['b'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_except(col1, col2)\n",
    "# Collection function: returns an array of the elements in col1 but not in col2, without duplicates.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_except(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bb2406-0828-408e-bf1c-70a1a3614e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_intersect(c1, c2)=['a', 'c'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_intersect(col1, col2)\n",
    "# Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_intersect(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d146da-18cb-4f22-aeeb-c4eb1e0c0abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|joined|\n",
      "+------+\n",
      "| a,b,c|\n",
      "|     a|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|joined|\n",
      "+------+\n",
      "| a,b,c|\n",
      "|a,NULL|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_join(col, delimiter, null_replacement=None)\n",
    "# Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
    "df.select(array_join(df.data, \",\").alias(\"joined\")).show()\n",
    "df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8d931e-53e1-40c7-b19d-58538cc3534d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max=3), Row(max=10)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_max(col)\n",
    "# Collection function: returns the maximum value of the array.\n",
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_max(df.data).alias('max')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ebcc890-c2c9-422c-9804-0232e6af50a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min=1), Row(min=-1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_min(col)[source]\n",
    "# Collection function: returns the minimum value of the array.\n",
    "df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
    "df.select(array_min(df.data).alias('min')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4327dd7-5325-48f4-a683-37236a01becc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_position(data, a)=3), Row(array_position(data, a)=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_position(col, value)\n",
    "# Collection function: Locates the position of the first occurrence of the given value in the given array.\n",
    "# Returns null if either of the arguments are null.\n",
    "# The position is not zero based, but 1 based index.\n",
    "# Returns 0 if the given value could not be found in the array.\n",
    "df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
    "df.select(array_position(df.data, \"a\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2759fd3a-89a4-4890-bc34-29e9cc437c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_remove(col, element)\n",
    "# Collection function: Remove all elements that equal to element from the given array.\n",
    "df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
    "df.select(array_remove(df.data, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b6f6bda-2be5-4ad8-96ca-28480cae6a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=['ab', 'ab', 'ab'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.sql.functions.array_repeat(col, count)\n",
    "# Collection function: creates an array containing a column repeated count times.\n",
    "df = spark.createDataFrame([('ab',)], ['data'])\n",
    "df.select(array_repeat(df.data, 3).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "415ae933-b25f-4220-8483-370eca810269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_sort(col)\n",
    "# Collection function: sorts the input array in ascending order. The elements of the input array must be orderable.\n",
    "# Null elements will be placed at the end of the returned array.\n",
    "df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
    "df.select(array_sort(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af92aed-d60c-415f-b2ba-e8bc2b950567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array_union(col1, col2)\n",
    "# Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    "df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "df.select(array_union(df.c1, df.c2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee024e5-85fc-4be6-bebb-78589d5087ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overlap=True), Row(overlap=False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arrays_overlap(a1, a2)\n",
    "# Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    "df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
    "df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e9c30f-30b3-491a-8cc9-ef9246e24c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arrays_zip(*cols)\n",
    "# Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
    "df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a409c093-45ae-4d10-91a7-97ec04232cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|         0|           17|\n",
      "|        28|           23|\n",
      "|        30|           24|\n",
      "|         8|           24|\n",
      "|        30|           26|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc(col)¶\n",
    "# Returns a sort expression based on the ascending order of the given column name.\n",
    "students.select(\"math score\", \"reading score\").orderBy(asc(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3303499-64a0-4072-a2ff-496ce9156286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|         0|           17|\n",
      "|        28|           23|\n",
      "|        30|           24|\n",
      "|         8|           24|\n",
      "|        30|           26|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc_nulls_first(col)¶\n",
    "# Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.\n",
    "students.select(\"math score\", \"reading score\").orderBy(asc_nulls_first(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa62b210-f65d-4581-81ff-b3b76cbc069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|         0|           17|\n",
      "|        28|           23|\n",
      "|        30|           24|\n",
      "|         8|           24|\n",
      "|        30|           26|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc_nulls_last(col)¶\n",
    "# Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.\n",
    "students.select(\"math score\", \"reading score\").orderBy(asc_nulls_last(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c7a49e3-6206-4ce4-9009-2bd7e3b83d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------------------+\n",
      "|    Player name|Value|ascii(Player name)|\n",
      "+---------------+-----+------------------+\n",
      "|Robert Garrigus|   71|                82|\n",
      "|   Bubba Watson|   77|                66|\n",
      "| Dustin Johnson|   83|                68|\n",
      "|Brett Wetterich|   54|                66|\n",
      "|    J.B. Holmes|  100|                74|\n",
      "+---------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ascii(col)\n",
    "# Computes the numeric value of the first character of the string column.\n",
    "tour.select(\"Player name\", \"Value\", ascii(\"Player name\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa62109f-8d83-4533-afa1-584e84b0d11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>ASIN((math score * 0.01))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>0.803802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>0.761489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>1.119770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>0.489291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>0.863313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   ASIN((math score * 0.01))  \n",
       "0                   0.803802  \n",
       "1                   0.761489  \n",
       "2                   1.119770  \n",
       "3                   0.489291  \n",
       "4                   0.863313  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/17 23:24:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# asin(col)\n",
    "# Returns inverse sine of col, as if computed by java.lang.Math.asin()\n",
    "students.limit(5).select(\"*\", asin(0.01 * students[\"math score\"])).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "084a2f2d-6b86-47d8-9f9d-9713ed124289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>ATAN((math score * 0.01))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>0.624023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>0.603983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>0.732815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>0.439361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>0.649870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   ATAN((math score * 0.01))  \n",
       "0                   0.624023  \n",
       "1                   0.603983  \n",
       "2                   0.732815  \n",
       "3                   0.439361  \n",
       "4                   0.649870  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# atan(col)\n",
    "# Returns inverse tangent of col, as if computed by java.lang.Math.atan()\n",
    "students.limit(5).select(\"*\", atan(0.01 * students[\"math score\"])).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c4ab291-b79f-42e4-a158-5e5abd27fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atan2(col1, col2)\n",
    "# Returns the theta component of the point (r, theta) in polar coordinates that corresponds to the point (x, y) in Cartesian coordinates, as if computed by java.lang.Math.atan2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71364637-2841-41d7-a06b-7b66b0683397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|gender|          average|\n",
      "+------+-----------------+\n",
      "|female|79.16981132075472|\n",
      "|  male| 87.6842105263158|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.sql.functions.avg(col)\n",
    "# Aggregate function: returns the average of the values in a group.\n",
    "students.filter(col(\"reading score\") > 80).groupBy(\"gender\").agg(avg(col(\"math score\")).alias(\"average\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37de17d6-f746-4985-90cc-633e0b80e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64(col)\n",
    "# Computes the BASE64 encoding of a binary column and returns it as a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "645f63c8-31f3-423a-a169-a760934c3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basestring\n",
    "# alias of builtins.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90893e81-91c1-4a11-ba64-e6f8a448cfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c='1000111'),\n",
       " Row(c='1001101'),\n",
       " Row(c='1010011'),\n",
       " Row(c='110110'),\n",
       " Row(c='1100100')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin(col)\n",
    "# Returns the string representation of the binary value of the given column\n",
    "tour.limit(5).select(bin(tour.Value).alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40d7f1d7-9a7c-411c-b4c0-231798b32396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zsavchenko/.local/share/virtualenvs/spark_env-J_TJEM2Z/lib/python3.11/site-packages/pyspark/sql/functions.py:2083: FutureWarning: Deprecated in 3.2, use bitwise_not instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use bitwise_not instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+\n",
      "| Name|Value|BitwiseNOT_Value|\n",
      "+-----+-----+----------------+\n",
      "|Alice|    5|              -6|\n",
      "|  Bob|   10|             -11|\n",
      "|Carol|    3|              -4|\n",
      "+-----+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bitwiseNOT(col)¶\n",
    "# Computes bitwise not.\n",
    "data = [(\"Alice\", 5), (\"Bob\", 10), (\"Carol\", 3)]\n",
    "columns = [\"Name\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.withColumn(\"BitwiseNOT_Value\", bitwiseNOT(col(\"Value\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b21f0dd2-71ff-469e-a968-d192c29af51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast(df)\n",
    "# Marks a DataFrame as small enough for use in broadcast joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be81fe93-7e0e-4eaf-bdb2-724387844b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=2.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bround(col, scale=0)\n",
    "# Round the given value to scale decimal places using HALF_EVEN rounding mode if scale >= 0 or at integral part when scale < 0.\n",
    "spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc8fe0c9-8425-47a0-a68f-099a48fb121c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>CBRT(math score)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>4.160168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>4.101566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>4.481405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>3.608826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>4.235824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   CBRT(math score)  \n",
       "0          4.160168  \n",
       "1          4.101566  \n",
       "2          4.481405  \n",
       "3          3.608826  \n",
       "4          4.235824  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cbrt(col)\n",
    "# Computes the cube-root of the given value.\n",
    "students.limit(5).select(\"*\", cbrt(students[\"math score\"])).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ed62aae-e5c2-4763-a4f6-0ae6843cd8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>CEIL(math score)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \\\n",
       "0                    none          72             72             74   \n",
       "1               completed          69             90             88   \n",
       "2                    none          90             95             93   \n",
       "3                    none          47             57             44   \n",
       "4                    none          76             78             75   \n",
       "\n",
       "   CEIL(math score)  \n",
       "0                72  \n",
       "1                69  \n",
       "2                90  \n",
       "3                47  \n",
       "4                76  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceil(col)¶\n",
    "# Computes the ceiling of the given value.\n",
    "students.limit(5).select(\"*\", ceil(students[\"math score\"])).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd3dddc6-df9c-4a0a-95bb-d14d75b17098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|NULL|NULL|\n",
      "|   1|NULL|\n",
      "|NULL|   2|\n",
      "+----+----+\n",
      "\n",
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n",
      "+----+----+----------------+\n",
      "|   a|   b|coalesce(a, 0.0)|\n",
      "+----+----+----------------+\n",
      "|NULL|NULL|             0.0|\n",
      "|   1|NULL|             1.0|\n",
      "|NULL|   2|             0.0|\n",
      "+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coalesce(*cols)\n",
    "# Returns the first column that is not null.\n",
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.show()\n",
    "cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
    "cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ebf065a-1088-4c03-90d5-7d84433d201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Value'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# col(col)\n",
    "# Returns a Column based on the given column name.\n",
    "col(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe4019e7-73bc-4bb3-bd1f-2e6f66edf1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_list(age)=[2, 5, 5])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect_list(col)¶\n",
    "# Aggregate function: returns a list of objects with duplicates.\n",
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(collect_list('age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "945c2dfd-dd10-4b86-9f7f-1106013319d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_set(age)=[5, 2])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect_set(col)¶\n",
    "# Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(collect_set('age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "775256f4-8a25-43dc-bbb4-051fbced4c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Value'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column(col)¶\n",
    "# Returns a Column based on the given column name.\n",
    "column(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c1befae-40dd-4c60-8f57-9d0a9fdef480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      s|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n",
      "+---------------+\n",
      "|            arr|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "|           NULL|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat(*cols)\n",
    "# Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns.\n",
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat(df.s, df.d).alias('s')).show()\n",
    "\n",
    "df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
    "df.select(concat(df.a, df.b, df.c).alias(\"arr\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60c24dd9-7880-4ae9-aee7-314d0b562d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd-123')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat_ws(sep, *cols)\n",
    "# Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat_ws('-', df.s, df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b08ecdda-3616-45c0-9766-358d5dc45556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hex='15')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv(col, fromBase, toBase)\n",
    "# Convert a number in a string column from one base to another.\n",
    "df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
    "df.select(conv(df.n, 2, 16).alias('hex')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "162cd8db-463c-43da-b46c-edc0ef6c28c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corr(col1, col2)\n",
    "# Returns a new Column for the Pearson Correlation Coefficient for col1 and col2.\n",
    "a = range(20)\n",
    "b = [2 * x for x in range(20)]\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(corr(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6c7300f-1483-4cf9-bb15-d72f8751c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos(col)\n",
    "# Parameters - col – angle in radians\n",
    "# Returns - cosine of the angle, as if computed by java.lang.Math.cos()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dda0020-6949-4810-af2b-84bf5e4f6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosh(col)\n",
    "# Parameters - col – hyperbolic angle\n",
    "# Returns - hyperbolic cosine of the angle, as if computed by java.lang.Math.cosh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a998848-7b51-4697-8a6e-0422a6d1331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(math score)|\n",
      "+-----------------+\n",
      "|             1000|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.sql.functions.count(col)\n",
    "# Aggregate function: returns the number of items in a group.\n",
    "students.select(count(\"math score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "158fce64-8145-48dd-a557-1714cdcddc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|1054793|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:=========>                                              (2 + 10) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|1054793|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# countDistinct(col, *cols)\n",
    "# Returns a new Column for distinct count of col or cols.\n",
    "tour.agg(countDistinct(tour.Value, tour[\"Player name\"]).alias('c')).show()\n",
    "tour.agg(countDistinct(\"Value\", \"Player name\").alias('c')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c65134bc-d52a-4e68-9c45-04bf339cf79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=0.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# covar_pop(col1, col2)\n",
    "# Returns a new Column for the population covariance of col1 and col2.\n",
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89242909-5b1b-4a99-a2a0-9202cd71e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=0.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# covar_samp(col1, col2)\n",
    "# Returns a new Column for the sample covariance of col1 and col2.\n",
    "a = [1] * 10\n",
    "b = [1] * 10\n",
    "df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
    "df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8e79dd7-478e-4680-8c00-d99e2fff14c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(crc32=2743272264)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crc32(col)\n",
    "# Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint.\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86e21d91-d20b-493c-81a9-5d109e252025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|map                    |\n",
      "+-----------------------+\n",
      "|{Robert Garrigus -> 71}|\n",
      "|{Bubba Watson -> 77}   |\n",
      "|{Dustin Johnson -> 83} |\n",
      "|{Brett Wetterich -> 54}|\n",
      "|{J.B. Holmes -> 100}   |\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------+\n",
      "|map                    |\n",
      "+-----------------------+\n",
      "|{Robert Garrigus -> 71}|\n",
      "|{Bubba Watson -> 77}   |\n",
      "|{Dustin Johnson -> 83} |\n",
      "|{Brett Wetterich -> 54}|\n",
      "|{J.B. Holmes -> 100}   |\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create_map(*cols)\n",
    "# Creates a new map column.\n",
    "# Parameters - cols – list of column names (string) or list of Column expressions that are grouped as key-value pairs, e.g. (key1, value1, key2, value2, …).\n",
    "\n",
    "tour.select(create_map('Player name', 'Value').alias(\"map\")).show(5, False)\n",
    "tour.select(create_map([tour[\"Player name\"], tour.Value]).alias(\"map\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fcaba97-992a-46b6-876d-4de5e430316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cume_dist()\n",
    "# Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5297b31-784d-4183-8d6c-ba05681837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date()\n",
    "# Returns the current date as a DateType column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb241817-679d-4af4-b8af-e786cd6f8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_timestamp()\n",
    "# Returns the current timestamp as a TimestampType column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "683848f3-b185-4335-abce-8761626df79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(next_date=datetime.date(2015, 4, 9))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date_add(start, days)\n",
    "# Returns the date that is days days after start\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(date_add(df.dt, 1).alias('next_date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a261db5-fa5e-4036-b19d-1c690b77401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='04/08/2015')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date_format(date, format)\n",
    "# Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\n",
    "# A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of datetime pattern can be used.\n",
    "# Use when ever possible specialized functions like year. These benefit from a specialized implementation.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49e6d407-c3c2-4983-91fb-f2e8157c66cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| prev_date|\n",
      "+----------+\n",
      "|2015-04-07|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_sub(start, days)\n",
    "# Returns the date that is days days before start\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(date_sub(df.dt, 1).alias('prev_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e12434f-808a-42cd-a60d-97c0828ebc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               year|\n",
      "+-------------------+\n",
      "|1997-01-01 00:00:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|              month|\n",
      "+-------------------+\n",
      "|1997-02-01 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_trunc(format, timestamp)\n",
    "# Returns timestamp truncated to the unit specified by the format.\n",
    "# Parameters - format – ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’, ‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’\n",
    "\n",
    "df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
    "df.select(date_trunc('year', df.t).alias('year')).show()\n",
    "\n",
    "df.select(date_trunc('mon', df.t).alias('month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b9a1167-b0d8-4a05-94af-bfe456ba755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(diff=32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datediff(end, start)\n",
    "# Returns the number of days from start to end.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "df.select(datediff(df.d2, df.d1).alias('diff')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27760267-15e1-44d9-a26c-aadf3668b7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=8)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dayofmonth(col)\n",
    "# Extract the day of the month of a given date as integer.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofmonth('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f47953c-d218-4827-83ba-272196e40675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=4)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dayofweek(col)\n",
    "# Extract the day of the week of a given date as integer.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofweek('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6d24cb7-67c6-4037-80d1-5be508a32c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=98)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dayofyear(col)\n",
    "# Extract the day of the year of a given date as integer.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofyear('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8226008-66d6-4d5f-b1b9-81b1b3df4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode(col, charset)\n",
    "# Computes the first argument into a string from a binary using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60bfb7a4-8e89-49a7-bade-4a0f99aaf21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees(col)\n",
    "# Converts an angle measured in radians to an approximately equivalent angle measured in degrees.\n",
    "# Parameters - col – angle in radians\n",
    "# Returns - angle in degrees, as if computed by java.lang.Math.toDegrees()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1038f328-06bb-4f75-8277-212e1636a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_rank()\n",
    "# Window function: returns the rank of rows within a window partition, without any gaps.\n",
    "# The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "# This is equivalent to the DENSE_RANK function in SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7a1b60b-d8a7-4cf3-9e9c-d9449da97e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|       100|          100|\n",
      "|        92|          100|\n",
      "|        97|          100|\n",
      "|        87|          100|\n",
      "|        87|          100|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# desc(col)\n",
    "# Returns a sort expression based on the descending order of the given column name.\n",
    "students.select(\"math score\", \"reading score\").orderBy(desc(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cf1f7d0-14f9-4c37-b61a-283210c88e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|       100|          100|\n",
      "|        92|          100|\n",
      "|        97|          100|\n",
      "|        87|          100|\n",
      "|        87|          100|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# desc_nulls_first(col)\n",
    "# Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.\n",
    "students.select(\"math score\", \"reading score\").orderBy(desc_nulls_first(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce7ea0ee-d618-43de-9d5c-e98b686a0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|math score|reading score|\n",
      "+----------+-------------+\n",
      "|       100|          100|\n",
      "|        92|          100|\n",
      "|        97|          100|\n",
      "|        87|          100|\n",
      "|        87|          100|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# desc_nulls_last(col)\n",
    "# Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values\n",
    "students.select(\"math score\", \"reading score\").orderBy(desc_nulls_last(\"reading score\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a155884-c557-4266-8a32-b10b32c384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|element_at(data, 1)|\n",
      "+-------------------+\n",
      "|                  a|\n",
      "|               NULL|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|element_at(data, a)|\n",
      "+-------------------+\n",
      "|                1.0|\n",
      "|               NULL|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# element_at(col, extraction)\n",
    "# Collection function: Returns element of array at given index in extraction if col is array. Returns value for the given key in extraction if col is map.\n",
    "# Parameters - col – name of column containing array or map\n",
    "#            - extraction – index to check for in array or key to check for in map\n",
    "# The position is not zero based, but 1 based index.\n",
    "\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
    "df.select(element_at(df.data, 1)).show()\n",
    "\n",
    "df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n",
    "df.select(element_at(df.data, lit(\"a\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4fa8852c-9f33-4047-8493-77feae75d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode(col, charset)\n",
    "# Computes the first argument into a binary from a string using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4126c518-ce6b-4325-aeff-241a8bcdd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp(col)\n",
    "# Computes the exponential of the given value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f6e1c67-3eb9-4acd-9bc8-4baac6671a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|anInt|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode(col)\n",
    "# Returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).show()\n",
    "\n",
    "eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d29bd911-92ec-45f8-9314-dffcd55fdcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+\n",
      "| id|  an_array| key|value|\n",
      "+---+----------+----+-----+\n",
      "|  1|[foo, bar]|   x|  1.0|\n",
      "|  2|        []|NULL| NULL|\n",
      "|  3|      NULL|NULL| NULL|\n",
      "+---+----------+----+-----+\n",
      "\n",
      "+---+----------+----+\n",
      "| id|     a_map| col|\n",
      "+---+----------+----+\n",
      "|  1|{x -> 1.0}| foo|\n",
      "|  1|{x -> 1.0}| bar|\n",
      "|  2|        {}|NULL|\n",
      "|  3|      NULL|NULL|\n",
      "+---+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode_outer(col)\n",
    "# Returns a new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
    "df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6ed4424-ce33-42bb-ad33-64d781404b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expm1(col)\n",
    "# Computes the exponential of the given value minus one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee4ae04d-8903-467c-817f-e5b45b6257ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|length(gender)|\n",
      "+--------------+\n",
      "|             6|\n",
      "|             6|\n",
      "|             6|\n",
      "|             4|\n",
      "|             4|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expr(str)\n",
    "# Parses the expression string into the column that it represents\n",
    "\n",
    "students.select(expr(\"length(gender)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2dad1bf6-7283-43cb-ad93-8d04811ee10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(f=120)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factorial(col)\n",
    "# Computes the factorial of the given value.\n",
    "\n",
    "df = spark.createDataFrame([(5,)], ['n'])\n",
    "df.select(factorial(df.n).alias('f')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9e202a0-1fc0-40f3-b3e0-2a00d1e52bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first(Value): string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first(col, ignorenulls=False)[source]\n",
    "# Aggregate function: returns the first value in a group.\n",
    "# The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "first_value = tour.select(first(\"Value\", ignorenulls=True)).alias(\"First_Value\")\n",
    "first_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "abd82cd2-5718-4832-81f5-8ea4ad0361d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Player name: string, First_Value: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = tour.groupBy(\"Player name\").agg(first(col(\"Value\")).alias(\"First_Value\"))\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0604778-3d05-4937-b069-1a15d3de8aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|first(Value)|\n",
      "+------------+\n",
      "|          71|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|   Player name|First_Value|\n",
      "+--------------+-----------+\n",
      "|A.J. McInerney|          2|\n",
      "|Aaron Baddeley|         94|\n",
      "|Aaron Goldberg|          2|\n",
      "| Aaron Krueger|          2|\n",
      "|    Aaron Pike|         40|\n",
      "+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "first_value.show()\n",
    "grouped_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8216607c-22a4-4de2-8239-3d91e46c4f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten(col)\n",
    "# Collection function: creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.\n",
    "\n",
    "df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
    "df.select(flatten(df.data).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "603e1cc1-1dbd-42b5-be7f-53519b311310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|value|floor_value|\n",
      "+-----+-----------+\n",
      "|  3.7|          3|\n",
      "|  4.2|          4|\n",
      "|  5.9|          5|\n",
      "+-----+-----------+\n",
      "\n",
      "+-----+-----------+\n",
      "|value|floor_value|\n",
      "+-----+-----------+\n",
      "|3.789|       3.78|\n",
      "|4.256|       4.25|\n",
      "|5.914|       5.91|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# floor(col)\n",
    "# Computes the floor of the given value.\n",
    "\n",
    "df = spark.createDataFrame([(3.7,), (4.2,), (5.9,)], [\"value\"])\n",
    "df.withColumn(\"floor_value\", floor(df[\"value\"])).show()\n",
    "\n",
    "# df = spark.createDataFrame([(\"2023-10-09 14:30:45\",), (\"2023-10-09 14:45:30\",)], [\"timestamp\"])\n",
    "# df = df.withColumn(\"date\", to_date(df[\"timestamp\"]))\n",
    "# df.withColumn(\"floor_date\", floor(df[\"date\"])).show()\n",
    "\n",
    "df = spark.createDataFrame([(3.789,), (4.256,), (5.914,)], [\"value\"])\n",
    "result_df = df.withColumn(\"floor_value\", floor(df[\"value\"] * 100) / 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1e4d954-c99d-48e9-ad62-53f6b6d2ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5.0000')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_number(col, d)\n",
    "# Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string.\n",
    "\n",
    "spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "652cf4f7-15f2-48ea-997b-59fe02e6277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='5 hello')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_string(format, *cols)\n",
    "# Formats the arguments in printf-style and returns the result as a string column.\n",
    "\n",
    "df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
    "df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16637492-deaa-4471-8c49-bdcfd235db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      csv|\n",
      "+---------+\n",
      "|{1, 2, 3}|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|      csv|\n",
      "+---------+\n",
      "|{1, 2, 3}|\n",
      "+---------+\n",
      "\n",
      "+-----+\n",
      "|  csv|\n",
      "+-----+\n",
      "|{abc}|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_csv(col, schema, options={})\n",
    "# Parses a column containing a CSV string to a row with the specified schema. Returns null, in the case of an unparseable string.\n",
    "\n",
    "data = [(\"1,2,3\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).show()\n",
    "\n",
    "value = data[0][0]\n",
    "df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).show()\n",
    "\n",
    "data = [(\"   abc\",)]\n",
    "df = spark.createDataFrame(data, (\"value\",))\n",
    "options = {'ignoreLeadingWhiteSpace': True}\n",
    "df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a74afc89-a47e-47d3-8eb4-25dea5e169f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|json|\n",
      "+----+\n",
      "| {1}|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_json(col, schema, options={})\n",
    "# Parses a column containing a JSON string into a MapType with StringType as keys type, StructType or ArrayType with the specified schema. Returns null, in the case of an unparseable string.\n",
    "\n",
    "data = [(1, '''{\"a\": 1}''')]\n",
    "schema = StructType([StructField(\"a\", IntegerType())])\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ec322d5-62d4-45c4-8746-a4f977639c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|json|\n",
      "+----+\n",
      "| {1}|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(from_json(df.value, \"a INT\").alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c775482d-1180-4320-85dd-6c047b18fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    json|\n",
      "+--------+\n",
      "|{a -> 1}|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcd3b224-ce51-4f34-9f3a-30e1210516f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| json|\n",
      "+-----+\n",
      "|[{1}]|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, '''[{\"a\": 1}]''')]\n",
    "schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f4153d0-08cb-4d6d-883b-34dd60d56391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  json|\n",
      "+------+\n",
      "|{NULL}|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4101b925-78f6-4e85-b788-e4f24a858dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     json|\n",
      "+---------+\n",
      "|[1, 2, 3]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, '''[1, 2, 3]''')]\n",
    "schema = ArrayType(IntegerType())\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.select(from_json(df.value, schema).alias(\"json\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1eb8843b-69df-4198-b7d5-343f5d55046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 ts|\n",
      "+-------------------+\n",
      "|2015-04-08 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
    "# Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
    "time_df.select(from_unixtime('unix_time').alias('ts')).show()\n",
    "spark.conf.unset(\"spark.sql.session.timeZone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2a1c1-d00a-4067-a423-5b8ccfce377b",
   "metadata": {},
   "source": [
    "#### rom_utc_timestamp(timestamp, tz)\n",
    "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.\n",
    "\n",
    "However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to the given timezone.\n",
    "\n",
    "This function may return confusing result if the input is a string with timezone, e.g. ‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.\n",
    "\n",
    "Parameters:\n",
    "* timestamp – the column that contains timestamps\n",
    "* tz – A string detailing the time zone ID that the input should be adjusted to. It should be in the format of either region-based zone IDs or zone offsets. Region IDs must have the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’. Other short names are not recommended to use because they can be ambiguous.\n",
    "\n",
    "tz can take a Column containing timezone ID strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1af86891-0f9a-4895-b89f-896ce784929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         local_time|\n",
      "+-------------------+\n",
      "|1997-02-28 02:30:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|         local_time|\n",
      "+-------------------+\n",
      "|1997-02-28 19:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
    "df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).show()\n",
    "df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52dd6d39-e92b-487a-b4f1-f51920e5f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|key|     c0|    c1|\n",
      "+---+-------+------+\n",
      "|  1| value1|value2|\n",
      "|  2|value12|  NULL|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_json_object(col, path)\n",
    "# Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid.\n",
    "\n",
    "data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), get_json_object(df.jstring, '$.f2').alias(\"c1\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2589d276-8f8a-43e1-9e15-97b1bf01a2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|greatest|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# greatest(*cols)\n",
    "# Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
    "\n",
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e24435a4-9046-4483-b968-cf2db620f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 203:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+--------------------+\n",
      "|   Player name|grouping(Player name)|          sum(Value)|\n",
      "+--------------+---------------------+--------------------+\n",
      "|          NULL|                    1|3.221497314902330...|\n",
      "|A.J. McInerney|                    0|          4202016.71|\n",
      "|Aaron Baddeley|                    0|1.5230582441420002E9|\n",
      "|Aaron Goldberg|                    0|         8062168.635|\n",
      "| Aaron Krueger|                    0|           4029172.0|\n",
      "|    Aaron Pike|                    0|  185.00000000000003|\n",
      "|     Aaron Rai|                    0|              541.57|\n",
      "|Aaron Townsend|                    0|          4025596.52|\n",
      "| Aaron Watkins|                    0|      1.2073087502E7|\n",
      "|    Aaron Wise|                    0| 8.629364958069998E8|\n",
      "| Abraham Ancer|                    0| 9.412399311720005E8|\n",
      "|    Adam Bland|                    0|1.0204229770000001E7|\n",
      "|    Adam Blyth|                    0|              412.09|\n",
      "|Adam Cornelson|                    0|          4033392.96|\n",
      "| Adam Crawford|                    0|   570.2600000000001|\n",
      "| Adam D'Amario|                    0|           4037064.0|\n",
      "|      Adam Gee|                    0|  218.26999999999998|\n",
      "|    Adam Groom|                    0|              502.11|\n",
      "|   Adam Hadwin|                    0|1.7608983787030015E9|\n",
      "|     Adam Hart|                    0|          4033445.65|\n",
      "+--------------+---------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# grouping(col)\n",
    "# Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
    "\n",
    "tour.cube(\"Player name\").agg(grouping(\"Player name\"), sum(\"Value\")).orderBy(\"Player name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "14d9de9a-d7e8-41b0-9f7d-11ae5021ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 206:====>                                                  (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+\n",
      "|   Player name|grouping_id()|          sum(Value)|\n",
      "+--------------+-------------+--------------------+\n",
      "|          NULL|            1|3.221497314902330...|\n",
      "|A.J. McInerney|            0|          4202016.71|\n",
      "|Aaron Baddeley|            0|1.5230582441420002E9|\n",
      "|Aaron Goldberg|            0|         8062168.635|\n",
      "| Aaron Krueger|            0|           4029172.0|\n",
      "|    Aaron Pike|            0|  185.00000000000003|\n",
      "|     Aaron Rai|            0|              541.57|\n",
      "|Aaron Townsend|            0|          4025596.52|\n",
      "| Aaron Watkins|            0|      1.2073087502E7|\n",
      "|    Aaron Wise|            0| 8.629364958069998E8|\n",
      "| Abraham Ancer|            0| 9.412399311720005E8|\n",
      "|    Adam Bland|            0|1.0204229770000001E7|\n",
      "|    Adam Blyth|            0|              412.09|\n",
      "|Adam Cornelson|            0|          4033392.96|\n",
      "| Adam Crawford|            0|   570.2600000000001|\n",
      "| Adam D'Amario|            0|           4037064.0|\n",
      "|      Adam Gee|            0|  218.26999999999998|\n",
      "|    Adam Groom|            0|              502.11|\n",
      "|   Adam Hadwin|            0|1.7608983787030015E9|\n",
      "|     Adam Hart|            0|          4033445.65|\n",
      "+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# grouping_id(*cols)\n",
    "# Aggregate function: returns the level of grouping, equals to (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + … + grouping(cn)\n",
    "# Note – The list of columns should match with grouping columns exactly, or empty (means all the grouping columns).\n",
    "\n",
    "tour.cube(\"Player name\").agg(grouping_id(), sum(\"Value\")).orderBy(\"Player name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33b78817-ee31-4a61-b64b-839c5eb6dfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hash=-757602832)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hash(*cols)\n",
    "# Calculates the hash code of given columns, and returns the result as an int column.\n",
    "\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9c9ce14d-e3cf-4b85-8b30-c5fc91eefae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hex(a)='414243', hex(b)='3')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hex(col)¶\n",
    "# Computes hex value of the given column, which could be pyspark.sql.types.StringType, pyspark.sql.types.BinaryType, pyspark.sql.types.IntegerType or pyspark.sql.types.LongType.\n",
    "\n",
    "spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2fcd5612-48ba-4ec6-9339-93c48198fd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hour=13)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hour(col)\n",
    "# Extract the hours of a given date as integer.\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
    "df.select(hour('ts').alias('hour')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b121a7-64ca-458c-848f-5c13321abca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypot(col1, col2)\n",
    "# Computes sqrt(a^2 + b^2) without intermediate overflow or underflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f9a867a-6e1f-4be1-bbba-50f120da1d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(v='Ab Cd')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initcap(col)\n",
    "# Translate the first letter of each word to upper case in the sentence.\n",
    "\n",
    "spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1eb4f2-917e-4838-9b0b-ceb56ac48867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file_name()\n",
    "# Creates a string column for the file name of the current Spark task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03f83fab-9d53-4158-a52b-bc4f45a33cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s=2)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instr(str, substr)\n",
    "# Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null.\n",
    "# Note – The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str.\n",
    "\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(instr(df.s, 'b').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e8d702ff-3f1d-4ae3-a4a5-bb10b7fca55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r1=False, r2=False), Row(r1=True, r2=True)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isnan(col)\n",
    "# An expression that returns true iff the column is NaN.\n",
    "\n",
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "96e7c875-64c4-41c3-9170-bbe001932f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r1=False, r2=False), Row(r1=True, r2=True)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isnull(col)\n",
    "# An expression that returns true iff the column is null.\n",
    "\n",
    "df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
    "df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eb876b96-9b64-4d08-b325-130c9cbe7bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json_tuple(col, *fields)\n",
    "# Creates a new row for a json column according to the given field names.\n",
    "\n",
    "data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
    "df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
    "df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac9cc8-fbe5-44ae-948f-dbdfc97253fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kurtosis(col)\n",
    "# Aggregate function: returns the kurtosis of the values in a group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b7a50-1e6b-492b-918e-b29cc336e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag(col, offset=1, default=None)\n",
    "# This is equivalent to the LAG function in SQL.\n",
    "# Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745fbce-9b00-49c3-ace1-9e1ade27074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last(col, ignorenulls=False)\n",
    "# Aggregate function: returns the last value in a group.\n",
    "# The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4823d074-51a8-4048-a155-261e92c1bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.date(1997, 2, 28))]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_day(date)[source]\n",
    "# Returns the last day of the month which the given date belongs to.\n",
    "\n",
    "df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
    "df.select(last_day(df.d).alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f39fe-7793-493d-ba6e-b3c346d25758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead(col, offset=1, default=None)\n",
    "# This is equivalent to the LEAD function in SQL.\n",
    "# Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57af3240-e3a9-4d37-81a6-1cd2d06c54ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(least=1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# least(*cols)\n",
    "# Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
    "\n",
    "df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5bfbd9b5-3985-40c6-a572-e9dbfd39e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(length=4)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length(col)\n",
    "# Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros.\n",
    "\n",
    "spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8af4a-01e6-49d8-b12d-6d4472225a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# levenshtein(left, right)\n",
    "# Computes the Levenshtein distance of the two given strings.\n",
    "\n",
    "df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
    "df0.select(levenshtein('l', 'r').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8047d-7230-43e7-be7c-a511f7839880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit(col)\n",
    "# Creates a Column of literal value.\n",
    "\n",
    "df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d78188-23db-4a98-9f85-66fd6b7ab0d1",
   "metadata": {},
   "source": [
    "#### locate(substr, str, pos=1)\n",
    "Locate the position of the first occurrence of substr in a string column, after position pos.\n",
    "\n",
    "Note The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str.\n",
    "\n",
    "Parameters:\n",
    "* substr – a string\n",
    "* str – a Column of pyspark.sql.types.StringType\n",
    "* pos – start position (zero based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54275182-e016-4199-8621-3d220b4d17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(locate('b', df.s, 1).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6c9028d-def5-41da-a6f6-02cdd0dba86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.85733', '1.83884', '1.95424', '1.67209', '1.88081']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# og(arg1, arg2=None)\n",
    "# Returns the first argument-based logarithm of the second argument.\n",
    "# If there is only one argument, then this takes the natural logarithm of the argument.\n",
    "\n",
    "students.limit(5).select(log(10.0, students[\"math score\"]).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8499512e-06f0-416b-9d6e-b9afc1dd809c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.27666', '4.23410', '4.49980', '3.85014', '4.33073']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.limit(5).select(log(students[\"math score\"]).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "948bd30c-adb9-4749-b3fa-2eda515d515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(log2=2.0)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log10(col)\n",
    "# Computes the logarithm of the given value in Base 10.\n",
    "\n",
    "\n",
    "# log1p(col)\n",
    "# Computes the natural logarithm of the given value plus one.\n",
    "\n",
    "\n",
    "# log2(col)\n",
    "# Returns the base-2 logarithm of the argument.\n",
    "\n",
    "spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37364d1-ed0a-4910-8012-7d8ec2b13ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower(col)\n",
    "# Converts a string expression to lower case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c35d8ba6-8b8f-4f31-8d40-3c0c746d5257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='##abcd')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lpad(col, len, pad)\n",
    "# Left-pad the string column to width len with pad.\n",
    "\n",
    "df = spark.createDataFrame([('abcd',)], ['s',])\n",
    "df.select(lpad(df.s, 6, '#').alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c95bc3-a192-457d-8534-6fe3fa07daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7f7b6-8cdd-4e88-9d6d-a6b0b52efed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96a083-50f4-406d-9064-c90e2d27ec51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fde74-4183-41b1-b02f-7bde9c41b48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd0ea2-aa43-47ec-8c2a-ef30152325c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb9952-4aee-412c-97de-ae773a8b2441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
